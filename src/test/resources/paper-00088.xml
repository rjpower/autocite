<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE pdf2xml SYSTEM "pdf2xml.dtd">

<pdf2xml>
<page number="1" position="absolute" top="0" left="0" height="999" width="659">
	<fontspec id="0" size="18" family="Times" color="#131413"/>
	<fontspec id="1" size="21" family="Times" color="#131413"/>
	<fontspec id="2" size="12" family="Times" color="#131413"/>
	<fontspec id="3" size="15" family="Times" color="#131413"/>
	<fontspec id="4" size="12" family="Times" color="#131413"/>
	<fontspec id="5" size="10" family="Times" color="#131413"/>
	<fontspec id="6" size="16" family="Times" color="#131413"/>
	<fontspec id="7" size="10" family="Times" color="#131413"/>
<text top="90" left="80" width="101" height="19" font="0"><b>Chapter 21</b></text>
<text top="114" left="80" width="340" height="22" font="1"><b>Harnessing the Power of Graphic</b></text>
<text top="141" left="80" width="169" height="22" font="1"><b>Processing Units</b></text>
<text top="201" left="80" width="233" height="14" font="2"><b>Xavier Andrade and Luigi Genovese</b></text>
<text top="341" left="80" width="136" height="16" font="3"><b>21.1 Introduction</b></text>
<text top="385" left="80" width="497" height="14" font="4">The continuous increment in the power of modern high-performance computing</text>
<text top="403" left="80" width="497" height="14" font="4">(HPC) platforms has further stimulated the interest of the electronic structure calcula-</text>
<text top="421" left="80" width="498" height="14" font="4">tions community for more computationally challenging studies. Systems which were</text>
<text top="439" left="80" width="497" height="14" font="4">intractable only few years ago become now accessible with the advent of modern</text>
<text top="457" left="80" width="497" height="14" font="4">machines. In the past few years, the possibility of using graphic processing units</text>
<text top="475" left="80" width="498" height="14" font="4">(GPU) for scientiﬁc calculations has raised a lot of interest as alternative to current</text>
<text top="493" left="80" width="497" height="14" font="4">calculations based on central processing units (CPU). A technology initially devel-</text>
<text top="511" left="80" width="497" height="14" font="4">oped for home computers has rapidly evolved in the direction of a programmable</text>
<text top="529" left="80" width="498" height="14" font="4">parallel streaming processor. The features of these devices, in particular the very</text>
<text top="547" left="80" width="497" height="14" font="4">low price performance ratio, together with the relatively low energy consumption</text>
<text top="565" left="80" width="497" height="14" font="4">per Flops (ﬂoating point operations per second), make them attractive platforms for</text>
<text top="583" left="80" width="199" height="14" font="4">intensive scientiﬁc computations.</text>
<text top="648" left="98" width="67" height="12" font="5">X. Andrade (</text>
<text top="640" left="165" width="16" height="24" font="6">B</text>
<text top="648" left="178" width="4" height="12" font="5">)</text>
<text top="663" left="98" width="354" height="12" font="5">Department of Chemistry and Chemical Biology, Harvard University,</text>
<text top="678" left="98" width="248" height="12" font="5">12 Oxford Street, Cambridge, MA 02138, USA</text>
<text top="693" left="98" width="126" height="12" font="5">e-mail: xavier@tddft.org</text>
<text top="715" left="98" width="59" height="12" font="5">X. Andrade</text>
<text top="730" left="98" width="424" height="12" font="5">ESTF and Departamento de Física de Materiales, University of the Basque Country</text>
<text top="745" left="98" width="285" height="12" font="5">UPV/EHU, Av. Tolosa 72, 20018 San Sebastían, Spain</text>
<text top="771" left="101" width="64" height="12" font="5">L. Genovese</text>
<text top="786" left="98" width="216" height="12" font="5">European Synchtrotron Radiation Facility,</text>
<text top="801" left="98" width="253" height="12" font="5">6 rue Horowitz, BP220, 38043 Grenoble, France</text>
<text top="828" left="98" width="64" height="12" font="5">L. Genovese</text>
<text top="843" left="98" width="335" height="12" font="5">Laboratoire de Simulation Atomistique, Commissariat à l’Énergie</text>
<text top="858" left="98" width="272" height="12" font="5">Atomique et aux Energies Alternatives, INAC/SP2M,</text>
<text top="873" left="98" width="251" height="12" font="5">17 avenue des Martyrs, 38054 Grenoble, France</text>
<text top="920" left="80" width="431" height="12" font="5">M. A. L. Marques et al. (eds.), <i>Fundamentals of Time-Dependent Density Functional</i></text>
<text top="920" left="559" width="19" height="12" font="5">401</text>
<text top="935" left="80" width="393" height="12" font="7"><i>Theory</i>, Lecture Notes in Physics 837, DOI: 10.1007/978-3-642-23518-4_21,</text>
<text top="950" left="80" width="217" height="12" font="5">© Springer-Verlag Berlin Heidelberg 2012</text>
</page>
<page number="2" position="absolute" top="0" left="0" height="999" width="659">
	<fontspec id="8" size="12" family="Times" color="#0000ff"/>
	<fontspec id="9" size="9" family="Times" color="#0000ff"/>
	<fontspec id="10" size="7" family="Times" color="#131413"/>
<text top="55" left="80" width="19" height="12" font="5">402</text>
<text top="55" left="430" width="148" height="12" font="5">X. Andrade and L. Genovese</text>
<text top="89" left="98" width="480" height="14" font="4">In this chapter we address the usage of GPU architectures in DFT and TDDFT</text>
<text top="107" left="80" width="497" height="14" font="4">computations. The objective is to give an overview on the problematic of porting</text>
<text top="125" left="80" width="498" height="14" font="4">a code to a hybrid CPU/GPU platform, in general and in the particular case of the</text>
<text top="143" left="80" width="497" height="14" font="4">KS picture for modelling electronic systems. We start by brieﬂy discussing GPU</text>
<text top="161" left="80" width="498" height="14" font="4">architecture and their special characteristics in comparison with CPUs. We then</text>
<text top="179" left="80" width="497" height="14" font="4">move to the aspects of programming a GPU code. Next we present two applica-</text>
<text top="197" left="80" width="497" height="14" font="4">tions of GPUs: ground-state DFT calculations in the BigDFT code (Genovese et al.</text>
<text top="215" left="80" width="497" height="14" font="4">2008) and real-time TDDFT propagation in the octopus code (Marques et al. 2003;</text>
<text top="233" left="80" width="497" height="14" font="4">Castro et al. 2006). Finally we discuss how GPUs could be used for other types of</text>
<text top="251" left="80" width="224" height="14" font="4">TDDFT applications and formalisms.</text>
<text top="296" left="80" width="325" height="16" font="3"><b>21.2 Basic Concepts in GPU Architectures</b></text>
<text top="340" left="80" width="497" height="14" font="4">In many aspects, GPUs are quite different from CPUs. Understanding the peculiarities</text>
<text top="358" left="80" width="497" height="14" font="4">of their architecture is essential to plan the GPU port of a program and to write efﬁcient</text>
<text top="376" left="80" width="66" height="14" font="4">GPU code.</text>
<text top="394" left="98" width="479" height="14" font="4">The ﬁrst particularity of GPUs is that they are co-processors controlled exclusively</text>
<text top="412" left="80" width="107" height="14" font="4">by a CPU (see Fig.</text>
<text top="412" left="190" width="26" height="14" font="8"><a href="pdfxml.html#3">21.1</a></text>
<text top="412" left="216" width="362" height="14" font="4"><a href="pdfxml.html#3">). </a>GPUs cannot be used alone and must have a CPU associated,</text>
<text top="430" left="80" width="497" height="14" font="4">thus forming a so-called hybrid architecture. GPUs can only access data that lies in</text>
<text top="448" left="80" width="497" height="14" font="4">its dedicated memory. Data must be explicitly copied by the programmer between</text>
<text top="466" left="80" width="497" height="14" font="4">CPU memory and GPU memory. They are normally connected through a PCI Express</text>
<text top="484" left="80" width="464" height="14" font="4">(PCIe) link, which has a relatively small bandwidth and high latency (see Table.</text>
<text top="484" left="547" width="26" height="14" font="8"><a href="pdfxml.html#3">21.1</a></text>
<text top="484" left="573" width="5" height="14" font="4"><a href="pdfxml.html#3">)</a></text>
<text top="502" left="98" width="480" height="14" font="4">GPUs are massively parallel processors, in a single chip they can include hundreds</text>
<text top="520" left="80" width="197" height="14" font="4">of ﬂoating point execution <a href="pdfxml.html#2">units,</a></text>
<text top="517" left="277" width="6" height="10" font="9"><a href="pdfxml.html#2">1</a></text>
<text top="520" left="288" width="290" height="14" font="4">while a multi-core CPU typically has around 32</text>
<text top="538" left="80" width="497" height="14" font="4">ﬂoating point execution units (considering all cores). As a result, the theoretical</text>
<text top="556" left="80" width="497" height="14" font="4">ﬂoating point throughput of a GPU is around one order of magnitude larger than the</text>
<text top="574" left="80" width="497" height="14" font="4">one of a CPU (part of the difference is compensated by the CPU higher operating</text>
<text top="591" left="80" width="497" height="14" font="4">frequency). Both processing units have approximately the same number of transistors</text>
<text top="609" left="80" width="497" height="14" font="4">and consume an amount of power of the same order, so this difference is mainly</text>
<text top="627" left="80" width="497" height="14" font="4">explained by the design strategy of each processor type, based on the tasks that each</text>
<text top="645" left="80" width="498" height="14" font="4">one is targeted to perform. CPUs are designed to run complex programs as fast as</text>
<text top="663" left="80" width="497" height="14" font="4">possible. GPUs, on the other hand, are designed to run simple programs in parallel</text>
<text top="681" left="80" width="497" height="14" font="4">over a large amount of data, so they can dedicate most of its transistors to execution</text>
<text top="699" left="80" width="33" height="14" font="4">units.</text>
<text top="717" left="98" width="480" height="14" font="4">In a GPU, execution units are organised in groups that form a multiprocessor.</text>
<text top="735" left="80" width="497" height="14" font="4">All the execution units in a multiprocessor share a control unit, so they perform the</text>
<text top="753" left="80" width="203" height="14" font="4">same instruction at the same time.</text>
<text top="771" left="98" width="480" height="14" font="4">To exploit the highly parallel nature of the processors, GPU programs use ﬁne-</text>
<text top="789" left="80" width="497" height="14" font="4">grained threads or tasks. These are intimately different from the CPU threads which</text>
<text top="807" left="80" width="497" height="14" font="4">are typically used in parallel environments like OpenMP. In a GPU the strategy is to</text>
<text top="825" left="80" width="497" height="14" font="4">have many more threads than execution units by assigning each thread a very small</text>
<text top="866" left="80" width="5" height="9" font="10">1</text>
<text top="868" left="97" width="481" height="12" font="5">For marketing reasons these execution units are sometimes called “cores”, although they are not</text>
<text top="883" left="80" width="137" height="12" font="5">comparable to a CPU core.</text>
</page>
<page number="3" position="absolute" top="0" left="0" height="999" width="659">
	<fontspec id="11" size="10" family="Times" color="#131413"/>
<text top="55" left="80" width="13" height="12" font="5">21</text>
<text top="55" left="105" width="258" height="12" font="5">Harnessing the Power of Graphic Processing Units</text>
<text top="55" left="559" width="19" height="12" font="5">403</text>
<text top="90" left="80" width="146" height="12" font="11"><b>Fig. 21.1 </b>A GPU associated</text>
<text top="105" left="80" width="59" height="12" font="5">with a CPU</text>
<text top="266" left="80" width="137" height="12" font="11"><b>Table 21.1 </b>Comparison of</text>
<text top="281" left="80" width="101" height="12" font="5">typical data transfer</text>
<text top="296" left="80" width="149" height="12" font="5">capabilities of a GPU, a CPU</text>
<text top="311" left="80" width="140" height="12" font="5">and the PCI Express (PCIe)</text>
<text top="325" left="80" width="118" height="12" font="5">link that connects them</text>
<text top="273" left="247" width="25" height="12" font="5">Type</text>
<text top="273" left="370" width="64" height="12" font="5">Latency [ns]</text>
<text top="273" left="487" width="91" height="12" font="5">Bandwidth [Gb/s]</text>
<text top="293" left="247" width="70" height="12" font="5">CPU memory</text>
<text top="293" left="370" width="13" height="12" font="5">40</text>
<text top="293" left="487" width="13" height="12" font="5">30</text>
<text top="310" left="247" width="71" height="12" font="5">GPU memory</text>
<text top="310" left="370" width="19" height="12" font="5">300</text>
<text top="310" left="487" width="19" height="12" font="5">100</text>
<text top="326" left="247" width="48" height="12" font="5">PCIe link</text>
<text top="326" left="370" width="32" height="12" font="5">20000</text>
<text top="326" left="487" width="6" height="12" font="5">8</text>
<text top="375" left="80" width="497" height="14" font="4">amount of the work. As the GPU can switch threads without cost, it can hide the</text>
<text top="393" left="80" width="486" height="14" font="4">latency of the operations of one thread by processing other threads while waiting.</text>
<text top="411" left="98" width="480" height="14" font="4">Memory access is also particular to GPUs. CPUs only have one type of memory</text>
<text top="429" left="80" width="491" height="14" font="4">and rely on caches to speed up the access to it. GPUs do not always have a <a href="pdfxml.html#3">cache</a></text>
<text top="426" left="571" width="6" height="10" font="9"><a href="pdfxml.html#3">2</a></text>
<text top="447" left="80" width="498" height="14" font="4">but instead they have a fast local memory shared by all the execution units in a multi-</text>
<text top="465" left="80" width="497" height="14" font="4">processor. This local or private memory must be explicitly used by the programmer</text>
<text top="483" left="80" width="497" height="14" font="4">to store data that needs to be accessed frequently. Main memory access also need</text>
<text top="501" left="80" width="497" height="14" font="4">to be done carefully. The memory bandwidth is higher than for a CPU but latency</text>
<text top="519" left="80" width="142" height="14" font="4">is also higher, see Table</text>
<text top="519" left="224" width="26" height="14" font="8"><a href="pdfxml.html#3">21.1</a></text>
<text top="519" left="250" width="328" height="14" font="4"><a href="pdfxml.html#3">. </a>Moreover, to obtain maximum memory transfer rates,</text>
<text top="537" left="80" width="485" height="14" font="4">the execution units in a multiprocessor must access sequential memory locations.</text>
<text top="555" left="98" width="368" height="14" font="4">So we summarise the principal features of GPU computation:</text>
<text top="582" left="80" width="498" height="14" font="4">• Due to the high latency of the communication, the programmer should try to limit</text>
<text top="599" left="93" width="363" height="14" font="4">the data transfer between CPU and GPU as most as possible.</text>
<text top="617" left="80" width="497" height="14" font="4">• Calculation workload is parallelised in many little chunks, which perform the same</text>
<text top="635" left="93" width="215" height="14" font="4">kind of operations on different data.</text>
<text top="653" left="80" width="497" height="14" font="4">• Data locality is of great importance to achieve good performance, since different</text>
<text top="671" left="93" width="280" height="14" font="4">multiprocessors have different local memories.</text>
<text top="689" left="80" width="465" height="14" font="4">• Memory access patterns should be as regular and homogeneous as possible.</text>
<text top="744" left="80" width="188" height="16" font="3"><b>21.3 GPU Programming</b></text>
<text top="788" left="80" width="498" height="14" font="4">Given their special characteristics, GPUs cannot be programmed directly using tradi-</text>
<text top="806" left="80" width="497" height="14" font="4">tional serial programming languages, so a special framework is needed. The CUDA</text>
<text top="824" left="80" width="498" height="14" font="4">programming language of Nvidia was probably the ﬁrst in the market and it is,</text>
<text top="865" left="80" width="5" height="9" font="10">2</text>
<text top="867" left="97" width="481" height="12" font="5">Fortunately for programmers, newer GPUs include a small cache. This makes GPU code opti-</text>
<text top="882" left="80" width="119" height="12" font="5">misation much simpler.</text>
</page>
<page number="4" position="absolute" top="0" left="0" height="999" width="659">
	<fontspec id="12" size="15" family="Times" color="#131413"/>
<text top="55" left="80" width="19" height="12" font="5">404</text>
<text top="55" left="430" width="148" height="12" font="5">X. Andrade and L. Genovese</text>
<text top="89" left="80" width="498" height="14" font="4">nowadays, the most advanced in terms of functionalities and maturity. Nonetheless,</text>
<text top="107" left="80" width="497" height="14" font="4">being an architecture-speciﬁc language, the code is not portable and there is no</text>
<text top="125" left="80" width="147" height="14" font="4">standard associated to it.</text>
<text top="189" left="80" width="228" height="16" font="12"><i><b>21.3.1 The OpenCL Language</b></i></text>
<text top="233" left="80" width="497" height="14" font="4">To avoid the problem of having different GPU-programming frameworks speciﬁc to</text>
<text top="251" left="80" width="497" height="14" font="4">each hardware a standard was proposed: the OpenCL speciﬁcation (Munshi 2009).</text>
<text top="269" left="80" width="497" height="14" font="4">Its ambition is to open a pathway toward cross-platform parallel computing, of which</text>
<text top="287" left="80" width="498" height="14" font="4">hybrid CPU/GPU architectures are a ﬁrst example. Its speciﬁcations are similar to the</text>
<text top="305" left="80" width="491" height="14" font="4">present organisation of the CUDA language, but with some useful generalisations.</text>
<text top="322" left="98" width="480" height="14" font="4">It must be pointed out that, while OpenCL code can be executed unchanged in</text>
<text top="340" left="80" width="497" height="14" font="4">different platforms (a GPU, a CPU, or other OpenCL supported device), this does</text>
<text top="358" left="80" width="498" height="14" font="4">not necessarily mean that the code optimised for one platform will run efﬁciently in</text>
<text top="376" left="80" width="95" height="14" font="4">other platforms.</text>
<text top="440" left="80" width="480" height="16" font="12"><i><b>21.3.2 Evaluation of Beneﬁts: Performance with Complex Codes</b></i></text>
<text top="484" left="80" width="497" height="14" font="4">The peculiarities of GPU architecture are thus of paramount importance to deter-</text>
<text top="502" left="80" width="497" height="14" font="4">mine if an application can have beneﬁts from it. A GPU program will be ported</text>
<text top="520" left="80" width="497" height="14" font="4">conveniently depending of the nature of its operations. An evaluation should be</text>
<text top="538" left="80" width="498" height="14" font="4">performed to understand the trade-off between rewriting and speed-up. The situation</text>
<text top="556" left="80" width="497" height="14" font="4">is even more complicated for a complex code with many operations, which may</text>
<text top="574" left="80" width="498" height="14" font="4">work in a parallel environment. For this case, the evaluation of the beneﬁts of using a</text>
<text top="591" left="80" width="395" height="14" font="4">GPU-accelerated code must be performed at three different levels.</text>
<text top="621" left="80" width="497" height="14" font="4">• Firstly, one has to evaluate the effective speed-up provided by the GPU code with</text>
<text top="639" left="93" width="485" height="14" font="4">respect to the corresponding CPU routines which perform the same operations.</text>
<text top="657" left="93" width="166" height="14" font="4">This is the “bare” speed-up.</text>
<text top="675" left="80" width="497" height="14" font="4">• At the second level, the “complete” speed-up has to be evaluated; the perfor-</text>
<text top="693" left="93" width="485" height="14" font="4">mances of the whole hybrid CPU/GPU code should be analysed with respect to</text>
<text top="711" left="93" width="485" height="14" font="4">the pure CPU executions. Clearly, this result depends of the importance of the</text>
<text top="729" left="93" width="485" height="14" font="4">ported routines in the context of the whole code [i.e., following Amdahl’s law</text>
<text top="747" left="93" width="364" height="14" font="4">(Amdahl 1967)] and the additional cost of CPU/GPU copies.</text>
<text top="765" left="80" width="497" height="14" font="4">• For a parallel code, there is still another step which has to be evaluated. This is the</text>
<text top="783" left="93" width="485" height="14" font="4">behaviour of the hybrid code in a parallel environment. Indeed, for parallel runs</text>
<text top="801" left="93" width="485" height="14" font="4">the picture is complicated by two things. The ﬁrst one is that since GPUs can be</text>
<text top="819" left="93" width="485" height="14" font="4">faster than CPUs, the relative cost of communication increases. The second issue</text>
<text top="837" left="93" width="485" height="14" font="4">is that we might need to copy data between remote GPUs, actually increasing the</text>
<text top="854" left="93" width="127" height="14" font="4">communication time.</text>
</page>
<page number="5" position="absolute" top="0" left="0" height="999" width="659">
	<fontspec id="13" size="15" family="Times" color="#131413"/>
<text top="55" left="80" width="13" height="12" font="5">21</text>
<text top="55" left="105" width="258" height="12" font="5">Harnessing the Power of Graphic Processing Units</text>
<text top="55" left="559" width="19" height="12" font="5">405</text>
<text top="89" left="98" width="480" height="14" font="4">There are, however, some additional considerations to make. First of all, as with</text>
<text top="107" left="80" width="497" height="14" font="4">all parallel systems, on a GPU the speed up depends on the size of the studied system.</text>
<text top="125" left="80" width="498" height="14" font="4">In this context, GPUs might not provide shorter calculation times, but they could be</text>
<text top="143" left="80" width="497" height="14" font="4">useful to study more complex systems in a similar time [as modelled by Gustafson’s</text>
<text top="161" left="80" width="497" height="14" font="4">law (Gustafson 1988)]. In second place, a direct migration of a CPU code might not</text>
<text top="179" left="80" width="497" height="14" font="4">be the best strategy to obtain an efﬁcient GPU code. Sometimes restructuring the</text>
<text top="197" left="80" width="498" height="14" font="4">calculation or even using different algorithms might be necessary to fully exploit the</text>
<text top="215" left="80" width="105" height="14" font="4">GPU capabilities.</text>
<text top="279" left="80" width="250" height="16" font="3"><b>21.4 GPUs for DFT and TDDFT</b></text>
<text top="322" left="80" width="497" height="14" font="4">The advantages offered by GPU programming are of great interest for physics and</text>
<text top="340" left="80" width="497" height="14" font="4">chemistry calculations. A lot of scientiﬁc applications have been recently ported</text>
<text top="358" left="80" width="497" height="14" font="4">on GPU, including, for example, molecular dynamics (Yang et al. 2007), quantum</text>
<text top="376" left="80" width="497" height="14" font="4">Monte-Carlo (Anderson et al. 2007), and ﬁnite element methods (Göddeke et al.</text>
<text top="394" left="80" width="497" height="14" font="4">2007). In the domain of electronic structure calculations, up to now, most efforts have</text>
<text top="412" left="80" width="497" height="14" font="4">been done in the context of Quantum Chemistry for DFT (Yasuda 2008; Uﬁmtsev and</text>
<text top="430" left="80" width="497" height="14" font="4">Martinez 2009) and Moller-Plesset (Vogt et al. 2008; Watson et al. 2010) methods.</text>
<text top="448" left="80" width="498" height="14" font="4">Given the small size of the basis set, usually the amount of data to be transferred</text>
<text top="466" left="80" width="497" height="14" font="4">to the GPU is limited and these implementations may beneﬁt of the acceleration of</text>
<text top="484" left="80" width="130" height="14" font="4">arithmetic operations.</text>
<text top="502" left="98" width="480" height="14" font="4">The situation is less developed for the condensed matter physics community,</text>
<text top="520" left="80" width="497" height="14" font="4">where systematic discretization schemes are used. In this case, the number of degrees</text>
<text top="538" left="80" width="497" height="14" font="4">of freedom is order of magnitudes larger, and this implies that the amount of data to</text>
<text top="556" left="80" width="498" height="14" font="4">be treated is large. This poses a problem for GPU computation as transferring data</text>
<text top="574" left="80" width="497" height="14" font="4">into the GPU memory is more time-consuming, so care should be taken in managing</text>
<text top="591" left="80" width="97" height="14" font="4">such operations.</text>
<text top="609" left="98" width="480" height="14" font="4">In the next sections we illustrate two applications for GPUs in codes that use</text>
<text top="627" left="80" width="497" height="14" font="4">systematic discretisation strategies. First the implementation of GPU acceleration in</text>
<text top="645" left="80" width="497" height="14" font="4">the BigDFT code, where GPUs are used for ground-state DFT calculations based on</text>
<text top="663" left="80" width="497" height="14" font="4">a wavelet basis set. Next, we discuss the GPU port of the octopus code to perform</text>
<text top="681" left="80" width="427" height="14" font="4">real-time TDDFT calculations based on a real-space grid discretisation.</text>
<text top="745" left="80" width="361" height="16" font="3"><b>21.5 GPU Implementation in the </b>BigDFT <b>Code</b></text>
<text top="789" left="80" width="497" height="14" font="4">The BigDFT code implements DFT calculations based on Daubechies wavelets basis</text>
<text top="807" left="80" width="497" height="14" font="4">set (Daubechies 1992), it is distributed under GNU–GPL license and integrated in the</text>
<text top="821" left="80" width="497" height="17" font="4">abinit (Gonze et al. 2002) software package. A separate, standalone version of this</text>
<text top="842" left="80" width="497" height="14" font="4">code (including the hybrid version) is also available (Genovese et al. 2008). Thanks</text>
<text top="860" left="80" width="497" height="14" font="4">to wavelet properties, this code shows high systematic convergence properties, very</text>
<text top="878" left="80" width="323" height="14" font="4">good performance and an excellent parallel efﬁciency.</text>
</page>
<page number="6" position="absolute" top="0" left="0" height="999" width="659">
<text top="55" left="80" width="19" height="12" font="5">406</text>
<text top="55" left="430" width="148" height="12" font="5">X. Andrade and L. Genovese</text>
<text top="87" left="80" width="437" height="16" font="12"><i><b>21.5.1 The Code Structure: Preliminary CPU Investigation</b></i></text>
<text top="131" left="80" width="497" height="14" font="4">Applying the Hamiltonian operator on the KS wave-function is only one of the</text>
<text top="149" left="80" width="498" height="14" font="4">operations which are performed in a DFT code. In general, an optimisation iteration</text>
<text top="167" left="80" width="279" height="14" font="4">of a KS wave-function is organised as follows:</text>
<text top="197" left="80" width="335" height="14" font="4">1. Application of the Hamiltonian onto wave-functions,</text>
<text top="215" left="80" width="113" height="14" font="4">2. Overlap matrix,</text>
<text top="233" left="80" width="118" height="14" font="4">3. Preconditioning,</text>
<text top="251" left="80" width="154" height="14" font="4">4. Wave-function update,</text>
<text top="269" left="80" width="130" height="14" font="4">5. Orthogonalisation.</text>
<text top="299" left="80" width="497" height="14" font="4">Any of these steps is associated to a different set of operations. Steps (2) and (4)</text>
<text top="316" left="80" width="497" height="14" font="4">are essentially related to linear algebra routines (BLAS calls), whereas step (5) is in</text>
<text top="334" left="80" width="497" height="14" font="4">general implemented via LAPACK routines. These routines can be accelerated using</text>
<text top="352" left="80" width="497" height="14" font="4">GPU ported linear algebra libraries. The CUBLAS package developed by NVidia</text>
<text top="370" left="80" width="497" height="14" font="4">can be easily linked to the code. Steps (1) and (3) are associated to the application of</text>
<text top="388" left="80" width="497" height="14" font="4">different operators on the wave-functions, and from the viewpoint of GPU porting,</text>
<text top="406" left="80" width="208" height="14" font="4">they have in general to be recoded.</text>
<text top="424" left="98" width="480" height="14" font="4">The details of the BigDFT code are presented elsewhere (Genovese et al. 2008,</text>
<text top="442" left="80" width="497" height="14" font="4">2009). Most of the operators which are associated to the KS Hamiltonian are compu-</text>
<text top="460" left="80" width="497" height="14" font="4">tationally written as a combination of convolutions with short, separable ﬁlters. The</text>
<text top="478" left="80" width="497" height="14" font="4">lowest level routine which will be ported on GPU is then a set of independent, one</text>
<text top="496" left="80" width="157" height="14" font="4">dimensional convolutions.</text>
<text top="559" left="80" width="479" height="16" font="12"><i><b>21.5.2 GPU Convolution Routines and CUBLAS Linear Algebra</b></i></text>
<text top="603" left="80" width="497" height="14" font="4">We have evaluated the performances of the GPU port of the 1D convolutions and</text>
<text top="621" left="80" width="498" height="14" font="4">their 3D counterpart. For these evaluations, we used a computer with an Intel Xeon</text>
<text top="639" left="80" width="497" height="14" font="4">Processor X5472 (3.0 GHz) and a NVidia Tesla S1070 card. The CPU version of</text>
<text top="653" left="80" width="497" height="17" font="4">BigDFT is deeply optimised with optimal loop unrolling and compiler options.</text>
<text top="675" left="80" width="497" height="14" font="4">The GPU code is compiled with the Intel Fortran compiler (version 10.1.011).</text>
<text top="693" left="80" width="498" height="14" font="4">All benchmarks are performed with double precision ﬂoating point numbers. With</text>
<text top="711" left="80" width="402" height="14" font="4">these options the magic ﬁlter convolutions run at about 3.4 GFlops.</text>
<text top="729" left="98" width="480" height="14" font="4">The GPU versions of the one-dimensional convolutions are about one order of</text>
<text top="747" left="80" width="497" height="14" font="4">magnitude faster than their CPU counterparts. Since these are the building blocks of</text>
<text top="765" left="80" width="496" height="14" font="4">the 3D convolutions, this gain in performance is reﬂected also on the 3D operators.</text>
<text top="783" left="98" width="480" height="14" font="4">Also the linear algebra operations can be executed on the card thanks to the</text>
<text top="801" left="80" width="497" height="14" font="4">CUBLAS routines. We obtain speed-ups between a factor of 20 and 60 for double</text>
<text top="819" left="80" width="497" height="14" font="4">precision calls to CUBLAS routines for a typical wave-function size of a BigDFT</text>
<text top="837" left="80" width="497" height="14" font="4">run as a function of the number of orbitals. These results take into account the amount</text>
<text top="854" left="80" width="341" height="14" font="4">of time needed for transferring data to and from the card.</text>
</page>
<page number="7" position="absolute" top="0" left="0" height="999" width="659">
<text top="55" left="80" width="13" height="12" font="5">21</text>
<text top="55" left="105" width="258" height="12" font="5">Harnessing the Power of Graphic Processing Units</text>
<text top="55" left="559" width="19" height="12" font="5">407</text>
<text top="89" left="98" width="479" height="14" font="4">From these tests, we can see that both GPU-ported sections are orders of magnitude</text>
<text top="107" left="80" width="497" height="14" font="4">faster than the corresponding CPU counterpart. We now discuss the performance of</text>
<text top="125" left="80" width="112" height="14" font="4">the complete code.</text>
<text top="171" left="80" width="352" height="16" font="12"><i><b>21.5.3 Performance Evaluation of Hybrid Code</b></i></text>
<text top="215" left="80" width="497" height="14" font="4">As a test system, we used the ZnO crystal, which has a wurtzite bulk-like structure.</text>
<text top="233" left="80" width="497" height="14" font="4">Such system has a relatively high density of valence electrons so that the number of</text>
<text top="251" left="80" width="361" height="14" font="4">orbitals is rather large even for a moderate number of atoms.</text>
<text top="269" left="98" width="480" height="14" font="4">We performed two kinds of tests. The ﬁrst is related to the behavior of the</text>
<text top="287" left="80" width="451" height="14" font="4">GPU-accelerated code on a single machine. Results can be found in Fig.</text>
<text top="287" left="533" width="26" height="14" font="8"><a href="pdfxml.html#8">21.2</a></text>
<text top="287" left="559" width="18" height="14" font="4"><a href="pdfxml.html#8">. </a>It</text>
<text top="305" left="80" width="497" height="14" font="4">can be seen that GPU acceleration contributes to a signiﬁcant reduction of the over-</text>
<text top="322" left="80" width="497" height="14" font="4">head of linear algebra operations and convolutions. Both CUDA and the more recent</text>
<text top="340" left="80" width="497" height="14" font="4">OpenCL implementation of these convolutions are tested. By combining these accel-</text>
<text top="358" left="80" width="497" height="14" font="4">erations with in-node MPI parallelisation, we may achieve a speed-up up to one</text>
<text top="376" left="80" width="497" height="14" font="4">order of magnitude faster than the mono-core CPU run. The second test compares</text>
<text top="394" left="80" width="497" height="14" font="4">the behavior of the hybrid code in a multi-node machine. We use the hybrid section of</text>
<text top="412" left="80" width="497" height="14" font="4">the CCRT Titane machine, with Intel X5570 CPUs and Nvidia Tesla S1070 GPUs.</text>
<text top="430" left="80" width="497" height="14" font="4">In this test we keep the size of the system ﬁxed and increase the number of MPI</text>
<text top="448" left="80" width="497" height="14" font="4">processes such as to decrease the number of orbitals per core. We then compare the</text>
<text top="466" left="80" width="498" height="14" font="4">speed-up of each run with the hybrid code. The parallel efﬁciency of the code is</text>
<text top="484" left="80" width="497" height="14" font="4">not particularly affected by the presence of the GPU. For this machine, the time-to-</text>
<text top="502" left="80" width="202" height="14" font="4">solution speed-up is around three.</text>
<text top="548" left="80" width="395" height="16" font="3"><b>21.6 TDDFT on GPUs: Implementation in </b>octopus</text>
<text top="591" left="80" width="497" height="14" font="4">In this section we detail the approach implemented in the octopus code (Marques</text>
<text top="609" left="80" width="497" height="14" font="4">et al. 2003, Castro et al. 2006) for TDDFT calculations on GPUs. octopus is a free</text>
<text top="627" left="80" width="497" height="14" font="4">software package that implements several TDDFT formalisms. Real-time propaga-</text>
<text top="645" left="80" width="498" height="14" font="4">tion is the most used method, due to its ﬂexibility. So up to now we have focused our</text>
<text top="663" left="80" width="148" height="14" font="4">efforts in this formalism.</text>
<text top="681" left="98" width="480" height="14" font="4">The implementation is based on the idea of using blocks of KS orbitals.</text>
<text top="699" left="80" width="497" height="14" font="4">We use the GPU to apply the KS Hamiltonian over these blocks, obtaining important</text>
<text top="717" left="80" width="497" height="14" font="4">performance gains not only for time propagation but also for ground-state DFT and</text>
<text top="735" left="80" width="497" height="14" font="4">Sternheimer calculations (Andrade et al. 2007). The scheme is also applied for code</text>
<text top="753" left="80" width="304" height="14" font="4">optimisation on CPUs with vectorial ﬂoating units.</text>
<text top="771" left="98" width="480" height="14" font="4">The GPU implementation is based on the OpenCL framework and it has been</text>
<text top="789" left="80" width="497" height="14" font="4">tested on Nvidia and AMD implementations. However, for the moment the code has</text>
<text top="807" left="80" width="497" height="14" font="4">only been optimized for Nvidia cards. The CPU vectorial code is designed for x86</text>
<text top="825" left="80" width="497" height="14" font="4">CPUs with SSE2 instructions and the IBM Blue Gene/P architecture, using compiler</text>
<text top="843" left="80" width="497" height="14" font="4">directives. The code is included in the current development version of octopus and</text>
<text top="860" left="80" width="224" height="14" font="4">it will be included in the next release.</text>
</page>
<page number="8" position="absolute" top="0" left="0" height="999" width="659">
	<fontspec id="14" size="5" family="Times" color="#040303"/>
	<fontspec id="15" size="5" family="Times" color="#040303"/>
	<fontspec id="16" size="7" family="Times" color="#040303"/>
	<fontspec id="17" size="7" family="Times" color="#040303"/>
	<fontspec id="18" size="6" family="Times" color="#040303"/>
<text top="55" left="80" width="19" height="12" font="5">408</text>
<text top="55" left="430" width="148" height="12" font="5">X. Andrade and L. Genovese</text>
<text top="306" left="104" width="7" height="7" font="14"> 0</text>
<text top="265" left="100" width="11" height="7" font="14"> 20</text>
<text top="223" left="100" width="11" height="7" font="14"> 40</text>
<text top="182" left="100" width="11" height="7" font="14"> 60</text>
<text top="141" left="100" width="11" height="7" font="14"> 80</text>
<text top="100" left="96" width="15" height="7" font="14"> 100</text>
<text top="311" left="146" width="22" height="7" font="15">CPU-mkl</text>
<text top="311" left="176" width="46" height="7" font="15">CPU-mkl-mpiCUDA</text>
<text top="311" left="236" width="82" height="7" font="15">CUDA-mkl OCL-cublasOCL-mkl</text>
<text top="311" left="327" width="67" height="7" font="15">CUDA-mpi CUDA-mkl-mpi</text>
<text top="311" left="387" width="40" height="7" font="15">OCL-cublas-mpi</text>
<text top="311" left="417" width="33" height="7" font="15">OCL-mkl-mpi</text>
<text top="306" left="454" width="7" height="7" font="14"> 0</text>
<text top="268" left="454" width="7" height="7" font="14"> 2</text>
<text top="231" left="454" width="7" height="7" font="14"> 4</text>
<text top="193" left="454" width="7" height="7" font="14"> 6</text>
<text top="156" left="454" width="7" height="7" font="14"> 8</text>
<text top="118" left="454" width="11" height="7" font="14"> 10</text>
<text top="213" left="93" width="0" height="9" font="16">Percent</text>
<text top="215" left="479" width="0" height="9" font="16">Speedup</text>
<text top="86" left="147" width="263" height="9" font="17">Badiane, X5550 + Fermi S2070 , ZnO 64 at.: CPU vs. Hybrid</text>
<text top="303" left="521" width="30" height="8" font="18">Comms</text>
<text top="294" left="521" width="24" height="8" font="18">LinAlg</text>
<text top="286" left="521" width="20" height="8" font="18">Conv</text>
<text top="277" left="521" width="18" height="8" font="18">CPU</text>
<text top="268" left="521" width="22" height="8" font="18">Other</text>
<text top="260" left="521" width="35" height="8" font="18">Speedup</text>
<text top="646" left="102" width="7" height="8" font="18"> 0</text>
<text top="595" left="97" width="12" height="8" font="18"> 20</text>
<text top="545" left="97" width="12" height="8" font="18"> 40</text>
<text top="495" left="97" width="12" height="8" font="18"> 60</text>
<text top="445" left="97" width="12" height="8" font="18"> 80</text>
<text top="394" left="93" width="16" height="8" font="18"> 100</text>
<text top="653" left="127" width="5" height="8" font="18">1</text>
<text top="653" left="140" width="5" height="8" font="18">2</text>
<text top="653" left="152" width="5" height="8" font="18">4</text>
<text top="653" left="165" width="5" height="8" font="18">6</text>
<text top="653" left="178" width="100" height="8" font="18">8 12 24 32 48 64 96 144</text>
<text top="653" left="295" width="5" height="8" font="18">1</text>
<text top="653" left="308" width="5" height="8" font="18">2</text>
<text top="653" left="321" width="5" height="8" font="18">4</text>
<text top="653" left="334" width="5" height="8" font="18">6</text>
<text top="653" left="347" width="100" height="8" font="18">8 12 24 32 48 64 96 144</text>
<text top="646" left="458" width="7" height="8" font="18"> 0</text>
<text top="610" left="458" width="7" height="8" font="18"> 1</text>
<text top="574" left="458" width="7" height="8" font="18"> 2</text>
<text top="538" left="458" width="7" height="8" font="18"> 3</text>
<text top="502" left="458" width="7" height="8" font="18"> 4</text>
<text top="466" left="458" width="7" height="8" font="18"> 5</text>
<text top="430" left="458" width="7" height="8" font="18"> 6</text>
<text top="394" left="458" width="7" height="8" font="18"> 7</text>
<text top="531" left="92" width="0" height="9" font="16">Percent</text>
<text top="552" left="480" width="0" height="9" font="16">Speedup with GPU</text>
<text top="668" left="255" width="68" height="9" font="17">No. of MPI proc</text>
<text top="381" left="211" width="151" height="9" font="17">Titane, ZnO 64 at.: CPU vs. Hybrid</text>
<text top="643" left="517" width="32" height="9" font="17">Comms</text>
<text top="634" left="517" width="26" height="9" font="17">LinAlg</text>
<text top="625" left="517" width="22" height="9" font="17">Conv</text>
<text top="616" left="517" width="19" height="9" font="17">CPU</text>
<text top="606" left="517" width="23" height="9" font="17">Other</text>
<text top="597" left="517" width="56" height="9" font="17">Efficiency (%)</text>
<text top="579" left="517" width="37" height="9" font="17">Speedup</text>
<text top="665" left="345" width="64" height="8" font="18">Hybrid code (rel.)</text>
<text top="665" left="178" width="38" height="8" font="18">CPU code</text>
<text top="695" left="80" width="498" height="12" font="11"><b>Fig. 21.2 </b>Performances of a run of the BigDFTcode for a 64 atoms ZnO system. Different accel-</text>
<text top="710" left="80" width="497" height="12" font="5">eration strategies are compared with respect to the time spent for a sequential pure CPU run. Two</text>
<text top="725" left="80" width="497" height="12" font="5">different platforms were used for this test; in the ﬁrst run (<i>top panel</i>), an Intel X5550 quad-core is</text>
<text top="740" left="80" width="497" height="12" font="5">associated to a Fermi S2070 card, with CUDA 3.2. Both CUDA and OpenCL (OCL) implemen-</text>
<text top="755" left="80" width="497" height="12" font="5">tations are tested. In the <i>bottom panel</i>, the system have been tested with an increasing number of</text>
<text top="770" left="80" width="497" height="12" font="5">processors (Intel X5570 2.93 GHz). The scaling efﬁciency of the calculation is also indicated. In</text>
<text top="785" left="80" width="497" height="12" font="5">the right side of bottom panel, the same calculation has been done using one Tesla S1070 card per</text>
<text top="800" left="80" width="165" height="12" font="5">CPU core, for both architectures</text>
</page>
<page number="9" position="absolute" top="0" left="0" height="999" width="659">
	<fontspec id="19" size="13" family="Helvetica" color="#131413"/>
	<fontspec id="20" size="12" family="Times" color="#131413"/>
<text top="55" left="80" width="13" height="12" font="5">21</text>
<text top="55" left="105" width="258" height="12" font="5">Harnessing the Power of Graphic Processing Units</text>
<text top="55" left="559" width="19" height="12" font="5">409</text>
<text top="108" left="117" width="20" height="15" font="19"><b>(a)</b></text>
<text top="202" left="116" width="20" height="15" font="19"><b>(b)</b></text>
<text top="256" left="80" width="498" height="12" font="11"><b>Fig. 21.3 </b>Example of memory layout for a block of four orbitals with ﬁve coefﬁcients each: <b>a</b></text>
<text top="271" left="80" width="497" height="12" font="5">Standard memory layout where each orbital is contiguous in memory. <b>b </b>Optimum memory where</text>
<text top="286" left="80" width="497" height="12" font="5">all the coefﬁcients in a block are contiguous. The arrows indicate the relation of the position of the</text>
<text top="301" left="80" width="161" height="12" font="5">ﬁrst coefﬁcient in both schemes</text>
<text top="348" left="80" width="384" height="16" font="12"><i><b>21.6.1 Working with Blocks of Kohn-Sham Orbitals</b></i></text>
<text top="391" left="80" width="497" height="14" font="4">The key to obtain good performance in a GPU is to apply the same instructions over</text>
<text top="409" left="80" width="497" height="14" font="4">several streams of independent data. In TDDFT, the operations to be performed to</text>
<text top="427" left="80" width="497" height="14" font="4">each orbital are exactly the same and fully independent. In this way, we can obtain</text>
<text top="445" left="80" width="497" height="14" font="4">these independent data-streams by working with a group of KS orbitals, that we call</text>
<text top="463" left="80" width="497" height="14" font="4">a block. This idea is the central strategy for the GPU porting of octopus, but the</text>
<text top="481" left="80" width="497" height="14" font="4">scheme is quite general. It does not depend on the type representation or basis set</text>
<text top="499" left="80" width="498" height="14" font="4">used, as long as there are operations to be applied simultaneously to several orbitals.</text>
<text top="517" left="80" width="498" height="14" font="4">It is general also in the sense that it can be a good code optimisation strategy not</text>
<text top="535" left="80" width="205" height="14" font="4">only for GPUs, but also for CPUs.</text>
<text top="553" left="98" width="480" height="14" font="4">For optimal execution, the number of orbitals in the block, or <i>block size</i>, needs to</text>
<text top="571" left="80" width="497" height="14" font="4">be compatible (an exact divisor or multiple) with the number of execution units in</text>
<text top="589" left="80" width="497" height="14" font="4">each multiprocessor of the GPU (typically 16, 32 or 64 in modern GPUs). So powers</text>
<text top="607" left="80" width="151" height="14" font="4">of two are a good <a href="pdfxml.html#9">choice.</a></text>
<text top="604" left="231" width="6" height="10" font="9"><a href="pdfxml.html#9">3</a></text>
<text top="625" left="98" width="480" height="14" font="4">Working with blocks of orbitals also allows to optimise memory accesses.</text>
<text top="643" left="80" width="497" height="14" font="4">However, choosing an appropriate memory layout is crucial. The order used in</text>
<text top="657" left="80" width="497" height="17" font="4">octopus, and many other DFT codes, is to store contiguously in memory each</text>
<text top="678" left="80" width="136" height="14" font="4">orbital, as seen in Fig.</text>
<text top="678" left="219" width="26" height="14" font="8"><a href="pdfxml.html#9">21.3</a></text>
<text top="678" left="245" width="333" height="14" font="4"><a href="pdfxml.html#9">a. </a>This is not optimal since values for the same coefﬁ-</text>
<text top="696" left="80" width="497" height="14" font="4">cient of different orbitals are scattered. The optimal memory arrangement is to have</text>
<text top="714" left="80" width="452" height="14" font="4">all coefﬁcients together, effectively transposing the order of the array, Fig.</text>
<text top="714" left="535" width="26" height="14" font="8"><a href="pdfxml.html#9">21.3</a></text>
<text top="714" left="561" width="11" height="14" font="4"><a href="pdfxml.html#9">b.</a></text>
<text top="711" left="571" width="6" height="10" font="9"><a href="pdfxml.html#9">4</a></text>
<text top="732" left="80" width="498" height="14" font="4">In this manner the GPU can maximise memory accesses since tasks read or write</text>
<text top="750" left="80" width="497" height="14" font="4">sequential memory locations. The transposition is done by the GPU after copying</text>
<text top="768" left="80" width="497" height="14" font="4">to GPU memory and undone before copying back to main memory. The cost of the</text>
<text top="786" left="80" width="397" height="14" font="4">transposition is negligible in comparison with the cost of the copy.</text>
<text top="830" left="80" width="5" height="9" font="10">3</text>
<text top="832" left="97" width="481" height="12" font="5">This has the additional advantage that integer multiplications by the block size can be done using</text>
<text top="847" left="80" width="180" height="12" font="5">the much faster bit shift operations.</text>
<text top="863" left="80" width="5" height="9" font="10">4</text>
<text top="865" left="97" width="481" height="12" font="5">Note that it is the block that is transposed with respect to the standard ordering, not the whole</text>
<text top="880" left="80" width="72" height="12" font="5">set of orbitals.</text>
</page>
<page number="10" position="absolute" top="0" left="0" height="999" width="659">
<text top="55" left="80" width="19" height="12" font="5">410</text>
<text top="55" left="430" width="148" height="12" font="5">X. Andrade and L. Genovese</text>
<text top="87" left="80" width="378" height="16" font="12"><i><b>21.6.2 Application of the Kohn-Sham Hamiltonian</b></i></text>
<text top="131" left="80" width="497" height="14" font="4">Since for real-time TDDFT the work to be done to each orbital is independent of</text>
<text top="149" left="80" width="497" height="14" font="4">the other orbitals, we can apply simultaneously the KS Hamiltonian over a block of</text>
<text top="167" left="80" width="497" height="14" font="4">orbitals. This is not always the case for ground-state DFT. In some algorithms, due</text>
<text top="185" left="80" width="497" height="14" font="4">to orthogonality constraints the calculation of the ground-state orbitals is sequential.</text>
<text top="203" left="80" width="497" height="14" font="4">There are, however, eigensolvers based on the idea of direct minimisation (Pulay</text>
<text top="221" left="80" width="299" height="14" font="4">1980) that are suitable for this type of parallelism.</text>
<text top="239" left="98" width="480" height="14" font="4">In octopus the orbitals are not always available in GPU memory, they must be</text>
<text top="257" left="80" width="497" height="14" font="4">copied before the calculation, and the result copied back to main memory. Since</text>
<text top="275" left="80" width="497" height="14" font="4">these copies are costly they are avoided as much as possible by keeping data in</text>
<text top="293" left="80" width="497" height="14" font="4">GPU memory if the Hamiltonian is going to be applied again. For example, in real-</text>
<text top="311" left="80" width="497" height="14" font="4">time TDDFT, for the application of the exponential of the Hamiltonian in the Taylor</text>
<text top="328" left="80" width="498" height="14" font="4">approximation (Castro et al. 2004a) only the initial orbitals and the resulting ones</text>
<text top="346" left="80" width="460" height="14" font="4">need to be copied, while the intermediate results are passed in GPU memory.</text>
<text top="364" left="98" width="480" height="14" font="4">To fully apply the Hamiltonian using the GPU in a real space code we need to</text>
<text top="382" left="80" width="497" height="14" font="4">calculate the action of the potential, and the kinetic energy operator. The kinetic</text>
<text top="400" left="80" width="497" height="14" font="4">part is the most important one and is discussed in the next section. The local part</text>
<text top="418" left="80" width="498" height="14" font="4">of the potential is in general very simple to apply, since in real space it is only a</text>
<text top="436" left="80" width="497" height="14" font="4">multiplication of two arrays. The non-local part of the potential, that appears when</text>
<text top="454" left="80" width="497" height="14" font="4">pseudo-potentials are used, is more interesting. In order to achieve a level of paral-</text>
<text top="472" left="80" width="497" height="14" font="4">lelism suitable for the GPU, the projectors corresponding to all atoms must be applied</text>
<text top="490" left="80" width="93" height="14" font="4">simultaneously.</text>
<text top="553" left="80" width="376" height="16" font="12"><i><b>21.6.3 The Kinetic Energy Operator in Real-Space</b></i></text>
<text top="594" left="80" width="497" height="17" font="4">octopus is based on a real-space grid discretisation. The basis for this method is the</text>
<text top="615" left="80" width="497" height="14" font="4">approximation of the Laplacian in the kinetic energy operator by high-order ﬁnite</text>
<text top="633" left="80" width="497" height="14" font="4">differences (Chelikowsky et al. 1994). This is the most time-consuming part, so it is</text>
<text top="651" left="80" width="498" height="14" font="4">essential to apply it as efﬁciently as possible. Memory access is a delicate issue in</text>
<text top="669" left="80" width="497" height="14" font="4">ﬁnite difference operators, since each point is used several times. In octopus, the</text>
<text top="687" left="80" width="497" height="14" font="4">order in which grid values are stored in memory is chosen such that points that are</text>
<text top="705" left="80" width="497" height="14" font="4">neighbours are close. The result is that memory accesses have a good locality and</text>
<text top="723" left="80" width="184" height="14" font="4">can proﬁt from cache memory.</text>
<text top="741" left="98" width="480" height="14" font="4">We can see how critical blocks are to realise the performance potential of the GPU</text>
<text top="759" left="80" width="38" height="14" font="4">in Fig.</text>
<text top="759" left="120" width="26" height="14" font="8"><a href="pdfxml.html#11">21.4</a></text>
<text top="759" left="146" width="431" height="14" font="4"><a href="pdfxml.html#11">a </a>. It shows the throughput obtained in a GPU and a CPU for the Laplacian</text>
<text top="777" left="80" width="497" height="14" font="4">for different block sizes. In the GPU, by using blocks we can obtain speed-ups of 5</text>
<text top="795" left="80" width="497" height="14" font="4">with respect to the case of working with a single orbital. And, while for a block size</text>
<text top="813" left="80" width="497" height="14" font="4">of 1 the GPU is only 1.4 times faster than a CPU, for a block size of 32 the GPU</text>
<text top="831" left="80" width="497" height="14" font="4">is almost 4.5 times faster. Due to the limited cache size, increasing the block size</text>
<text top="848" left="80" width="389" height="14" font="4">beyond 32 orbitals decreases the performance in both processors.</text>
</page>
<page number="11" position="absolute" top="0" left="0" height="999" width="659">
	<fontspec id="21" size="6" family="Times" color="#131413"/>
	<fontspec id="22" size="7" family="Times" color="#131413"/>
	<fontspec id="23" size="12" family="Helvetica" color="#131413"/>
	<fontspec id="24" size="8" family="Times" color="#131413"/>
<text top="55" left="80" width="13" height="12" font="5">21</text>
<text top="55" left="105" width="258" height="12" font="5">Harnessing the Power of Graphic Processing Units</text>
<text top="55" left="559" width="19" height="12" font="5">411</text>
<text top="233" left="212" width="12" height="8" font="21">  16</text>
<text top="233" left="240" width="8" height="8" font="21">32</text>
<text top="233" left="267" width="8" height="8" font="21">64</text>
<text top="239" left="283" width="39" height="8" font="21">128             </text>
<text top="244" left="203" width="40" height="9" font="10">Block size</text>
<text top="224" left="113" width="4" height="8" font="21">0</text>
<text top="210" left="109" width="8" height="8" font="21">10</text>
<text top="194" left="109" width="8" height="8" font="21">20</text>
<text top="179" left="109" width="8" height="8" font="21">30</text>
<text top="164" left="109" width="8" height="8" font="21">40</text>
<text top="148" left="109" width="8" height="8" font="21">50</text>
<text top="133" left="109" width="8" height="8" font="21">60</text>
<text top="119" left="109" width="8" height="8" font="21">70</text>
<text top="102" left="109" width="8" height="8" font="21">80</text>
<text top="87" left="109" width="8" height="8" font="21">90</text>
<text top="188" left="97" width="0" height="9" font="22">Thro</text>
<text top="169" left="97" width="0" height="9" font="22">ug</text>
<text top="159" left="97" width="0" height="9" font="22">hp</text>
<text top="150" left="97" width="0" height="9" font="22">u</text>
<text top="145" left="97" width="0" height="9" font="22">t [GFlops]</text>
<text top="101" left="145" width="77" height="8" font="21">CPU: Intel Core i7 975</text>
<text top="111" left="145" width="76" height="8" font="21">GPU: Nvidia GTX 480</text>
<text top="233" left="464" width="8" height="8" font="21">16</text>
<text top="233" left="489" width="8" height="8" font="21">32</text>
<text top="233" left="514" width="8" height="8" font="21">64</text>
<text top="233" left="537" width="12" height="8" font="21">128</text>
<text top="233" left="561" width="12" height="8" font="21">256</text>
<text top="243" left="452" width="40" height="9" font="10">Block size</text>
<text top="224" left="361" width="4" height="8" font="21">0</text>
<text top="190" left="361" width="4" height="8" font="21">5</text>
<text top="155" left="358" width="8" height="8" font="21">10</text>
<text top="121" left="359" width="8" height="8" font="21">15</text>
<text top="87" left="358" width="8" height="8" font="21">20</text>
<text top="191" left="347" width="0" height="9" font="22">Thro</text>
<text top="173" left="347" width="0" height="9" font="22">ug</text>
<text top="163" left="347" width="0" height="9" font="22">hp</text>
<text top="154" left="347" width="0" height="9" font="22">u</text>
<text top="149" left="347" width="0" height="9" font="22">t [GFlops]</text>
<text top="102" left="391" width="76" height="8" font="21">CPU: Intel Core i7 975</text>
<text top="111" left="391" width="76" height="8" font="21">GPU: Nvidia GTX 480</text>
<text top="233" left="120" width="4" height="8" font="21">1</text>
<text top="233" left="145" width="4" height="8" font="21">2</text>
<text top="233" left="169" width="4" height="8" font="21">4</text>
<text top="233" left="194" width="4" height="8" font="21">8</text>
<text top="233" left="285" width="41" height="8" font="21">             256 </text>
<text top="233" left="369" width="4" height="8" font="21">1</text>
<text top="233" left="394" width="4" height="8" font="21">2</text>
<text top="233" left="418" width="4" height="8" font="21">4</text>
<text top="233" left="443" width="4" height="8" font="21">8</text>
<text top="90" left="85" width="18" height="14" font="23"><b>(a)</b></text>
<text top="91" left="334" width="19" height="14" font="23"><b>(b)</b></text>
<text top="271" left="80" width="498" height="12" font="11"><b>Fig. 21.4 </b>Comparison of the throughput of octopus for different block sizes. <b>a </b>Comparison of</text>
<text top="286" left="80" width="401" height="12" font="5">the throughput of the Laplacian <b>a </b>Execution of a real-time propagation for a C</text>
<text top="290" left="482" width="10" height="9" font="10">60</text>
<text top="286" left="496" width="82" height="12" font="5">molecule. GPU:</text>
<text top="301" left="80" width="371" height="12" font="5">Nvidia GTX 480. CPU: Intel Core i7 975, 3.33 GHz running four threads</text>
<text top="350" left="116" width="156" height="10" font="24">Copy to/from GPU memory: 30.3%</text>
<text top="351" left="346" width="169" height="10" font="24">Calculation of the KS potential: 14.1%</text>
<text top="383" left="346" width="196" height="10" font="24">Other operations executed on the CPU: 5.3%</text>
<text top="413" left="346" width="177" height="10" font="24">Operations executed on the GPU: 50.3%</text>
<text top="451" left="80" width="434" height="12" font="11"><b>Fig. 21.5 </b>Breakdown of the execution of a real-time propagation of octopus of a C</text>
<text top="455" left="514" width="10" height="9" font="10">60</text>
<text top="451" left="528" width="50" height="12" font="5">molecule.</text>
<text top="466" left="80" width="410" height="12" font="5">Execution on a Nvidia GTX 480 GPU and a Core i7 975 CPU using four threads</text>
<text top="516" left="80" width="316" height="16" font="12"><i><b>21.6.4 Overall Performance Improvements</b></i></text>
<text top="560" left="80" width="41" height="14" font="4">In Fig.</text>
<text top="560" left="123" width="26" height="14" font="8"><a href="pdfxml.html#11">21.4</a></text>
<text top="560" left="149" width="428" height="14" font="4"><a href="pdfxml.html#11">b </a>we present the throughput obtained for a real-time propagation with</text>
<text top="574" left="80" width="497" height="17" font="4">octopus performed with a GPU and a CPU. For this case the GPU is three times</text>
<text top="596" left="80" width="497" height="14" font="4">faster than the CPU. This represents a smaller speed-up than the one of the individual</text>
<text top="614" left="80" width="498" height="14" font="4">components. There are two reasons for this. First, copies between CPU and GPU</text>
<text top="632" left="80" width="497" height="14" font="4">memory consume a considerable amount of time. Second, the parts of the code that</text>
<text top="650" left="80" width="447" height="14" font="4">are executed on the CPU limit the speed-up that can be obtained. In Fig.</text>
<text top="650" left="529" width="26" height="14" font="8"><a href="pdfxml.html#11">21.5</a></text>
<text top="650" left="560" width="17" height="14" font="4">we</text>
<text top="668" left="80" width="497" height="14" font="4">show how the time is spent during the execution of octopus on a GPU. Roughly</text>
<text top="686" left="80" width="497" height="14" font="4">half of the time is used executing GPU code, while 30% is spent in GPU/CPU copies.</text>
<text top="704" left="80" width="435" height="14" font="4">The remaining 20% corresponds to tasks that are performed by the CPU.</text>
<text top="758" left="80" width="288" height="16" font="3"><b>21.7 Future Developments in TDDFT</b></text>
<text top="802" left="80" width="497" height="14" font="4">Up to now most of the applications of GPU computing to electronic structure prob-</text>
<text top="820" left="80" width="497" height="14" font="4">lems have been centred around ground state DFT methods. In many cases these</text>
<text top="838" left="80" width="497" height="14" font="4">developments can be directly applied to TDDFT formalisms, specially for methods</text>
<text top="856" left="80" width="497" height="14" font="4">based on the direct representation of the Hamiltonian instead of the spectral represen-</text>
<text top="874" left="80" width="498" height="14" font="4">tation (in terms of unoccupied orbitals). There are, however, approaches in TDDFT</text>
</page>
<page number="12" position="absolute" top="0" left="0" height="999" width="659">
	<fontspec id="25" size="9" family="Times" color="#131413"/>
	<fontspec id="26" size="9" family="Times" color="#131413"/>
	<fontspec id="27" size="12" family="Times" color="#131413"/>
	<fontspec id="28" size="12" family="Helvetica" color="#131413"/>
<text top="55" left="80" width="19" height="12" font="5">412</text>
<text top="55" left="430" width="148" height="12" font="5">X. Andrade and L. Genovese</text>
<text top="89" left="80" width="497" height="14" font="4">that use different types of operations from ground state DFT, and whose application</text>
<text top="107" left="80" width="240" height="14" font="4">to GPU computing must still be studied.</text>
<text top="125" left="98" width="480" height="14" font="4">In the Casida linear response formalism (Casida 1996), the main operation to be</text>
<text top="143" left="80" width="369" height="14" font="4">performed is the calculation of Coulomb integrals of the form</text>
<text top="188" left="192" width="10" height="14" font="20"><i>K</i></text>
<text top="193" left="203" width="18" height="10" font="25"><i>i j kl</i></text>
<text top="185" left="226" width="12" height="19" font="4">=</text>
<text top="188" left="259" width="7" height="14" font="4">d</text>
<text top="185" left="267" width="6" height="10" font="26">3</text>
<text top="188" left="273" width="6" height="14" font="20"><i>r</i></text>
<text top="188" left="300" width="7" height="14" font="4">d</text>
<text top="185" left="308" width="6" height="10" font="26">3</text>
<text top="188" left="314" width="6" height="14" font="20"><i>r</i></text>
<text top="171" left="327" width="8" height="19" font="4">ϕ</text>
<text top="169" left="336" width="6" height="15" font="26">∗</text>
<text top="182" left="335" width="3" height="10" font="25"><i>i</i></text>
<text top="171" left="342" width="27" height="19" font="4">(<i><b>r</b></i>)ϕ</text>
<text top="169" left="370" width="6" height="15" font="26">∗</text>
<text top="182" left="371" width="3" height="10" font="25"><i>j</i></text>
<text top="171" left="377" width="27" height="19" font="4">(<i><b>r</b></i>)ϕ</text>
<text top="180" left="404" width="5" height="10" font="25"><i>k</i></text>
<text top="171" left="411" width="27" height="19" font="4">(<i><b>r</b></i>)ϕ</text>
<text top="180" left="438" width="3" height="10" font="25"><i>l</i></text>
<text top="171" left="442" width="19" height="19" font="4">(<i><b>r</b></i>)</text>
<text top="195" left="371" width="46" height="20" font="4">|<i><b>r </b></i>− <i><b>r </b></i>|</text>
<text top="185" left="463" width="4" height="19" font="4">.</text>
<text top="188" left="542" width="36" height="14" font="4">(21.1)</text>
<text top="230" left="98" width="480" height="14" font="4">When using Gaussian type orbitals, these integrals can be calculated in analytic</text>
<text top="247" left="80" width="498" height="14" font="4">form. In this context, Uﬁmtsev and Martinez proposed a method to calculate the</text>
<text top="265" left="80" width="497" height="14" font="4">analogous integrals that appear in HF using GPUs (Uﬁmtsev and Martinez 2009). A</text>
<text top="283" left="80" width="497" height="14" font="4">key point of their approach is to re-order the integrals by the type of orbital, avoiding</text>
<text top="301" left="80" width="498" height="14" font="4">branches that can degrade performance in GPU code. Up to now, their method is</text>
<text top="319" left="80" width="497" height="14" font="4">only valid for <i>s </i>and <i>p </i>orbitals. This limits the applicability to TDDFT, since a proper</text>
<text top="337" left="80" width="497" height="14" font="4">description of the excited states using Gaussian type orbitals requires higher angular</text>
<text top="355" left="80" width="148" height="14" font="4">momentum components.</text>
<text top="373" left="98" width="271" height="14" font="4">In other discretizations, the evaluation of Eq.</text>
<text top="373" left="373" width="26" height="14" font="8"><a href="pdfxml.html#12">22.1</a></text>
<text top="373" left="403" width="174" height="14" font="4">is done by calculating one of</text>
<text top="391" left="80" width="497" height="14" font="4">the integrals as the solution of a Poisson equation. There exist many Poisson solvers</text>
<text top="409" left="80" width="497" height="14" font="4">for different boundary conditions [see e.g. (Genovese et al. 2006, 2007)] that can be</text>
<text top="427" left="80" width="497" height="14" font="4">applied to electronic structure problems. In other areas, some Poisson solvers have</text>
<text top="445" left="80" width="497" height="14" font="4">already been applied using GPUs (Bolz et al. 2005, Shi et al. 2009, Jeschke et al.</text>
<text top="463" left="80" width="219" height="14" font="4">2009, Grossauer and Thoman 2008).</text>
<text top="481" left="98" width="480" height="14" font="4">For ground state DFT, a single Poisson solution is required per iteration. While</text>
<text top="498" left="80" width="497" height="14" font="4">in a scheme like Casida (or HF), several Poisson solutions are required and they can</text>
<text top="516" left="80" width="498" height="14" font="4">be calculated simultaneously. This gives the possibility of using a scheme of blocks</text>
<text top="534" left="80" width="224" height="14" font="4">to exploit the parallelism of the GPU.</text>
<text top="552" left="98" width="480" height="14" font="4">Another common approach for TDDFT, specially for periodic systems, is to</text>
<text top="570" left="80" width="498" height="14" font="4">calculate the response functions using perturbation theory. This is usually done in a</text>
<text top="588" left="80" width="228" height="14" font="4">plane wave basis [as, for example, the</text>
<text top="588" left="312" width="45" height="14" font="28">yambo</text>
<text top="588" left="361" width="217" height="14" font="4">code does (Marini et al. 2009)]. The</text>
<text top="606" left="80" width="498" height="14" font="4">most time consuming operations in this type of calculations are fast Fourier trans-</text>
<text top="624" left="80" width="497" height="14" font="4">forms and dense matrix linear algebra. These operations are common and standard</text>
<text top="642" left="80" width="497" height="14" font="4">high-performance libraries exist or should be available soon for GPU architectures.</text>
<text top="660" left="80" width="497" height="14" font="4">However, since these calculations require large quantities of memory, it is unlikely</text>
<text top="678" left="80" width="497" height="14" font="4">that they can ﬁt the whole data-set in the limited memory of a GPU. So, the main</text>
<text top="696" left="80" width="497" height="14" font="4">challenge for these approaches will come from efﬁciently managing CPU/GPU copy</text>
<text top="714" left="80" width="66" height="14" font="4">operations.</text>
<text top="777" left="80" width="132" height="16" font="3"><b>21.8 Conclusions</b></text>
<text top="821" left="80" width="498" height="14" font="4">The process of porting a code to a hybrid CPU/GPU architecture requires some effort</text>
<text top="839" left="80" width="497" height="14" font="4">to learn the GPU programming techniques and to write and optimise the GPU code.</text>
<text top="857" left="80" width="497" height="14" font="4">This process however, should only affect routines that take care of well deﬁned oper-</text>
<text top="875" left="80" width="497" height="14" font="4">ations like the application of the Hamiltonian, the solution of the Poisson equation,</text>
</page>
<page number="13" position="absolute" top="0" left="0" height="999" width="659">
<text top="55" left="80" width="13" height="12" font="5">21</text>
<text top="55" left="105" width="258" height="12" font="5">Harnessing the Power of Graphic Processing Units</text>
<text top="55" left="559" width="19" height="12" font="5">413</text>
<text top="89" left="80" width="497" height="14" font="4">or orthogonalisation. Once these “building blocks” are ported to a GPU architecture,</text>
<text top="107" left="80" width="497" height="14" font="4">the different types of calculations that use them do not need to be modiﬁed if a proper</text>
<text top="125" left="80" width="497" height="14" font="4">abstraction layer is used. High performance GPU libraries could be of great help in</text>
<text top="143" left="80" width="117" height="14" font="4">the porting process.</text>
<text top="161" left="98" width="480" height="14" font="4">Currently, one of the main challenges of writing an efﬁcient GPU code is to deal</text>
<text top="179" left="80" width="497" height="14" font="4">with the problem of copying data between main memory and GPU memory over a</text>
<text top="197" left="80" width="497" height="14" font="4">relatively slow link. Unlike other previous GPU limitations, like single precision or</text>
<text top="215" left="80" width="497" height="14" font="4">the lack of cache, it is not clear that this issue will be solved completely by GPU</text>
<text top="233" left="80" width="497" height="14" font="4">vendors in the near future. While CPU chips will soon include a GPU that shares</text>
<text top="251" left="80" width="497" height="14" font="4">the same memory, it will most likely correspond to a slow GPU not suitable for</text>
<text top="269" left="80" width="497" height="14" font="4">HPC. Fast GPUs require higher memory bandwidths than what CPU memory can</text>
<text top="287" left="80" width="497" height="14" font="4">provide, so in the foreseeable future they will still have their own memory banks.</text>
<text top="305" left="80" width="497" height="14" font="4">However, we will see increments in the speed of the CPU/GPU link and perhaps</text>
<text top="322" left="80" width="365" height="14" font="4">other improvements, like direct MPI access to GPU memory.</text>
<text top="340" left="98" width="480" height="14" font="4">Programmable GPUs, and the tools to write programs for them, have only been</text>
<text top="358" left="80" width="497" height="14" font="4">available for a short time. So only a few DFT or TDDFT GPU codes are available,</text>
<text top="376" left="80" width="497" height="14" font="4">and the application to different types of calculations still needs to be investigated.</text>
<text top="394" left="80" width="497" height="14" font="4">It is clear, though, that the use of hybrid CPU/GPU architectures is an efﬁcient way</text>
<text top="412" left="80" width="497" height="14" font="4">to harness high amounts of computing power and that is suitable for TDDFT. For</text>
<text top="430" left="80" width="497" height="14" font="4">the two applications shown, the calculation time is reduced by around a factor of</text>
<text top="448" left="80" width="497" height="14" font="4">three with respect to an optimised multi-core CPU calculation. This factor could</text>
<text top="466" left="80" width="497" height="14" font="4">be increased still, since reported performances are far from the GPU theoretical</text>
<text top="484" left="80" width="498" height="14" font="4">throughput and there are certainly many more optimisation opportunities left to</text>
<text top="502" left="80" width="498" height="14" font="4">discover. In fact, larger speed-ups for GPU ports can be found in literature for other</text>
<text top="520" left="80" width="497" height="14" font="4">applications. However, one should be aware that sometimes reported performance</text>
<text top="538" left="80" width="122" height="14" font="4">gains are <a href="pdfxml.html#13">unrealistic</a></text>
<text top="535" left="202" width="6" height="10" font="9"><a href="pdfxml.html#13">5</a></text>
<text top="538" left="213" width="364" height="14" font="4">and are based on comparisons of optimised GPU code with</text>
<text top="556" left="80" width="170" height="14" font="4">poorly optimised CPU code.</text>
<text top="574" left="98" width="480" height="14" font="4">The lower cost per GFlops for hybrid architectures will surely enhance their</text>
<text top="591" left="80" width="497" height="14" font="4">diffusion in the near future. For example, in late 2010 already three out to the top</text>
<text top="609" left="80" width="497" height="14" font="4">four fastest supercomputers are GPU based, including the number one (Stone and</text>
<text top="627" left="80" width="497" height="14" font="4">Xin 2010). As GPUs are beginning to be integrated in HPC platforms, OpenCL</text>
<text top="645" left="80" width="497" height="14" font="4">support might become a standard feature of scientiﬁc codes, as MPI parallelisation</text>
<text top="663" left="80" width="497" height="14" font="4">is today. In fact, it is possible that GPUs will set the trend for high performance</text>
<text top="681" left="80" width="498" height="14" font="4">computing. The combination of highly parallel processors and a standard framework</text>
<text top="699" left="80" width="497" height="14" font="4">to write code for them, OpenCL, could well be the base for the future of numerically</text>
<text top="717" left="80" width="497" height="14" font="4">intensive calculations, comprising not only GPUs but also CPUs and accelerator</text>
<text top="735" left="80" width="497" height="14" font="4">boards. In fact, processor vendors like AMD, IBM or Intel already provide OpenCL</text>
<text top="753" left="80" width="498" height="14" font="4">implementations for their CPUs. So the key issue behind the GPU “revolution” is the</text>
<text top="771" left="80" width="497" height="14" font="4">development of in-chip parallel programming languages, that will allow engineers</text>
<text top="789" left="80" width="497" height="14" font="4">to start putting more execution units in processors of any type, knowing that many</text>
<text top="807" left="80" width="267" height="14" font="4">applications will be able to proﬁt from them.</text>
<text top="863" left="80" width="5" height="9" font="10">5</text>
<text top="865" left="97" width="481" height="12" font="5">Unrealistic in the sense that the speed-up is larger than the theoretical GPU/CPU performance</text>
<text top="880" left="80" width="26" height="12" font="5">ratio.</text>
</page>
</pdf2xml>
