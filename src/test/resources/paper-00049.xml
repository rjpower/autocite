<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE pdf2xml SYSTEM "pdf2xml.dtd">

<pdf2xml>
<page number="1" position="absolute" top="0" left="0" height="1263" width="892">
	<fontspec id="0" size="28" family="Times" color="#000000"/>
	<fontspec id="1" size="12" family="Times" color="#000000"/>
	<fontspec id="2" size="11" family="Courier" color="#000000"/>
	<fontspec id="3" size="11" family="Courier" color="#00007f"/>
	<fontspec id="4" size="9" family="Times" color="#000000"/>
	<fontspec id="5" size="9" family="Times" color="#000000"/>
	<fontspec id="6" size="9" family="Times" color="#000000"/>
	<fontspec id="7" size="12" family="Times" color="#000000"/>
	<fontspec id="8" size="12" family="Times" color="#00007f"/>
<text top="277" left="236" width="422" height="28" font="0">Declarative Parallel Programming</text>
<text top="315" left="387" width="118" height="28" font="0">for GPUs</text>
<text top="379" left="211" width="70" height="13" font="1">Eric HOLK</text>
<text top="379" left="300" width="93" height="13" font="1">William BYRD</text>
<text top="379" left="412" width="115" height="13" font="1">Nilesh MAHAJAN</text>
<text top="379" left="546" width="136" height="13" font="1">Jeremiah WILLCOCK</text>
<text top="397" left="293" width="108" height="13" font="1">Arun CHAUHAN</text>
<text top="397" left="420" width="22" height="13" font="1">and</text>
<text top="397" left="461" width="139" height="13" font="1">Andrew LUMSDAINE</text>
<text top="415" left="186" width="520" height="13" font="1">School of Informatics and Computing, Indiana University, Bloomington, Indiana, USA</text>
<text top="435" left="215" width="8" height="11" font="2"><a href="mailto:eholk@indiana.edu">{</a></text>
<text top="435" left="223" width="40" height="11" font="3"><a href="mailto:eholk@indiana.edu">eholk</a></text>
<text top="435" left="263" width="8" height="11" font="2"><a href="mailto:eholk@indiana.edu">,</a></text>
<text top="435" left="271" width="49" height="11" font="3"><a href="mailto:webyrd@indiana.edu">webyrd</a></text>
<text top="435" left="320" width="8" height="11" font="2"><a href="mailto:webyrd@indiana.edu">,</a></text>
<text top="435" left="328" width="65" height="11" font="3"><a href="mailto:nnmahaja@indiana.edu">nnmahaja</a></text>
<text top="435" left="392" width="8" height="11" font="2"><a href="mailto:nnmahaja@indiana.edu">,</a></text>
<text top="435" left="400" width="57" height="11" font="3"><a href="mailto:jewillco@indiana.edu">jewilco</a></text>
<text top="435" left="457" width="8" height="11" font="2"><a href="mailto:jewillco@indiana.edu">,</a></text>
<text top="435" left="465" width="65" height="11" font="3"><a href="mailto:achauhan@indiana.edu">achauhan</a></text>
<text top="435" left="529" width="8" height="11" font="2"><a href="mailto:achauhan@indiana.edu">,</a></text>
<text top="435" left="537" width="32" height="11" font="3"><a href="mailto:lums@indiana.edu">lums</a></text>
<text top="435" left="570" width="105" height="11" font="2"><a href="mailto:lums@indiana.edu">}</a>@indiana.edu</text>
<text top="474" left="249" width="395" height="11" font="4"><b>Abstract. </b>The recent rise in the popularity of Graphics Processing Units (GPUs)</text>
<text top="489" left="249" width="395" height="11" font="5">has been fueled by software frameworks, such as NVIDIA’s Compute Uniﬁed De-</text>
<text top="504" left="249" width="394" height="11" font="5">vice Architecture (CUDA) and Khronos Group’s OpenCL that make GPUs avail-</text>
<text top="519" left="249" width="394" height="11" font="5">able for general purpose computing. However, CUDA and OpenCL are still low-</text>
<text top="534" left="249" width="395" height="11" font="5">level approaches that require users to handle details about data layout and move-</text>
<text top="549" left="249" width="395" height="11" font="5">ment across levels of memory hierarchy. We propose a <i>declarative </i>approach to</text>
<text top="564" left="249" width="395" height="11" font="5">coordinating computation and data movement between CPU and GPU, through a</text>
<text top="579" left="249" width="394" height="11" font="5">domain-speciﬁc language that we called <i>Harlan</i>. Not only does a declarative lan-</text>
<text top="594" left="249" width="394" height="11" font="5">guage obviate the need for the programmer to write low-level error-prone boiler-</text>
<text top="609" left="249" width="394" height="11" font="5">plate code, by raising the abstraction of specifying GPU computation it also allows</text>
<text top="624" left="249" width="395" height="11" font="5">the compiler to optimize data movement and overlap between CPU and GPU com-</text>
<text top="639" left="249" width="395" height="11" font="5">putation. By focusing on the “what”, and not the “how”, of data layout, data move-</text>
<text top="653" left="249" width="395" height="11" font="5">ment, and computation scheduling, the language eliminates the sources of many</text>
<text top="668" left="249" width="287" height="11" font="5">programming errors related to correctness and performance.</text>
<text top="695" left="249" width="319" height="11" font="4"><b>Keywords. </b>GPGPUs, declarative parallel programming, compilers</text>
<text top="761" left="183" width="81" height="13" font="7"><b>Introduction</b></text>
<text top="797" left="183" width="526" height="13" font="1">One of the most important developments in computing in the past few years has been the</text>
<text top="815" left="183" width="526" height="13" font="1">rise of graphics processing units (GPUs) for general purpose computing (also known as</text>
<text top="833" left="183" width="526" height="13" font="1">“GPGPU”). Driven by the demand for high-quality real-time graphics for video games,</text>
<text top="851" left="183" width="526" height="13" font="1">GPU performance has been increasing faster than that of conventional CPUs, including</text>
<text top="869" left="183" width="526" height="13" font="1">multi-core CPUs. A high-end GPU card can perform over a trillion ﬂoating-point oper-</text>
<text top="887" left="183" width="526" height="13" font="1">ations per second (1 tera FLOP)—faster than the fastest supercomputer in 1997. More</text>
<text top="905" left="183" width="526" height="13" font="1">recently, “hybrid” clusters of CPUs and GPUs have become popular in the realm of high-</text>
<text top="923" left="183" width="526" height="13" font="1">performance computing (HPC). According to the November 2010 Top500 results, the</text>
<text top="941" left="183" width="526" height="13" font="1">fastest supercomputer in the world is Tianhe-1A, a Chinese hybrid cluster with a mixture</text>
<text top="959" left="183" width="526" height="13" font="1">of Intel CPUs and NVIDIA GPUs; the number three supercomputer is also a Chinese</text>
<text top="977" left="183" width="104" height="13" font="1">hybrid machine <a href="pdfxml.html#8">[</a></text>
<text top="977" left="287" width="15" height="13" font="8"><a href="pdfxml.html#8">18</a></text>
<text top="977" left="302" width="407" height="13" font="1"><a href="pdfxml.html#8">]</a>. The popularity of hybrid architectures is likely to increase, since</text>
<text top="995" left="183" width="526" height="13" font="1">the primary design consideration for the next generation of supercomputers is energy</text>
<text top="1013" left="183" width="526" height="13" font="1">efﬁciency, and since GPUs are much more energy efﬁcient than CPUs (as measured in</text>
<text top="1031" left="183" width="106" height="13" font="1">FLOPS per watt).</text>
<text top="1049" left="210" width="475" height="13" font="1">The introduction of NVIDIA’s Compute Uniﬁed Device Architecture (CUDA) <a href="pdfxml.html#8">[</a></text>
<text top="1049" left="686" width="15" height="13" font="8"><a href="pdfxml.html#8">14</a></text>
<text top="1049" left="701" width="9" height="13" font="1"><a href="pdfxml.html#8">]</a>,</text>
<text top="1067" left="183" width="367" height="13" font="1">and more recently the Khronos Group’s OpenCL framework <a href="pdfxml.html#8">[</a></text>
<text top="1067" left="551" width="15" height="13" font="8"><a href="pdfxml.html#8">13</a></text>
<text top="1067" left="566" width="144" height="13" font="1"><a href="pdfxml.html#8">]</a>, along with GPU hard-</text>
</page>
<page number="2" position="absolute" top="0" left="0" height="1263" width="892">
	<fontspec id="9" size="12" family="Times" color="#000000"/>
<text top="231" left="183" width="526" height="13" font="1">ware advances to better support non-image-based computing, has made GPGPU pro-</text>
<text top="249" left="183" width="526" height="13" font="1">gramming much easier than before. However, CUDA and OpenCL are still relatively</text>
<text top="267" left="183" width="526" height="13" font="1">low-level approaches to GPGPU programming. Programmers are required to write boil-</text>
<text top="285" left="183" width="526" height="13" font="1">erplate code, handle low-level details of data layout and memory movement, determine</text>
<text top="303" left="183" width="527" height="13" font="1">how many blocks and threads are required for a computation, and so forth. Programming</text>
<text top="321" left="183" width="526" height="13" font="1">hybrid clusters is even harder, since cluster architectures complicate data movement, and</text>
<text top="339" left="183" width="475" height="13" font="1">introduce additional levels of memory hierarchy and computational granularity.</text>
<text top="357" left="210" width="500" height="13" font="1">We propose a declarative approach to coordinating computation and data movement.</text>
<text top="375" left="183" width="427" height="13" font="1">It builds on our earlier declarative language to specify communication <a href="pdfxml.html#8">[</a></text>
<text top="375" left="611" width="15" height="13" font="8"><a href="pdfxml.html#8">11</a></text>
<text top="375" left="626" width="9" height="13" font="1"><a href="pdfxml.html#8">]</a>.</text>
<text top="429" left="183" width="175" height="13" font="7"><b>1. Declarative Foundations</b></text>
<text top="464" left="183" width="526" height="13" font="1">GPGPU programming, and especially hybrid cluster programming, is complicated by</text>
<text top="482" left="183" width="526" height="13" font="1">many factors. For best performance, programmers must carefully manage low-level de-</text>
<text top="500" left="183" width="312" height="13" font="1">tails of memory movement, strided memory access <a href="pdfxml.html#8">[</a></text>
<text top="500" left="496" width="15" height="13" font="8"><a href="pdfxml.html#8">12</a></text>
<text top="500" left="510" width="199" height="13" font="1"><a href="pdfxml.html#8">]</a>, and thread synchronization and</text>
<text top="518" left="183" width="526" height="13" font="1">management. When GPGPUs are used in hybrid clusters, programming becomes even</text>
<text top="536" left="183" width="526" height="13" font="1">more cumbersome. In order to fully leverage the power of GPUs, and especially hybrid</text>
<text top="554" left="183" width="237" height="13" font="1">clusters, a different approach is needed.</text>
<text top="572" left="210" width="499" height="13" font="1">We advocate a declarative approach to programming hybrid clusters and GPUs. Our</text>
<text top="590" left="183" width="526" height="13" font="1">declarative approach provides the user with a straightforward mechanism for expressing</text>
<text top="608" left="183" width="526" height="13" font="1">the semantics the user wants for the data layout, memory movement, and computation</text>
<text top="626" left="183" width="526" height="13" font="1">coordination in her programs. The user can express the “what” but can leave the “how”</text>
<text top="644" left="183" width="502" height="13" font="1">to the tool developers, avoiding the myriad details described in the previous section.</text>
<text top="662" left="210" width="299" height="13" font="1">The two pillars of our approach are the following:</text>
<text top="696" left="183" width="526" height="12" font="9">• Development and analysis of a declarative “computational kernel” language as an ap-</text>
<text top="711" left="198" width="512" height="13" font="1">proach for coordinating computation, data layout, and memory movement within a</text>
<text top="729" left="198" width="226" height="13" font="1">single machine containing GPUs; and</text>
<text top="752" left="183" width="526" height="12" font="9">• Integration of the kernel language within Kanor, our declarative language for cluster</text>
<text top="766" left="198" width="86" height="13" font="1">programming.</text>
<text top="798" left="210" width="499" height="13" font="1">Our approach leverages our current work on Kanor, a declarative language for spec-</text>
<text top="816" left="183" width="526" height="13" font="1">ifying communication on distributed-memory clusters. Kanor is unusual in that the pro-</text>
<text top="834" left="183" width="526" height="13" font="1">grammer declaratively, but explicitly, speciﬁes the essence of the communication pat-</text>
<text top="851" left="183" width="526" height="13" font="1">tern. The programmer lets the implementation handle the details when appropriate, but</text>
<text top="869" left="183" width="526" height="13" font="1">retains the option to hand-encode communications when necessary, providing a balance</text>
<text top="887" left="183" width="422" height="13" font="1">between declarativeness and performance predictability and tunability.</text>
<text top="905" left="210" width="499" height="13" font="1">Similarly, our computational kernel language (named “Harlan”) allows the user to</text>
<text top="923" left="183" width="526" height="13" font="1">declaratively, but explicitly, describe (potentially asynchronous) computational kernels</text>
<text top="941" left="183" width="526" height="13" font="1">and to coordinate computation, data layout, and memory movement. As with Kanor, this</text>
<text top="959" left="183" width="526" height="13" font="1">approach gives the programmer enough control to write efﬁcient code, while abstract-</text>
<text top="977" left="183" width="526" height="13" font="1">ing over the low-level details that make GPU programming so difﬁcult. Integrating Har-</text>
<text top="995" left="183" width="526" height="13" font="1">lan into Kanor results in a uniﬁed, high-level, ﬂexible language suitable for efﬁciently</text>
<text top="1013" left="183" width="526" height="13" font="1">programming hybrid clusters, traditional (CPU-based) clusters, and GPUs on a single</text>
<text top="1031" left="183" width="55" height="13" font="1">machine.</text>
<text top="1049" left="210" width="499" height="13" font="1">Declaratively specifying data layout, memory movement, and computation coordi-</text>
<text top="1067" left="183" width="526" height="13" font="1">nation requirements results in a system with well-deﬁned semantics. Thus, for instance,</text>
</page>
<page number="3" position="absolute" top="0" left="0" height="1263" width="892">
	<fontspec id="10" size="12" family="Courier" color="#000000"/>
	<fontspec id="11" size="12" family="Times" color="#000000"/>
<text top="231" left="183" width="526" height="13" font="1">interactions between data movement and computation can be automatically veriﬁed.</text>
<text top="250" left="183" width="526" height="13" font="1">Common GPU programming mistakes (e.g., deadlock through incorrect use of synchro-</text>
<text top="268" left="183" width="121" height="13" font="1">nization constructs <a href="pdfxml.html#8">[</a></text>
<text top="268" left="305" width="7" height="13" font="8"><a href="pdfxml.html#8">3</a></text>
<text top="268" left="312" width="4" height="13" font="1"><a href="pdfxml.html#8">,</a></text>
<text top="268" left="316" width="15" height="13" font="8"><a href="pdfxml.html#8">17</a></text>
<text top="268" left="331" width="155" height="13" font="1"><a href="pdfxml.html#8">]</a>) can thereby be avoided.</text>
<text top="286" left="210" width="500" height="13" font="1">Moreover, a declarative approach with well-deﬁned semantics provides opportuni-</text>
<text top="304" left="183" width="526" height="13" font="1">ties for sophisticated optimizations, analyses, and tools. For example, the implementation</text>
<text top="322" left="183" width="526" height="13" font="1">could use a combination of heuristics and autotuning to determine how many stages of a</text>
<text top="340" left="183" width="527" height="13" font="1">reduction (if any) should be performed on the CPU rather than on the GPU, depending</text>
<text top="358" left="183" width="526" height="13" font="1">on the speciﬁc machine’s hardware. An example of another optimization would be dou-</text>
<text top="376" left="183" width="526" height="13" font="1">ble or triple buffering to hide direct memory access (DMA) latency when moving data</text>
<text top="394" left="183" width="526" height="13" font="1">from the GPU to main memory—this optimization could be performed automatically by</text>
<text top="412" left="183" width="413" height="13" font="1">the compiler, perhaps using guidance from programmer declarations.</text>
<text top="430" left="210" width="499" height="13" font="1">It is important to emphasize at this point that we are not proposing a “silver bullet” or</text>
<text top="448" left="183" width="526" height="13" font="1">“magic compiler” that will somehow make GPGPU or hybrid cluster programming easy.</text>
<text top="467" left="183" width="526" height="13" font="1">Rather, we are seeking to abstract away many of the low-level details that make GPU/-</text>
<text top="485" left="183" width="526" height="13" font="1">cluster programming difﬁcult, while still giving the programmer enough control over</text>
<text top="503" left="183" width="511" height="13" font="1">data arrangement and computation coordination to write high-performance programs.</text>
<text top="558" left="183" width="173" height="13" font="7"><b>2. A Declarative Language</b></text>
<text top="595" left="183" width="145" height="13" font="1">As described in Section</text>
<text top="595" left="334" width="7" height="13" font="8"><a href="pdfxml.html#2">1</a></text>
<text top="595" left="341" width="368" height="13" font="1"><a href="pdfxml.html#2">, </a>a CUDA or OpenCL programmer must handle a variety of</text>
<text top="613" left="183" width="526" height="13" font="1">low-level details that have nothing to do with the problem domain. For example, consider</text>
<text top="631" left="183" width="158" height="13" font="1">the CUDA code in Figure</text>
<text top="631" left="346" width="7" height="13" font="8"><a href="pdfxml.html#4">1</a></text>
<text top="631" left="354" width="356" height="13" font="1"><a href="pdfxml.html#4">, </a>which sums two vectors; this code is verbose, containing</text>
<text top="649" left="183" width="526" height="13" font="1">boiler-plate code for moving data to and from the GPU, calculating thread indices, and</text>
<text top="667" left="183" width="526" height="13" font="1">so forth. Ideally, the programmer could ignore these details and write something like the</text>
<text top="685" left="183" width="184" height="13" font="1">code in the top part of Figure</text>
<text top="685" left="373" width="7" height="13" font="8"><a href="pdfxml.html#5">2</a></text>
<text top="685" left="380" width="330" height="13" font="1"><a href="pdfxml.html#5">, </a>which expresses the desired kernel computation and</text>
<text top="703" left="183" width="526" height="13" font="1">data movement much more succinctly. This code snippet indicates that the expression</text>
<text top="722" left="183" width="81" height="12" font="10">z = x + y</text>
<text top="721" left="269" width="156" height="13" font="1">runs on the GPU for each</text>
<text top="722" left="429" width="9" height="12" font="10">x</text>
<text top="721" left="438" width="4" height="13" font="1">,</text>
<text top="722" left="447" width="9" height="12" font="10">y</text>
<text top="721" left="460" width="22" height="13" font="1">and</text>
<text top="722" left="486" width="9" height="12" font="10">z</text>
<text top="721" left="500" width="82" height="13" font="1">in the vectors</text>
<text top="722" left="586" width="9" height="12" font="10">X</text>
<text top="721" left="595" width="4" height="13" font="1">,</text>
<text top="722" left="603" width="9" height="12" font="10">Y</text>
<text top="721" left="617" width="22" height="13" font="1">and</text>
<text top="722" left="643" width="9" height="12" font="10">Z</text>
<text top="721" left="652" width="58" height="13" font="1">. The end</text>
<text top="739" left="183" width="74" height="13" font="1">result is that</text>
<text top="740" left="262" width="9" height="12" font="10">Z</text>
<text top="739" left="275" width="166" height="13" font="1">contains the sum of vectors</text>
<text top="740" left="444" width="9" height="12" font="10">X</text>
<text top="739" left="458" width="22" height="13" font="1">and</text>
<text top="740" left="483" width="9" height="12" font="10">Y</text>
<text top="739" left="492" width="217" height="13" font="1">. This level of expression allows the</text>
<text top="757" left="183" width="526" height="13" font="1">compiler to automatically perform transformations, such as pipelining, depending on the</text>
<text top="776" left="183" width="112" height="13" font="1">size of the vectors.</text>
<text top="794" left="210" width="297" height="13" font="1">The compiler translates the speciﬁcation in Figure</text>
<text top="794" left="510" width="7" height="13" font="8"><a href="pdfxml.html#5">2</a></text>
<text top="794" left="521" width="189" height="13" font="1">into low-level code in CUDA or</text>
<text top="812" left="183" width="527" height="13" font="1">OpenCL. In this section we describe our approach to Harlan’s design, implementation,</text>
<text top="830" left="183" width="105" height="13" font="1">and optimization.</text>
<text top="867" left="183" width="72" height="13" font="11"><i>2.1. Design</i></text>
<text top="904" left="183" width="526" height="13" font="1">Harlan enables the programmer to specify sections of code to run on the GPU or other</text>
<text top="922" left="183" width="527" height="13" font="1">accelerator over certain ranges of data. This level of expression gives the programmer</text>
<text top="940" left="183" width="526" height="13" font="1">control of where the computation takes place, and implicitly deﬁnes what data must move</text>
<text top="958" left="183" width="526" height="13" font="1">and when such movement must occur. However, the compiler and runtime maintain a</text>
<text top="976" left="183" width="526" height="13" font="1">great deal of ﬂexibility to perform data layout transformations or optimizations such as</text>
<text top="994" left="183" width="65" height="13" font="1">pipelining.</text>
<text top="1012" left="210" width="499" height="13" font="1">One key issue in programming GPUs is managing whether data is resident on the de-</text>
<text top="1031" left="183" width="526" height="13" font="1">vice or host memory. This is especially difﬁcult in CUDA, as the language does not make</text>
<text top="1049" left="183" width="526" height="13" font="1">any distinction at a language level between data on the GPU and data on the CPU—both</text>
<text top="1067" left="183" width="526" height="13" font="1">are represented as pointers. Our kernel blocks, on the other hand, indicate syntactically</text>
</page>
<page number="4" position="absolute" top="0" left="0" height="1263" width="892">
	<fontspec id="12" size="11" family="Times" color="#000000"/>
	<fontspec id="13" size="11" family="Times" color="#000000"/>
	<fontspec id="14" size="11" family="Times" color="#000000"/>
<text top="231" left="183" width="526" height="13" font="1">which portions of code should run on the GPU if possible, and thus imply what data</text>
<text top="250" left="183" width="526" height="13" font="1">must be on the GPU. In the most naïve sense, all the data needed by a kernel is moved</text>
<text top="269" left="183" width="526" height="13" font="1">to the GPU upon entering a kernel expression and the data is moved back afterwards. In</text>
<text top="287" left="183" width="526" height="13" font="1">practice, however, the compiler may use dataﬂow analysis to eliminate unnecessary data</text>
<text top="306" left="183" width="526" height="13" font="1">movement. Freeing the programmer from worrying about these details also lowers the</text>
<text top="324" left="183" width="526" height="13" font="1">potential for errors that arise from, for example, dereferencing a device pointer from the</text>
<text top="343" left="183" width="110" height="13" font="1">host or vice-versa.</text>
<text top="361" left="210" width="499" height="13" font="1">We have designed kernels so that they are expressions that return values. This deci-</text>
<text top="380" left="183" width="526" height="13" font="1">sion improves expressiveness and compositionality. Using kernels as expressions allows</text>
<text top="399" left="183" width="215" height="13" font="1">rewriting the ﬁrst example in Figure</text>
<text top="399" left="402" width="7" height="13" font="8"><a href="pdfxml.html#5">2</a></text>
<text top="399" left="413" width="17" height="13" font="1">as:</text>
<text top="400" left="433" width="206" height="12" font="12">Z = <b>kernel</b>(x : X, y : Y) { x + y }</text>
<text top="399" left="639" width="70" height="13" font="1">. Kernel ex-</text>
<text top="417" left="183" width="526" height="13" font="1">pressions are similar to the map operator in functional programming languages. We also</text>
<text top="436" left="183" width="510" height="13" font="1">provide support for reductions, enabling programming styles such as MapReduce <a href="pdfxml.html#8">[</a></text>
<text top="436" left="693" width="7" height="13" font="8"><a href="pdfxml.html#8">8</a></text>
<text top="436" left="701" width="9" height="13" font="1"><a href="pdfxml.html#8">]</a>.</text>
<text top="454" left="183" width="78" height="13" font="1">For instance:</text>
<text top="455" left="265" width="218" height="14" font="12">Z = +/<b>kernel</b>(x : X, y : Y) { x ∗ y }</text>
<text top="454" left="483" width="226" height="13" font="1">. Here, the values returned by the ker-</text>
<text top="473" left="183" width="207" height="13" font="1">nel are to be summed and stored in</text>
<text top="474" left="394" width="9" height="12" font="10">z</text>
<text top="473" left="403" width="306" height="13" font="1">. This approach gives the programmer more control</text>
<text top="492" left="183" width="526" height="13" font="1">and is more natural when nesting reductions. Making this reduction information explicit</text>
<text top="510" left="183" width="404" height="13" font="1">provides more information for the compiler to use in optimizations.</text>
<text top="529" left="210" width="499" height="13" font="1">Our goal is to avoid restrictions on what code is allowed within a kernel block. We</text>
<text top="547" left="183" width="526" height="13" font="1">allow arbitrary procedure calls within kernels, including recursive calls. This implies the</text>
<text top="566" left="183" width="526" height="13" font="1">ability for kernels to be nested, since even if we tried to restrict this, kernels might still</text>
<text top="584" left="183" width="526" height="13" font="1">call functions that use kernels. This eliminates the cognitive burden caused by constructs</text>
<text top="644" left="185" width="76" height="12" font="13"><b>_ _ g l o b a l _ _</b></text>
<text top="644" left="274" width="198" height="12" font="13"><b>v o i d </b>a d d _ k e r n e l ( <b>i n t </b>s i z e ,</text>
<text top="644" left="484" width="230" height="14" font="13"><b>f l o a t </b>∗X, <b>f l o a t </b>∗Y, <b>f l o a t </b>∗Z )</text>
<text top="660" left="184" width="6" height="12" font="12">{</text>
<text top="677" left="202" width="157" height="12" font="13"><b>i n t </b>i = t h r e a d I d x . x ;</text>
<text top="693" left="202" width="287" height="12" font="13"><b>i f </b>( i &lt; s i z e ) { Z [ i ] = X[ i ] + Y[ i ] ; }</text>
<text top="709" left="184" width="6" height="12" font="12">}</text>
<text top="742" left="185" width="198" height="12" font="13"><b>v o i d </b>v e c t o r _ a d d ( <b>i n t </b>s i z e ,</text>
<text top="742" left="396" width="230" height="14" font="13"><b>f l o a t </b>∗X, <b>f l o a t </b>∗Y, <b>f l o a t </b>∗Z )</text>
<text top="758" left="184" width="6" height="12" font="12">{</text>
<text top="775" left="202" width="157" height="14" font="13"><b>f l o a t </b>∗dX , ∗dY , ∗dZ ;</text>
<text top="791" left="201" width="117" height="12" font="12">c u d a M a l l o c (&amp;dX ,</text>
<text top="791" left="331" width="172" height="14" font="12">s i z e ∗ <b>s i z e o f </b>( <b>f l o a t </b>) ) ;</text>
<text top="808" left="201" width="117" height="12" font="12">c u d a M a l l o c (&amp;dY ,</text>
<text top="808" left="331" width="172" height="14" font="12">s i z e ∗ <b>s i z e o f </b>( <b>f l o a t </b>) ) ;</text>
<text top="824" left="201" width="117" height="12" font="12">c u d a M a l l o c (&amp;dZ ,</text>
<text top="824" left="331" width="172" height="14" font="12">s i z e ∗ <b>s i z e o f </b>( <b>f l o a t </b>) ) ;</text>
<text top="857" left="200" width="134" height="12" font="12">cudaMemcpy ( dX , X ,</text>
<text top="857" left="347" width="366" height="14" font="12">s i z e ∗ <b>s i z e o f </b>( <b>f l o a t </b>) , cudaMemcpyHostToDevice ) ;</text>
<text top="873" left="200" width="134" height="12" font="12">cudaMemcpy ( dY , Y ,</text>
<text top="873" left="347" width="366" height="14" font="12">s i z e ∗ <b>s i z e o f </b>( <b>f l o a t </b>) , cudaMemcpyHostToDevice ) ;</text>
<text top="906" left="201" width="335" height="12" font="12">a d d _ k e r n e l &lt; &lt; &lt;1 , s i z e &gt; &gt; &gt;( s i z e , dX , dY , dZ ) ;</text>
<text top="938" left="200" width="134" height="12" font="12">cudaMemcpy ( Z , dZ ,</text>
<text top="938" left="347" width="366" height="14" font="12">s i z e ∗ <b>s i z e o f </b>( <b>f l o a t </b>) , cudaMemcpyDeviceToHost ) ;</text>
<text top="971" left="201" width="101" height="12" font="12">c u d a F r e e ( dX ) ;</text>
<text top="988" left="201" width="101" height="12" font="12">c u d a F r e e ( dY ) ;</text>
<text top="1004" left="201" width="101" height="12" font="12">c u d a F r e e ( dZ ) ;</text>
<text top="1020" left="184" width="6" height="12" font="12">}</text>
<text top="1066" left="334" width="224" height="11" font="4"><b>Figure 1. </b>CUDA code for adding two vectors.</text>
</page>
<page number="5" position="absolute" top="0" left="0" height="1263" width="892">
	<fontspec id="15" size="11" family="Times" color="#000000"/>
	<fontspec id="16" size="12" family="Courier" color="#000000"/>
<text top="247" left="185" width="254" height="12" font="13"><b>v o i d </b>v e c t o r _ a d d ( v e c t o r &lt; <b>f l o a t </b>&gt; X ,</text>
<text top="247" left="452" width="125" height="12" font="12">v e c t o r &lt; <b>f l o a t </b>&gt; Y ,</text>
<text top="247" left="589" width="125" height="12" font="12">v e c t o r &lt; <b>f l o a t </b>&gt; Z )</text>
<text top="263" left="184" width="370" height="12" font="12">{ <b>k e r n e l </b>( x : X , y : Y , z : Z ) { z = x + y ; } }</text>
<text top="306" left="186" width="390" height="12" font="12">t o t a l = + / <b>k e r n e l </b>( row : Rows ) { + / <b>k e r n e l </b>( x : row ) ;</text>
<text top="306" left="589" width="12" height="12" font="12">} ;</text>
<text top="350" left="185" width="368" height="14" font="12">h a n d l e = <b>a s y n c k e r n e l </b>( x : X , y : Y) { x ∗ y } ;</text>
<text top="366" left="185" width="440" height="14" font="15"><i>/ </i>∗ <i>O t he r c o n c u r r e n t k e r n e l s or program c ode go h e r e . </i>∗ <i>/</i></text>
<text top="382" left="185" width="150" height="12" font="12">z = + / <b>w a i t </b>( h a n d l e ) ;</text>
<text top="426" left="185" width="337" height="14" font="13"><b>k e r n e l </b>( x : X , y : Y , z : Z ) { z = x ∗ y ; }</text>
<text top="442" left="184" width="417" height="12" font="13"><b>@communicate </b>{Y[ i ] @r &lt;&lt;= Z [ i ]@( ( r + 1 ) % NUM_NODES ) ,</text>
<text top="458" left="201" width="134" height="12" font="13"><b>where </b>r <b>i n world </b>,</text>
<text top="458" left="347" width="29" height="12" font="12">i <b>i n</b></text>
<text top="458" left="388" width="108" height="12" font="12">0 . . . l e n g t h (Y) }</text>
<text top="475" left="185" width="337" height="14" font="13"><b>k e r n e l </b>( x : X , y : Y , z : Z ) { z = x ∗ y ; }</text>
<text top="521" left="183" width="526" height="11" font="4"><b>Figure 2. </b>Harlan code for adding two vectors, sum reduction using nested kernels, asynchronous kernels, and</text>
<text top="536" left="183" width="275" height="11" font="5">kernels interspersed with communication across a cluster.</text>
<text top="568" left="183" width="526" height="13" font="1">such as CUDA’s local and global functions and allows programs such as the second</text>
<text top="586" left="183" width="109" height="13" font="1">example in Figure</text>
<text top="586" left="296" width="7" height="13" font="8"><a href="pdfxml.html#5">2</a></text>
<text top="586" left="303" width="4" height="13" font="1"><a href="pdfxml.html#5">.</a></text>
<text top="603" left="210" width="155" height="13" font="1">In the most basic form, a</text>
<text top="604" left="371" width="54" height="12" font="16"><b>kernel</b></text>
<text top="603" left="430" width="280" height="13" font="1">block provides language support for data par-</text>
<text top="621" left="183" width="526" height="13" font="1">allelism. Allowing multiple kernels to run concurrently can provide more ﬂexibility in</text>
<text top="639" left="183" width="526" height="13" font="1">expressing algorithms and gives more freedom to the compiler for scheduling. Modern</text>
<text top="657" left="183" width="526" height="13" font="1">GPUs, such as those using NVIDIA’s Fermi architecture, support running multiple ker-</text>
<text top="675" left="183" width="96" height="13" font="1">nels in parallel <a href="pdfxml.html#8">[</a></text>
<text top="675" left="279" width="15" height="13" font="8"><a href="pdfxml.html#8">15</a></text>
<text top="675" left="294" width="165" height="13" font="1"><a href="pdfxml.html#8">]</a>. Harlan allows an optional</text>
<text top="676" left="462" width="45" height="12" font="16"><b>async</b></text>
<text top="675" left="511" width="199" height="13" font="1">annotation on kernel expressions.</text>
<text top="693" left="183" width="471" height="13" font="1">The expression returns a handle that the program could then wait on. Figure</text>
<text top="693" left="660" width="7" height="13" font="8"><a href="pdfxml.html#5">2</a></text>
<text top="693" left="672" width="37" height="13" font="1">shows</text>
<text top="711" left="183" width="145" height="13" font="1">an example of using an</text>
<text top="712" left="334" width="108" height="12" font="16"><b>async kernel</b></text>
<text top="711" left="441" width="268" height="13" font="1">. In this example, the kernel may run asyn-</text>
<text top="729" left="183" width="304" height="13" font="1">chronously from the rest of the main program. The</text>
<text top="729" left="491" width="108" height="12" font="16"><b>wait</b>(handle)</text>
<text top="729" left="603" width="107" height="13" font="1">expression blocks</text>
<text top="746" left="183" width="526" height="13" font="1">until the kernel completes and returns the value that would have been returned by the</text>
<text top="764" left="183" width="526" height="13" font="1">kernel were it executed synchronously. This added ﬂexibility can simplify the expression</text>
<text top="782" left="183" width="114" height="13" font="1">of task parallelism.</text>
<text top="817" left="183" width="124" height="13" font="11"><i>2.2. Implementation</i></text>
<text top="852" left="183" width="526" height="13" font="1">Perhaps the biggest language design challenge is determining how to effectively map</text>
<text top="870" left="183" width="526" height="13" font="1">our language features onto hardware. One difﬁculty is that GPUs currently have more</text>
<text top="888" left="183" width="526" height="13" font="1">limited control ﬂow options than CPUs. In effect, they support only small branches and</text>
<text top="906" left="183" width="527" height="13" font="1">looping constructs; richer concepts like recursion natively supported. Many languages</text>
<text top="923" left="183" width="526" height="13" font="1">address this by limiting the expressiveness of GPU kernels. We intend to avoid these</text>
<text top="941" left="183" width="526" height="13" font="1">restrictions. Kernels should be allowed to run arbitrary computations, including recur-</text>
<text top="959" left="183" width="526" height="13" font="1">sive procedure calls and spawning additional kernels. Launching kernels from within</text>
<text top="977" left="183" width="526" height="13" font="1">kernels leads to nested data parallelism, which has been the subject of much existing</text>
<text top="995" left="183" width="59" height="13" font="1">research <a href="pdfxml.html#8">[</a></text>
<text top="995" left="243" width="7" height="13" font="8"><a href="pdfxml.html#8">2</a></text>
<text top="995" left="250" width="4" height="13" font="1"><a href="pdfxml.html#8">,</a></text>
<text top="995" left="254" width="15" height="13" font="8"><a href="pdfxml.html#8">16</a></text>
<text top="995" left="269" width="4" height="13" font="1"><a href="pdfxml.html#8">,</a></text>
<text top="995" left="273" width="7" height="13" font="8"><a href="pdfxml.html#8">6</a></text>
<text top="995" left="280" width="60" height="13" font="1"><a href="pdfxml.html#8">]</a>. NESL <a href="pdfxml.html#8">[</a></text>
<text top="995" left="340" width="7" height="13" font="8"><a href="pdfxml.html#8">2</a></text>
<text top="995" left="348" width="4" height="13" font="1"><a href="pdfxml.html#8">,</a></text>
<text top="995" left="352" width="7" height="13" font="8"><a href="pdfxml.html#8">1</a></text>
<text top="995" left="359" width="350" height="13" font="1"><a href="pdfxml.html#8">] </a>implements nested parallelism primarily through ﬂatten-</text>
<text top="1013" left="183" width="526" height="13" font="1">ing. More recent work argues that except in the most unbalanced workloads, leaving the</text>
<text top="1031" left="183" width="526" height="13" font="1">program in its original nested form provides more opportunities to exploit hierarchies</text>
<text top="1049" left="183" width="133" height="13" font="1">in modern machines <a href="pdfxml.html#8">[</a></text>
<text top="1049" left="316" width="7" height="13" font="8"><a href="pdfxml.html#8">5</a></text>
<text top="1049" left="324" width="386" height="13" font="1"><a href="pdfxml.html#8">]</a>. More evaluation is clearly necessary, especially in the deeper</text>
<text top="1067" left="183" width="318" height="13" font="1">hierarchies present in hybrid GPU cluster computers.</text>
</page>
<page number="6" position="absolute" top="0" left="0" height="1263" width="892">
<text top="231" left="210" width="499" height="13" font="1">Existing work has demonstrated that it is possible to support richer control ﬂow on</text>
<text top="250" left="183" width="52" height="13" font="1">a GPU <a href="pdfxml.html#8">[</a></text>
<text top="250" left="236" width="7" height="13" font="8"><a href="pdfxml.html#8">9</a></text>
<text top="250" left="243" width="466" height="13" font="1"><a href="pdfxml.html#8">]</a>. The approach is to write a SIMD interpreter that interprets different pro-</text>
<text top="268" left="183" width="526" height="13" font="1">grams as data. Naturally, this incurs a certain amount of overhead, although the results</text>
<text top="286" left="183" width="526" height="13" font="1">suggested that this overhead will be acceptable in many cases. We propose a hybrid ap-</text>
<text top="304" left="183" width="526" height="13" font="1">proach, generating native GPU code when possible, and using the interpreter approach</text>
<text top="322" left="183" width="526" height="13" font="1">for particularly difﬁcult kernels. We expect that future GPUs will continue to relax the</text>
<text top="340" left="183" width="218" height="13" font="1">restrictions on program control ﬂow.</text>
<text top="358" left="210" width="499" height="13" font="1">Data movement within a hybrid cluster introduces more opportunities. Soon it will</text>
<text top="376" left="183" width="526" height="13" font="1">be common for GPUs to be able to interact with network hardware directly, with-</text>
<text top="394" left="183" width="526" height="13" font="1">out involvement from the CPU. While a naïve implementation of Harlan with Kanor</text>
<text top="413" left="183" width="108" height="12" font="16"><b>@communicate</b></text>
<text top="412" left="295" width="414" height="13" font="1">blocks would always copy data from the GPU to the CPU before do-</text>
<text top="431" left="183" width="526" height="13" font="1">ing network transfers, this is unnecessary. The dataﬂow analysis that is used to optimize</text>
<text top="449" left="183" width="527" height="13" font="1">data movement between the CPU and GPU memory can also inform communication</text>
<text top="467" left="183" width="526" height="13" font="1">code generation to produce direct GPU to GPU transfers even across nodes in a cluster.</text>
<text top="485" left="183" width="160" height="13" font="1">The last example in Figure</text>
<text top="485" left="347" width="7" height="13" font="8"><a href="pdfxml.html#5">2</a></text>
<text top="485" left="358" width="351" height="13" font="1">shows communication code between two kernels. The pro-</text>
<text top="503" left="183" width="526" height="13" font="1">gram ﬁrst performs a computation, then all nodes exchange data, and the computation</text>
<text top="521" left="183" width="192" height="13" font="1">continues. None of the variables</text>
<text top="522" left="378" width="9" height="12" font="10">X</text>
<text top="521" left="387" width="4" height="13" font="1">,</text>
<text top="522" left="394" width="9" height="12" font="10">Y</text>
<text top="521" left="406" width="22" height="13" font="1">and</text>
<text top="522" left="431" width="9" height="12" font="10">Z</text>
<text top="521" left="444" width="266" height="13" font="1">are accessed off of the GPU between the two</text>
<text top="539" left="183" width="527" height="13" font="1">kernels, so there is no reason to move them off of the GPU. Instead, the communication</text>
<text top="557" left="183" width="311" height="13" font="1">can directly transfer between the GPU and network.</text>
<text top="595" left="183" width="125" height="13" font="11"><i>2.2.1. Optimizations</i></text>
<text top="622" left="183" width="96" height="13" font="11"><i>Data Movement</i></text>
<text top="622" left="294" width="416" height="13" font="1">Since data movement between CPU and GPU is implicit, the compiler</text>
<text top="640" left="183" width="526" height="13" font="1">must infer when data need to be copied. Data that are not live at the end of a kernel</text>
<text top="658" left="183" width="526" height="13" font="1">need not be copied back into the CPU. Similarly, data that would be used only by a</text>
<text top="676" left="183" width="526" height="13" font="1">subsequent kernel may be kept on the GPU. However, this must be balanced against the</text>
<text top="694" left="183" width="252" height="13" font="1">GPU memory footprint of the application.</text>
<text top="713" left="210" width="499" height="13" font="1">A second class of data locality optimizations relate to various memory types on</text>
<text top="731" left="183" width="526" height="13" font="1">contemporary GPUs, as mentioned earlier. For instance, the compiler can identify read-</text>
<text top="749" left="183" width="526" height="13" font="1">only data that are live and expected to be used soon, to allocate those in the faster constant</text>
<text top="767" left="183" width="53" height="13" font="1">memory.</text>
<text top="794" left="183" width="100" height="13" font="11"><i>Splitting Kernels</i></text>
<text top="794" left="298" width="411" height="13" font="1">Kernels deﬁned in Harlan may often map directly to kernels in CUDA</text>
<text top="813" left="183" width="526" height="13" font="1">or OpenCL, but they are not required to. CUDA and OpenCL kernels have restrictions</text>
<text top="831" left="183" width="526" height="13" font="1">on control ﬂow and procedure calls, and lack synchronization capabilities. Thus, the</text>
<text top="849" left="183" width="526" height="13" font="1">compiler may need to split a kernel in Harlan in order to implement it using CUDA</text>
<text top="867" left="183" width="52" height="13" font="1">kernels <a href="pdfxml.html#8">[</a></text>
<text top="867" left="236" width="7" height="13" font="8"><a href="pdfxml.html#8">4</a></text>
<text top="867" left="243" width="466" height="13" font="1"><a href="pdfxml.html#8">]</a>. Multiple splits may be possible, so the compiler must select the one that is</text>
<text top="885" left="183" width="240" height="13" font="1">likely to minimize performance penalty.</text>
<text top="912" left="183" width="191" height="13" font="11"><i>Scheduling Concurrent Kernels</i></text>
<text top="912" left="389" width="320" height="13" font="1">Carefully scheduling the kernels has the potential to</text>
<text top="931" left="183" width="526" height="13" font="1">dramatically improve data locality and avoid unnecessary CPU-GPU data movement. A</text>
<text top="949" left="183" width="526" height="13" font="1">data dependence graph between kernels, with edge weights representing the amount of</text>
<text top="967" left="183" width="507" height="13" font="1">shared data, can provide the compiler the necessary information to create a schedule.</text>
<text top="994" left="183" width="262" height="13" font="11"><i>Generating Code for Reduction Operations</i></text>
<text top="994" left="460" width="250" height="13" font="1">Reduction operations may be more effec-</text>
<text top="1012" left="183" width="526" height="13" font="1">tively done on the CPU, even if the partial results need to be transferred from the GPU,</text>
<text top="1031" left="183" width="526" height="13" font="1">because low occupancy in the later parts of a reduction operation can result in sub-</text>
<text top="1049" left="183" width="526" height="13" font="1">optimal GPU utilization. However, the data movement cost might dominate if the results</text>
<text top="1067" left="183" width="526" height="13" font="1">are to be used back on the GPUs. The compiler will need to consider this tradeoff and</text>
</page>
<page number="7" position="absolute" top="0" left="0" height="1263" width="892">
<text top="385" left="266" width="91" height="11" font="5">(a) Vector addition</text>
<text top="385" left="543" width="76" height="11" font="5">(b) Dot product</text>
<text top="416" left="270" width="350" height="11" font="4"><b>Figure 3. </b>Performance of code generated by prototype Harlan compiler.</text>
<text top="448" left="183" width="526" height="13" font="1">may choose to generate code that employs a hybrid strategy of using both CPU and GPU</text>
<text top="466" left="183" width="223" height="13" font="1">for completing a reduction operation.</text>
<text top="518" left="183" width="89" height="13" font="7"><b>3. Evaluation</b></text>
<text top="718" left="481" width="58" height="13" font="1">Figure 4.:</text>
<text top="720" left="543" width="147" height="11" font="5">Mandelbrot on CPU and GPU.</text>
<text top="552" left="183" width="237" height="13" font="1">As proof of concept, we studied three</text>
<text top="569" left="183" width="237" height="13" font="1">benchmarks in Harlan, vector addition,</text>
<text top="587" left="183" width="237" height="13" font="1">vector dot product, and Mandelbrot set</text>
<text top="605" left="183" width="237" height="13" font="1">generation. We evaluated their perfor-</text>
<text top="623" left="183" width="237" height="13" font="1">mance on 2.8 GHz Quad-Core Intel</text>
<text top="641" left="183" width="237" height="13" font="1">Xeon with 8 GB 1066 MHz DDR3</text>
<text top="659" left="183" width="237" height="13" font="1">RAM and ATI Radeon HD 5770 graph-</text>
<text top="677" left="183" width="237" height="13" font="1">ics processors with 1024 MB memory,</text>
<text top="695" left="183" width="237" height="13" font="1">running Mac OS X Lion 10.7.1. Fig-</text>
<text top="712" left="183" width="19" height="13" font="1">ure</text>
<text top="712" left="207" width="7" height="13" font="8"><a href="pdfxml.html#7">3</a></text>
<text top="712" left="220" width="201" height="13" font="1">shows the GPU running times of</text>
<text top="730" left="183" width="237" height="13" font="1">OpenCL generated by our compiler for</text>
<text top="748" left="183" width="526" height="13" font="1">two of the benchmarks, vector addition and dot product, for increasing vector sizes. Fig-</text>
<text top="766" left="183" width="19" height="13" font="1">ure</text>
<text top="766" left="206" width="7" height="13" font="8"><a href="pdfxml.html#7">4</a></text>
<text top="766" left="217" width="492" height="13" font="1">shows running times of compiler-generated OpenCL code for Mandelbrot on CPU</text>
<text top="784" left="183" width="526" height="13" font="1">and GPU. Unsurprisingly, running OpenCL on GPUs is faster than running the OpenCL</text>
<text top="802" left="183" width="526" height="13" font="1">on CPU. The slight discontinuity of Mandelbrot times on the GPU seems to be an artefact</text>
<text top="820" left="183" width="410" height="13" font="1">of the ATI Radeon’s memory-hierarchy optimization called <i>fastpath</i>.</text>
<text top="872" left="183" width="90" height="13" font="7"><b>4. Conclusion</b></text>
<text top="906" left="183" width="527" height="13" font="1">General-purpose GPU programming is arduous on multiple fronts, requiring program-</text>
<text top="924" left="183" width="526" height="13" font="1">mers to manually program data layout, memory accesses, and boiler-plate code, among</text>
<text top="942" left="183" width="526" height="13" font="1">other details. Even with state-of-the-art technologies such as CUDA or OpenCL, atten-</text>
<text top="959" left="183" width="526" height="13" font="1">tion to such detail is necessary to enable GPUs to achieve their performance potential. At</text>
<text top="977" left="183" width="526" height="13" font="1">the same time, data orchestrations necessary for optimal GPU access may not be human-</text>
<text top="995" left="183" width="526" height="13" font="1">friendly, or even directly related to the problem being solved. For example, strided mem-</text>
<text top="1013" left="183" width="526" height="13" font="1">ory accesses may incur an order-of-magnitude performance penalty compared to unit</text>
<text top="1031" left="183" width="526" height="13" font="1">strides. Similarly, data movement from the GPU to main memory may require double or</text>
<text top="1049" left="183" width="526" height="13" font="1">triple buffering to hide DMA latency. Ideally, such data access issues should be managed</text>
<text top="1067" left="183" width="409" height="13" font="1">automatically by the compiler, assisted by programmer declarations.</text>
</page>
<page number="8" position="absolute" top="0" left="0" height="1263" width="892">
	<fontspec id="17" size="9" family="Courier" color="#00007f"/>
<text top="231" left="210" width="499" height="13" font="1">In this paper we introduced Harlan, which affords a <i>declarative approach to GPGPU</i></text>
<text top="249" left="183" width="526" height="13" font="11"><i>programming</i>, allowing users to specify the “what” not the “how” of data layout, data</text>
<text top="267" left="183" width="526" height="13" font="1">movement, and computation scheduling and coordination. Consider the normally difﬁ-</text>
<text top="285" left="183" width="526" height="13" font="1">cult task of setting up blocks of threads on a GPU for optimal efﬁciency. Using a declar-</text>
<text top="303" left="183" width="527" height="13" font="1">ative approach, the programmer can specify what computation needs to be performed,</text>
<text top="321" left="183" width="526" height="13" font="1">but let the language and runtime determine “how” the computation should be broken into</text>
<text top="339" left="183" width="439" height="13" font="1">thread blocks (perhaps using a combination of autotuning and heuristics).</text>
<text top="357" left="210" width="499" height="13" font="1">The declarative approach is especially promising for hybrid CPU/GPU clusters, in</text>
<text top="375" left="183" width="526" height="13" font="1">which movement of data between compute nodes adds even more complexity. Our end</text>
<text top="393" left="183" width="526" height="13" font="1">goal is a system that removes much of the “accidental” or “artifactual” burden of pro-</text>
<text top="411" left="183" width="526" height="13" font="1">gramming large-scale hybrid resources (such as Roadrunner or Tianhe-1A), without sac-</text>
<text top="429" left="183" width="127" height="13" font="1">riﬁcing performance.</text>
<text top="482" left="183" width="70" height="13" font="7"><b>References</b></text>
<text top="517" left="189" width="14" height="11" font="5">[1]</text>
<text top="517" left="215" width="494" height="11" font="5">G. E. Blelloch. Nesl: A nested data-parallel language (version 3.1). Technical report, Pittsburgh, PA,</text>
<text top="532" left="215" width="57" height="11" font="5">USA, 1995.</text>
<text top="547" left="189" width="14" height="11" font="5">[2]</text>
<text top="547" left="215" width="414" height="11" font="5">G. E. Blelloch. Programming parallel algorithms. <i>Commun. ACM</i>, 39(3):85–97, 1996.</text>
<text top="562" left="189" width="14" height="11" font="5">[3]</text>
<text top="562" left="215" width="494" height="11" font="5">M. Boyer, K. Skadron, and W. Weimer. Automated Dynamic Analysis of CUDA Programs. In <i>Third</i></text>
<text top="577" left="215" width="303" height="11" font="6"><i>Workshop on Software Tools for MultiCore Systems</i>, Apr. 2008.</text>
<text top="592" left="189" width="14" height="11" font="5">[4]</text>
<text top="592" left="215" width="494" height="11" font="5">S. Carrillo, J. Siegel, and X. Li. A control-structure splitting optimization for GPGPU. In <i>Proceedings</i></text>
<text top="607" left="215" width="359" height="11" font="6"><i>of the 6th ACM Symposium on Computing Frontiers</i>, pages 147–150, 2009.</text>
<text top="622" left="189" width="14" height="11" font="5">[5]</text>
<text top="622" left="215" width="494" height="11" font="5">B. C. Catanzaro, M. Garland, and K. Keutzer. Copperhead: compiling an embedded data parallel lan-</text>
<text top="637" left="215" width="389" height="11" font="5">guage. In C. Cascaval and P.-C. Yew, editors, <i>PPOPP</i>, pages 47–56. ACM, 2011.</text>
<text top="652" left="189" width="14" height="11" font="5">[6]</text>
<text top="652" left="215" width="494" height="11" font="5">M. M. T. Chakravarty, G. Keller, R. Lechtchinsky, and W. Pfannenstiel. Nepal - nested data paral-</text>
<text top="667" left="215" width="494" height="11" font="5">lelism in haskell. In <i>Proceedings of the 7th International Euro-Par Conference Manchester on Parallel</i></text>
<text top="682" left="215" width="379" height="11" font="6"><i>Processing</i>, Euro-Par ’01, pages 524–534, London, UK, 2001. Springer-Verlag.</text>
<text top="697" left="189" width="14" height="11" font="5">[7]</text>
<text top="697" left="215" width="494" height="11" font="5">K. Chandy and C. Kesselman. CC++: A declarative concurrent object oriented programming notation.</text>
<text top="712" left="215" width="369" height="11" font="6"><i>Research Directions in Concurrent Object-Oriented Programming</i>, Jan 1993.</text>
<text top="727" left="189" width="14" height="11" font="5">[8]</text>
<text top="727" left="215" width="494" height="11" font="5">J. Dean and S. Ghemawat. Mapreduce: Simpliﬁed data processing on large clusters. In <i>OSDI</i>, pages</text>
<text top="742" left="215" width="75" height="11" font="5">137–150, 2004.</text>
<text top="756" left="189" width="14" height="11" font="5">[9]</text>
<text top="756" left="215" width="494" height="11" font="5">H. Dietz and B. Young. Mimd interpretation on a GPU. In G. Gao, L. Pollock, J. Cavazos, and X. Li,</text>
<text top="771" left="215" width="494" height="11" font="5">editors, <i>Languages and Compilers for Parallel Computing</i>, volume 5898 of <i>Lecture Notes in Computer</i></text>
<text top="786" left="215" width="277" height="11" font="6"><i>Science</i>, pages 65–79. Springer Berlin / Heidelberg, 2010.</text>
<text top="801" left="183" width="20" height="11" font="5">[10]</text>
<text top="801" left="215" width="494" height="11" font="5">I. Foster. Compositional parallel programming languages. <i>ACM Transactions on Programming Lan-</i></text>
<text top="816" left="215" width="226" height="11" font="6"><i>guages and Systems (TOPLAS</i>, 18(4), Jul 1996.</text>
<text top="831" left="183" width="20" height="11" font="5">[11]</text>
<text top="831" left="215" width="493" height="11" font="5">E. Holk, W. E. Byrd, J. Willcock, T. Hoeﬂer, A. Chauhan, and A. Lumsdaine. Kanor – A Declarative</text>
<text top="846" left="215" width="494" height="11" font="5">Language for Explicit Communication. In <i>Thirteenth International Symposium on Practical Aspects of</i></text>
<text top="861" left="215" width="293" height="11" font="6"><i>Declarative Languages (PADL’11)</i>, Austin, Texas, Jan. 2011.</text>
<text top="876" left="183" width="20" height="11" font="5">[12]</text>
<text top="876" left="215" width="494" height="11" font="5">B. Jang, D. Schaa, P. Mistry, and D. Kaeli. Exploiting memory access patterns to improve memory</text>
<text top="891" left="215" width="494" height="11" font="5">performance in data-parallel architectures. <i>IEEE Transactions on Parallel and Distributed Systems</i>,</text>
<text top="906" left="215" width="90" height="11" font="5">22:105–118, 2011.</text>
<text top="921" left="183" width="20" height="11" font="5">[13]</text>
<text top="921" left="215" width="443" height="11" font="5">Khronos OpenCL Working Group. <i>The OpenCL Speciﬁcation, version 1.1</i>, September 2010.</text>
<text top="936" left="183" width="20" height="11" font="5">[14]</text>
<text top="936" left="215" width="430" height="11" font="5">NVIDIA Corporation. <i>NVIDIA CUDA Reference Manual, version 3.2 Beta</i>, August 2010.</text>
<text top="951" left="183" width="20" height="11" font="5">[15]</text>
<text top="951" left="215" width="494" height="11" font="5">NVIDIA Corporation. <i>NVIDIA CUDA C Programming Guide</i>, version 4.0 edition, Feb. 2011. Included</text>
<text top="966" left="215" width="190" height="11" font="5">with CUDA 4.0 SDK release candidate.</text>
<text top="981" left="183" width="20" height="11" font="5">[16]</text>
<text top="981" left="215" width="494" height="11" font="5">S. Peyton Jones. Harnessing the multicores: Nested data parallelism in haskell. In <i>Proceedings of the</i></text>
<text top="995" left="215" width="494" height="11" font="6"><i>6th Asian Symposium on Programming Languages and Systems</i>, APLAS ’08, pages 138–138, Berlin,</text>
<text top="1010" left="215" width="167" height="11" font="5">Heidelberg, 2008. Springer-Verlag.</text>
<text top="1025" left="183" width="20" height="11" font="5">[17]</text>
<text top="1025" left="215" width="494" height="11" font="5">J. Sanders and E. Kandrot. <i>CUDA by Example: An Introduction to General-Purpose GPU Programming</i>.</text>
<text top="1040" left="215" width="197" height="11" font="5">Addison-Wesley Professional, July 2010.</text>
<text top="1055" left="183" width="20" height="11" font="5">[18]</text>
<text top="1055" left="215" width="202" height="11" font="5">Top500.org. Top500 list, November 2010.</text>
<text top="1056" left="422" width="252" height="9" font="17"><a href="http://www.top500.org/lists/2010/11">http://www.top500.org/lists/2010/11</a></text>
<text top="1055" left="673" width="3" height="11" font="5"><a href="http://www.top500.org/lists/2010/11">.</a></text>
</page>
</pdf2xml>
