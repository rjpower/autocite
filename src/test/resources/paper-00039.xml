<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE pdf2xml SYSTEM "pdf2xml.dtd">

<pdf2xml>
<page number="1" position="absolute" top="0" left="0" height="1209" width="919">
	<fontspec id="0" size="22" family="Times" color="#ffffff"/>
	<fontspec id="1" size="12" family="Times" color="#231f20"/>
	<fontspec id="2" size="25" family="Times" color="#231f20"/>
	<fontspec id="3" size="14" family="Times" color="#231f20"/>
	<fontspec id="4" size="10" family="Times" color="#231f20"/>
	<fontspec id="5" size="12" family="Times" color="#231f20"/>
	<fontspec id="6" size="5" family="Times" color="#231f20"/>
<text top="67" left="79" width="218" height="24" font="0">RESEARCH PAPERS</text>
<text top="782" left="477" width="371" height="14" font="1">accuracy,  less  noise  sensitivity,  more  flexibility  and </text>
<text top="805" left="477" width="371" height="14" font="1">compatibility  with  different  types  of  processors.  These </text>
<text top="828" left="477" width="371" height="14" font="1">digital implementations can be done either with a digital </text>
<text top="851" left="477" width="371" height="14" font="1">signal processor or FPGA or programmable logic design. </text>
<text top="874" left="477" width="371" height="14" font="1">An  FPGA-based  implementation,  would  be  the  best </text>
<text top="897" left="477" width="371" height="14" font="1">choice from the previously mention platforms since it can </text>
<text top="920" left="477" width="371" height="14" font="1">work in parallel as is the case of ANNs behavior (Cantrell &amp; </text>
<text top="943" left="477" width="371" height="14" font="1">Wurtz, 1993)(Baker &amp; Hammerstrom, 1989)(Blais &amp; Mertz, </text>
<text top="966" left="477" width="371" height="14" font="1">2001)(Vargas,  Barba,  Torres  &amp;  Mattos,  2011).  Previous </text>
<text top="989" left="477" width="371" height="14" font="1">research on implementing various kinds of neural networks </text>
<text top="1012" left="477" width="371" height="14" font="1">on the HDL platform in (Ali, &amp; Mohammed, 2010)(Omondi &amp; </text>
<text top="1035" left="477" width="371" height="14" font="1">Rajapakse, 2006)(Izeboudjen, Farah, Bessalah, Bouridane </text>
<text top="1058" left="477" width="371" height="14" font="1">&amp; Chikhi, 2008)(Schemmel, Meier &amp; Schurmann, 2001) has </text>
<text top="1081" left="477" width="371" height="14" font="1">focused  on  developing  the  neuron  models  and  their </text>
<text top="782" left="76" width="98" height="14" font="1">INTRODUCTION</text>
<text top="809" left="76" width="371" height="14" font="1">The neuroscience, study of the human brain, is thousands </text>
<text top="832" left="76" width="371" height="14" font="1">of years old. This fascination with the human brain has led </text>
<text top="855" left="76" width="371" height="14" font="1">to  the  development  of  Artificial  Neural  Networks  (ANNs) </text>
<text top="878" left="76" width="371" height="14" font="1">which  have  been  made  possible  due  to  advances  in </text>
<text top="901" left="76" width="371" height="14" font="1">electronics. ANNs have been used successfully in a broad </text>
<text top="924" left="76" width="371" height="14" font="1">spectrum  of  applications  such  as  pattern  recognition, </text>
<text top="947" left="76" width="371" height="14" font="1">data classification, control systems signal processing and </text>
<text top="970" left="76" width="371" height="14" font="1">functional  approximations,  etc.  Much  work  has  been </text>
<text top="993" left="76" width="371" height="14" font="1">done in these fields that rely on software simulations and </text>
<text top="1016" left="76" width="371" height="14" font="1">investigating  the  capabilities  of  the  ANN  models  using </text>
<text top="1039" left="76" width="371" height="14" font="1">both  analog  and  digital  implementations  (Torres-Huitzil, </text>
<text top="1061" left="76" width="371" height="14" font="1">Girau,  &amp;  Gauffriau,  2007).  Digital  implementations  are </text>
<text top="1084" left="76" width="371" height="14" font="1">more  popular  for  their  basic  advantages  of  higher </text>
<text top="137" left="147" width="627" height="27" font="2">DESIGN ENHANCEMENT OF COMBINATIONAL NEURAL</text>
<text top="169" left="170" width="581" height="27" font="2">NETWORKS USING HDL BASED FPGA FRAMEWORK</text>
<text top="202" left="294" width="331" height="27" font="2">FOR PATTERN RECOGNITION</text>
<text top="290" left="196" width="145" height="17" font="3">PRIYANKA MEKALA *</text>
<text top="325" left="78" width="764" height="12" font="4"><i>* Research Assistant and PhD Candidate, Department of Electrical and Computer Engineering, Florida International University, Miami, FL, USA.</i></text>
<text top="339" left="133" width="654" height="12" font="4"><i>** Assistant Professor, Department of Electrical and Computer Engineering, Florida International University, Miami, FL, USA.</i></text>
<text top="369" left="429" width="63" height="14" font="5"><i>ABSTRACT</i></text>
<text top="396" left="76" width="772" height="14" font="5"><i>The fast emerging highly-integrated multimedia devices require complex video/image processing tasks leading to a </i></text>
<text top="419" left="76" width="772" height="14" font="5"><i>very challenging design process; as it demands more efficient and high processing systems. Neural networks are used in </i></text>
<text top="442" left="76" width="772" height="14" font="5"><i>many of these imaging applications to represent the complex input-output relationships. Software implementation of </i></text>
<text top="465" left="76" width="772" height="14" font="5"><i>these networks attain accuracy with tradeoffs between processing performance (to achieve specified frame rates, </i></text>
<text top="487" left="76" width="772" height="14" font="5"><i>working on large image data sets), power and cost constraints. The current trends involve conventional processor being </i></text>
<text top="510" left="76" width="772" height="14" font="5"><i>replaced by the Field programmable gate array (FPGA) systems due to their high performance when processing huge </i></text>
<text top="533" left="76" width="772" height="14" font="5"><i>amount of data.  The goal is to design the Combinational Neural Networks (CNN) for pattern recognition using an FPGA </i></text>
<text top="556" left="76" width="772" height="14" font="5"><i>based platform for accelerated performance. The enhancement in speed and computation from the hardware is </i></text>
<text top="579" left="76" width="772" height="14" font="5"><i>being compared to the software (using MATLAB) model. The employment of HDL on the FPGA enables operations to be </i></text>
<text top="602" left="76" width="772" height="14" font="5"><i>performed in parallel. Thus allowing the exploitation of the vast parallelism found in many real-world applications such as </i></text>
<text top="625" left="76" width="772" height="14" font="5"><i>in robotics, controller free gaming and sign/gesture recognition. As a validation of the CNN hardware model a case </i></text>
<text top="648" left="76" width="772" height="14" font="5"><i>study in pattern recognition is being explored and implemented on Xilinx Spartan 3E FPGA board. To measure the quality </i></text>
<text top="671" left="76" width="772" height="14" font="5"><i>of learning in the trained network mean squared error is used. The processing performance of this non-linear stochastic </i></text>
<text top="694" left="76" width="772" height="14" font="5"><i>tool is determined by comparing the HDL (parallel model) simulations with the MATLAB design (sequential model). The </i></text>
<text top="717" left="76" width="436" height="14" font="5"><i>gain in training time and memory used for processing is also derived.</i></text>
<text top="744" left="76" width="571" height="14" font="5"><i>Keywords: VHDL, Combinatorial Neural Networks, Back Propagation, Pattern Recognition.</i></text>
<text top="263" left="451" width="17" height="17" font="3">By</text>
<text top="290" left="597" width="111" height="17" font="3">JEFFREY FAN **</text>
<text top="1140" left="86" width="7" height="12" font="4"><i>6</i></text>
<text top="1140" left="323" width="288" height="12" font="4"><i>i-manager’s Journal o  Electronics Engineering, Vol.</i></text>
<text top="1140" left="445" width="7" height="12" font="4"><i>n</i></text>
<text top="1140" left="611" width="3" height="12" font="4"><i> </i></text>
<text top="1142" left="624" width="34" height="7" font="6">l</text>
<text top="1142" left="666" width="34" height="7" font="6">l</text>
<text top="1142" left="672" width="2" height="8" font="6"> </text>
<text top="1140" left="615" width="221" height="12" font="4"><i>2   No. 1  September - November 2011</i></text>
</page>
<page number="2" position="absolute" top="0" left="0" height="1209" width="919">
	<fontspec id="7" size="10" family="Times" color="#231f20"/>
	<fontspec id="8" size="8" family="Times" color="#231f20"/>
<text top="67" left="79" width="218" height="24" font="0">RESEARCH PAPERS</text>
<text top="144" left="477" width="371" height="14" font="1">are adaptation, parallelism, classification, optimization and </text>
<text top="167" left="477" width="371" height="14" font="1">generalization.  The debate over whether to build a generic </text>
<text top="190" left="477" width="371" height="14" font="1">system  that  can  be  reprogrammed  on  user  demands/ </text>
<text top="212" left="477" width="371" height="14" font="1">applications  or  a  single  specialized  dedicated  to  one </text>
<text top="235" left="477" width="371" height="14" font="1">application  with  high  speed  performance  still  prevails </text>
<text top="258" left="477" width="180" height="14" font="1">(Omondi, &amp; Rajapakse, 2006).</text>
<text top="285" left="477" width="371" height="14" font="1">The researchers propose a HDL based design methodology </text>
<text top="308" left="477" width="371" height="14" font="1">to tradeoff the high level application requirements and the </text>
<text top="331" left="477" width="371" height="14" font="1">low  level  FPGA  hardware  for  patter  recognition.  The  HDL </text>
<text top="354" left="477" width="371" height="14" font="1">description  has  the  advantages  of  being  generic,  flexible, </text>
<text top="377" left="477" width="371" height="14" font="1">dynamic reconfiguration on user demand and useful to gain </text>
<text top="400" left="477" width="371" height="14" font="1">more control of parallel processes. Efficient reusability and </text>
<text top="423" left="477" width="371" height="14" font="1">performance is derived by providing the characteristics of </text>
<text top="446" left="477" width="371" height="14" font="1">entities  into  a  model  library  (Izeboudjen,  Farah,  Bessalah, </text>
<text top="469" left="477" width="371" height="14" font="1">Bouridane &amp; Chikhi, 2008). Table 1 shows the comparison of </text>
<text top="491" left="477" width="371" height="14" font="1">VHDL  with  the  procedural  languages  and  outlines  the </text>
<text top="514" left="477" width="371" height="14" font="1">advantages  of  characterizing  digital  hardware  using </text>
<text top="537" left="477" width="372" height="14" font="1">hardware  description  language  based  on  entity </text>
<text top="144" left="76" width="371" height="14" font="1">validation.  The  computations  involved  mostly  the  fixed </text>
<text top="167" left="76" width="371" height="14" font="1">point  integer  rather  than  floating  point  which  results  in </text>
<text top="189" left="76" width="371" height="14" font="1">some false outputs. This can be fixed by introducing libraries </text>
<text top="212" left="76" width="371" height="14" font="1">defining  the  float  type  variables  and  vectors.  Pattern </text>
<text top="235" left="76" width="371" height="14" font="1">recognition  using  the  neural  networks  is  dealt  recently  in </text>
<text top="258" left="76" width="371" height="14" font="1">(Vargas, Barba, Torres &amp; Mottos, 2011). The values of pixels </text>
<text top="281" left="76" width="371" height="14" font="1">of an image frame were used as inputs for recognition </text>
<text top="304" left="76" width="372" height="14" font="1">t h u s  c a u s i n g  i n c r e a s e d  m e m o r y  u s a g e  a n d </text>
<text top="327" left="76" width="371" height="14" font="1">computations. We choose to perform the recognition on </text>
<text top="350" left="76" width="371" height="14" font="1">the bitmapped (depth 4) image rather than the grayscale </text>
<text top="373" left="76" width="371" height="14" font="1">(8 bits) image. Gain in bandwidth is achieved in terms of </text>
<text top="396" left="76" width="371" height="14" font="1">memory storage. Also, once the image is preprocessed, </text>
<text top="419" left="76" width="371" height="14" font="1">the  features  are  extracted  and  used  as  inputs  in  this </text>
<text top="442" left="76" width="371" height="14" font="1">architecture proposed. In this paper, the authors present the </text>
<text top="465" left="76" width="371" height="14" font="1">new  generic  design  of  Combinational  Neural  Network </text>
<text top="488" left="76" width="371" height="14" font="1">(CNN) proposed in earlier research (Mekala, Erdogan &amp; Fan, </text>
<text top="511" left="76" width="371" height="14" font="1">2010)  for  pattern  recognition  based  on  Xilinx  Spartan  3E </text>
<text top="534" left="76" width="371" height="14" font="1">board using VHDL model called HDL-CNN. The simulation of </text>
<text top="556" left="76" width="371" height="14" font="1">VHDL  models  are  facilitated  by  the  use  of  stimulus </text>
<text top="579" left="76" width="371" height="14" font="1">sequences and checkers (e.g., VHDL test benches, mean </text>
<text top="602" left="76" width="371" height="14" font="1">square  error).  The  training  time  and  computations </text>
<text top="625" left="76" width="371" height="14" font="1">variations (which depend on global parameters defined </text>
<text top="648" left="76" width="371" height="14" font="1">by the user) are analyzed and displayed in later sections of </text>
<text top="671" left="76" width="371" height="14" font="1">this paper. Comparison is made in order to establish the </text>
<text top="694" left="76" width="298" height="14" font="1">performance in speed of the model proposed.</text>
<text top="721" left="76" width="371" height="14" font="1">The rest of the paper is as follows: section 2 presents the </text>
<text top="744" left="76" width="371" height="14" font="1">design progression using HDL and FPGA logistics, section 3 </text>
<text top="767" left="76" width="371" height="14" font="1">explains the Combinational Neural Networks (CNN) and </text>
<text top="790" left="76" width="371" height="14" font="1">HDL-CNN, section 4 explains the Sign/Gesture recognition </text>
<text top="812" left="76" width="371" height="14" font="1">model,  section  5  presents  the  result,  and,  section  6 </text>
<text top="835" left="76" width="138" height="14" font="1">concludes the paper.</text>
<text top="862" left="76" width="204" height="14" font="1">1. Design progression using HDL</text>
<text top="889" left="76" width="371" height="14" font="1">The first question that comes to mind is: Why use a high level </text>
<text top="912" left="76" width="371" height="14" font="1">design methodology (such as HDL) for CNN implementation </text>
<text top="935" left="76" width="371" height="14" font="1">as  opposed  to  other  object–oriented  simulations.  The </text>
<text top="958" left="76" width="371" height="14" font="1">answer  would  be  that  high  speed  processing  can  be </text>
<text top="981" left="76" width="371" height="14" font="1">achieved through dedicated hardware working in parallel </text>
<text top="1003" left="76" width="371" height="14" font="1">which can be implemented on FPGAs using HDL. ANNs are </text>
<text top="1026" left="76" width="367" height="14" font="1">powerful systems capable of modeling the complex input-</text>
<text top="1049" left="76" width="371" height="14" font="1">output  relationships.  Information  is  processed  via  the </text>
<text top="1072" left="76" width="371" height="14" font="1">mathematical models using the interconnections of neurons. </text>
<text top="1095" left="76" width="371" height="14" font="1">Some interesting features displayed by the network engine </text>
<text top="1097" left="553" width="217" height="12" font="7">Table 1. VHDL vs. Procedural languages</text>
<text top="1140" left="826" width="7" height="12" font="4"><i>7</i></text>
<text top="1142" left="381" width="34" height="7" font="6">l</text>
<text top="1140" left="80" width="288" height="12" font="4"><i>i-manager’s Journal o  Electronics Engineering, Vol.</i></text>
<text top="1140" left="388" width="3" height="11" font="4"><i> </i></text>
<text top="1140" left="391" width="33" height="12" font="4"><i>No. 1 </i></text>
<text top="1142" left="423" width="34" height="7" font="6">l</text>
<text top="1142" left="429" width="2" height="8" font="6"> </text>
<text top="1140" left="202" width="7" height="12" font="4"><i>n</i></text>
<text top="1140" left="368" width="14" height="12" font="4"><i> 2 </i></text>
<text top="1140" left="431" width="161" height="12" font="4"><i>September - November 2011</i></text>
<text top="811" left="478" width="176" height="11" font="8">VHDL  provide  ways  to  describe </text>
<text top="824" left="478" width="176" height="11" font="8">propagation  of  time  and  signal </text>
<text top="838" left="478" width="176" height="11" font="8">dependencies. Hardware oriented – </text>
<text top="851" left="478" width="176" height="11" font="8">Digital  logic  design  (The  operations </text>
<text top="864" left="478" width="176" height="11" font="8">and  structure  are  described  in  gate </text>
<text top="877" left="478" width="172" height="11" font="8">level and RT level – hierarchal design).</text>
<text top="898" left="478" width="177" height="11" font="8">VHDL  supports  unsynthesizable </text>
<text top="911" left="478" width="176" height="11" font="8">constructs  that  are  useful  in  writing </text>
<text top="924" left="478" width="176" height="11" font="8">high-level models, test benches and </text>
<text top="937" left="478" width="176" height="11" font="8">other non-hardware artifacts needed </text>
<text top="950" left="478" width="118" height="11" font="8">in hardware logic design. </text>
<text top="973" left="478" width="176" height="11" font="8">VHDL has static type checking-many </text>
<text top="986" left="478" width="176" height="11" font="8">errors can be caught before synthesis </text>
<text top="1000" left="478" width="83" height="11" font="8">and/or simulation.</text>
<text top="1021" left="478" width="176" height="11" font="8">VHDL  has  a  rich  collection  of  data </text>
<text top="1034" left="478" width="176" height="11" font="8">types  and  well-defined  standard </text>
<text top="1047" left="478" width="176" height="11" font="8">with  a  full-featured  language  and </text>
<text top="1061" left="478" width="177" height="11" font="8">module  system  (libraries  and </text>
<text top="1074" left="478" width="54" height="11" font="8">packages).</text>
<text top="811" left="670" width="176" height="11" font="8">No way to describe time and signal </text>
<text top="824" left="670" width="176" height="11" font="8">dependency.  Software  oriented- </text>
<text top="838" left="670" width="177" height="11" font="8">Binar y  executable  (Data  flow </text>
<text top="851" left="670" width="173" height="11" font="8">language and non-hierarchal design)</text>
<text top="898" left="670" width="176" height="11" font="8">Explicit  constructs  and  assignments </text>
<text top="911" left="670" width="176" height="11" font="8">are not supported by the procedural </text>
<text top="924" left="670" width="53" height="11" font="8">languages.</text>
<text top="973" left="670" width="176" height="11" font="8">Errors  can  be  analyzed  only  after </text>
<text top="986" left="670" width="176" height="11" font="8">debugging. Synthesis errors are hard </text>
<text top="1000" left="670" width="46" height="11" font="8">to debug.</text>
<text top="1021" left="670" width="176" height="11" font="8">Object  oriented  programs  are  written </text>
<text top="1034" left="670" width="176" height="11" font="8">with pure logical or algorithmic thinking. </text>
<text top="1047" left="670" width="176" height="11" font="8">Inherently procedural (single-threaded), </text>
<text top="1061" left="670" width="176" height="11" font="8">with  limited  syntactical  and  semantic </text>
<text top="1074" left="670" width="136" height="11" font="8">support to handle concurrency.</text>
<text top="568" left="492" width="135" height="11" font="8">VHDL (Hardware descriptive)</text>
<text top="584" left="479" width="170" height="11" font="8">VHDL contains components that are</text>
<text top="597" left="478" width="195" height="11" font="8">concurrent i.e. run in parallel/ simultaneously.</text>
<text top="568" left="654" width="188" height="11" font="8">Procedural languages (C, C++ , MATLAB)</text>
<text top="584" left="683" width="162" height="11" font="8">Traditional software languages like C,</text>
<text top="597" left="682" width="152" height="11" font="8">C++, and MATLAB are sequential.</text>
</page>
<page number="3" position="absolute" top="0" left="0" height="1209" width="919">
	<fontspec id="9" size="5" family="Times" color="#231f20"/>
	<fontspec id="10" size="12" family="Times" color="#ed1c28"/>
	<fontspec id="11" size="12" family="Times" color="#48b842"/>
	<fontspec id="12" size="8" family="Times" color="#231f20"/>
	<fontspec id="13" size="3" family="Times" color="#231f20"/>
	<fontspec id="14" size="3" family="Times" color="#231f20"/>
	<fontspec id="15" size="7" family="Times" color="#231f20"/>
	<fontspec id="16" size="2" family="Times" color="#231f20"/>
<text top="67" left="79" width="218" height="24" font="0">RESEARCH PAPERS</text>
<text top="434" left="477" width="337" height="14" font="1">linear function of multiply and accumulate as follows:</text>
<text top="461" left="801" width="16" height="14" font="1">(1)</text>
<text top="487" left="477" width="371" height="14" font="1">The  synaptic  weight  of  the  connection  is  given  by  w ; </text>
<text top="495" left="836" width="4" height="7" font="6">ki</text>
<text top="510" left="477" width="371" height="14" font="1">where 'p' is the number of incoming inputs to the neuron. </text>
<text top="533" left="477" width="367" height="14" font="1">The output of the model is given by y  given by the pre-</text>
<text top="541" left="723" width="3" height="7" font="6">k</text>
<text top="556" left="477" width="371" height="14" font="1">output  passed  through  the  activation  function  φ(.) </text>
<text top="579" left="477" width="322" height="14" font="1">(Sigmoid function defined in Eq. 3) as shown below:</text>
<text top="607" left="611" width="52" height="13" font="1">j</text>
<text top="607" left="624" width="45" height="13" font="1">n</text>
<text top="606" left="801" width="16" height="14" font="1">(2)</text>
<text top="632" left="477" width="217" height="14" font="5"><i>2.2 HDL- CNN architecture Model</i></text>
<text top="659" left="477" width="371" height="14" font="1">The  CNN  is  built  on  the  basic  network  of  a  back </text>
<text top="682" left="477" width="371" height="14" font="1">propagation  described  in  previous  research  (Mekala, </text>
<text top="705" left="477" width="371" height="14" font="1">Erdogan  &amp;  Fan,  2010).  The  features  extracted  from  the </text>
<text top="728" left="477" width="371" height="14" font="1">prior  module  block  are  divided  into  classes  or  stages </text>
<text top="751" left="477" width="371" height="14" font="1">where a set of features describe some information on the </text>
<text top="774" left="477" width="371" height="14" font="1">probability of the decision of the recognition. Hence from </text>
<text top="797" left="477" width="371" height="14" font="1">a  set  of  <i>M</i>  actual  features  to  be  extracted,  the </text>
<text top="820" left="477" width="371" height="14" font="1">probabilistic decision is made on three levels with first level </text>
<text top="843" left="477" width="371" height="14" font="1">monitoring the other two levels. Each level is fed with a </text>
<text top="865" left="477" width="371" height="14" font="1">vector <i>V</i> of variable length. Hence there exists vector V of </text>
<text top="888" left="477" width="371" height="14" font="1">size <i>L1</i>, <i>L2</i> and <i>L3</i>. Since the platform is being designed to </text>
<text top="911" left="477" width="371" height="14" font="1">serve as a generic model and flexible to user demands, </text>
<text top="934" left="477" width="371" height="14" font="1">the  value  of  <i>M</i>  varies  from  application  to  application </text>
<text top="957" left="477" width="371" height="14" font="1">depending on the linearity of the output classes. A parallel </text>
<text top="980" left="477" width="371" height="14" font="1">communication  bus  is  provided  between  the  feature </text>
<text top="1003" left="477" width="371" height="14" font="1">extraction  layer  and  the  three  level  CNN  recognition </text>
<text top="1026" left="477" width="371" height="14" font="1">model in order to allow the flow of the three level sets of the </text>
<text top="1049" left="477" width="371" height="14" font="1">vector  data  as  well  as  an  initialization  clock  signal  to </text>
<text top="1072" left="477" width="371" height="14" font="1">choose either level2 or level3 once the decision on level1 </text>
<text top="1095" left="477" width="371" height="14" font="1">is  made  (Mekala,  Erdogan  &amp;  Fan,  2010).  The  time  and </text>
<text top="607" left="585" width="6" height="13" font="5"><i>y</i></text>
<text top="606" left="594" width="44" height="14" font="1"> =  ( )</text>
<text top="614" left="591" width="3" height="7" font="9"><i>k</i></text>
<text top="614" left="632" width="3" height="7" font="9"><i>k</i></text>
<text top="144" left="76" width="371" height="14" font="1">connections, concurrent  operations,  propagation  delay </text>
<text top="167" left="76" width="371" height="14" font="1">and  timing  information  (Omondi  &amp;  Rajapakse,  2006) </text>
<text top="189" left="76" width="371" height="14" font="1">(Berry, 2002)(Schemmel, Meier &amp; Schurmann, 2001)(Short, </text>
<text top="212" left="76" width="154" height="14" font="1">2009)(Ashenden, 1995).</text>
<text top="239" left="76" width="371" height="14" font="1">2.  Hardware  Descriptive  Language  -  Combinational </text>
<text top="262" left="76" width="178" height="14" font="1">Neural Networks (HDL- CNN)</text>
<text top="289" left="76" width="371" height="14" font="1">The  CNN  is  a  special  class  of  ANN  being  described  as </text>
<text top="312" left="76" width="371" height="14" font="1">follows. The design resembles the tree structure in addition </text>
<text top="335" left="76" width="371" height="14" font="1">to  the  generic  architecture  of  a  neural  network.  The </text>
<text top="358" left="76" width="371" height="14" font="1">previous research on the software solution CNN design as </text>
<text top="380" left="76" width="371" height="14" font="1">proposed in (Mekala, Erdogan &amp; Fan, 2010) is based on </text>
<text top="403" left="76" width="371" height="14" font="1">the address search of the virtual memory of a CPU. This </text>
<text top="426" left="76" width="371" height="14" font="1">paper examines an alternative implementation of the CNN </text>
<text top="449" left="76" width="371" height="14" font="1">on  the  hardware  platform  called  as  the  HDL-CNN  which </text>
<text top="472" left="76" width="371" height="14" font="1">modifies the architecture with the help of VHDL design on a </text>
<text top="495" left="76" width="371" height="14" font="1">FPGA to better the performance. A basic neural network </text>
<text top="518" left="76" width="371" height="14" font="1">engine and the extension of back-propagation network on </text>
<text top="541" left="76" width="274" height="14" font="1">to the HDL- CNN model are described below.</text>
<text top="568" left="76" width="216" height="14" font="5"><i>2.1 Generic Neuron Model in HDL</i></text>
<text top="594" left="76" width="371" height="14" font="1">In  order  to  model  an  artificial  neuron  from  a  biological </text>
<text top="617" left="76" width="371" height="14" font="1">neuron, three basic components are used - input to the </text>
<text top="640" left="76" width="371" height="14" font="1">neurons,  synaptic  weights  and  activation  threshold </text>
<text top="663" left="76" width="371" height="14" font="1">function. The synapses of the biological neuron (i.e. the </text>
<text top="686" left="76" width="371" height="14" font="1">one which interconnects the neural network and gives the </text>
<text top="709" left="76" width="371" height="14" font="1">strength  of  the  connection)  are  modeled  as  synaptic </text>
<text top="732" left="76" width="371" height="14" font="1">weights.  Mathematically  they  can  be  considered  as </text>
<text top="755" left="76" width="371" height="14" font="1">functions-  two  linear  and  one  non-linear.  All  inputs  are </text>
<text top="778" left="76" width="371" height="14" font="1">modified  by  the  weights  and  summed  altogether.  This </text>
<text top="801" left="76" width="371" height="14" font="1">activity  is  referred  as  a  linear  combination.  The  linear </text>
<text top="824" left="76" width="371" height="14" font="1">combination of the input stage and aggregation is being </text>
<text top="847" left="76" width="371" height="14" font="1">modeled  as  a  simple  MAC  (multiply  and  accumulate) </text>
<text top="870" left="76" width="367" height="14" font="1">function. The output of the MAC is passed through a non-</text>
<text top="893" left="76" width="371" height="14" font="1">linear  activation  threshold  to  determine  the  output.  The </text>
<text top="915" left="76" width="371" height="14" font="1">activation function considered could be – step function </text>
<text top="938" left="76" width="371" height="14" font="1">(simplest non-linear function), ramp function or a sigmoid </text>
<text top="961" left="76" width="371" height="14" font="1">function  (Mehrotra,  Chilukuri  &amp;  Ranka,  1997)(Caudill  &amp; </text>
<text top="984" left="76" width="371" height="14" font="1">Butler,  1992)(Stergiou  &amp;  Siganos,  1996)(Dreyfus,  2005). </text>
<text top="1007" left="76" width="371" height="14" font="1">The  Figure  1  shows  the  neural  network  engine  with  the </text>
<text top="1030" left="76" width="371" height="14" font="1">three layer structure (Fausett, 1994). Each neuron receives </text>
<text top="1053" left="76" width="371" height="14" font="1">several  inputs  i.e.  <i>x </i>  and  generates  pre-output  <i>v </i>  (<i>k</i> </text>
<text top="1061" left="213" width="1" height="7" font="9"><i>i</i></text>
<text top="1061" left="420" width="3" height="7" font="9"><i>k</i></text>
<text top="1076" left="76" width="371" height="14" font="1">representing the neuron generating output) through the </text>
<text top="144" left="156" width="4" height="14" font="10"> </text>
<text top="984" left="452" width="4" height="14" font="10"> </text>
<text top="495" left="287" width="10" height="14" font="11">   </text>
<text top="408" left="551" width="219" height="12" font="7">Figure 1. Generic Neural Network Model</text>
<text top="1140" left="86" width="7" height="12" font="4"><i>8</i></text>
<text top="1140" left="323" width="288" height="12" font="4"><i>i-manager’s Journal o  Electronics Engineering, Vol.</i></text>
<text top="1140" left="445" width="7" height="12" font="4"><i>n</i></text>
<text top="1140" left="611" width="3" height="12" font="4"><i> </i></text>
<text top="1142" left="624" width="34" height="7" font="6">l</text>
<text top="1142" left="666" width="34" height="7" font="6">l</text>
<text top="1142" left="672" width="2" height="8" font="6"> </text>
<text top="1140" left="615" width="221" height="12" font="4"><i>2   No. 1  September - November 2011</i></text>
<text top="173" left="545" width="5" height="10" font="12"><i>x</i></text>
<text top="179" left="550" width="3" height="5" font="13">0</text>
<text top="204" left="546" width="5" height="10" font="12"><i>x</i></text>
<text top="210" left="550" width="3" height="5" font="13">1</text>
<text top="241" left="545" width="5" height="10" font="12"><i>x</i></text>
<text top="247" left="550" width="3" height="5" font="13">2</text>
<text top="175" left="596" width="12" height="10" font="12"><i>wk</i></text>
<text top="181" left="608" width="3" height="5" font="14"><i>0</i></text>
<text top="205" left="596" width="12" height="10" font="12"><i>wk</i></text>
<text top="211" left="608" width="3" height="5" font="14"><i>1</i></text>
<text top="243" left="596" width="12" height="10" font="12"><i>wk</i></text>
<text top="249" left="608" width="3" height="5" font="14"><i>2</i></text>
<text top="177" left="619" width="62" height="10" font="12"><i>wk = bk </i>(bias)</text>
<text top="183" left="630" width="4" height="5" font="14"><i>0 </i></text>
<text top="306" left="597" width="12" height="10" font="12"><i>wk</i></text>
<text top="312" left="608" width="3" height="5" font="14"><i>p</i></text>
<text top="311" left="552" width="4" height="9" font="15"><i>x</i></text>
<text top="316" left="556" width="2" height="4" font="16"><i>p</i></text>
<text top="322" left="544" width="23" height="11" font="8">Input</text>
<text top="335" left="540" width="31" height="11" font="8">signals</text>
<text top="322" left="584" width="40" height="11" font="8">Synaptic</text>
<text top="335" left="586" width="36" height="11" font="8">Weights</text>
<text top="276" left="649" width="45" height="11" font="8">Summing</text>
<text top="289" left="649" width="40" height="11" font="8">Junction</text>
<text top="254" left="671" width="51" height="13" font="1">S</text>
<text top="250" left="698" width="9" height="10" font="12"><i>vk</i></text>
<text top="242" left="779" width="29" height="11" font="8">Ouput</text>
<text top="254" left="789" width="9" height="10" font="12"><i>yk</i></text>
<text top="307" left="738" width="33" height="10" font="8">q</text>
<text top="307" left="744" width="5" height="10" font="12"><i>k</i></text>
<text top="319" left="721" width="44" height="11" font="8">Threshold</text>
<text top="219" left="721" width="46" height="11" font="8">Activation</text>
<text top="232" left="724" width="40" height="11" font="8">Function</text>
<text top="255" left="732" width="38" height="10" font="8">j</text>
<text top="255" left="738" width="34" height="10" font="8"> ·</text>
<text top="254" left="741" width="10" height="10" font="8">( )</text>
<text top="356" left="566" width="86" height="11" font="8">LINEAR FUNCTIONS</text>
<text top="369" left="583" width="52" height="11" font="8">MAC LAYER</text>
<text top="356" left="708" width="107" height="11" font="8">NON-LINEAR FUNCTION</text>
</page>
<page number="4" position="absolute" top="0" left="0" height="1209" width="919">
<text top="67" left="79" width="218" height="24" font="0">RESEARCH PAPERS</text>
<text top="144" left="477" width="371" height="14" font="1">change  to  influence  the  weight  change  and  α  is  the </text>
<text top="167" left="477" width="371" height="14" font="1">learning rate adopted for the training. The mathematical </text>
<text top="190" left="477" width="371" height="14" font="1">equations are given in Table 2 for the design of each level </text>
<text top="212" left="477" width="206" height="14" font="1">of CNN adopted (Fausett, 1994).</text>
<text top="239" left="477" width="371" height="14" font="1">Generally the error threshold adjustment and learning rate </text>
<text top="262" left="477" width="371" height="14" font="1">(generally between 0.5 and 0) variations adds little to the </text>
<text top="285" left="477" width="371" height="14" font="1">process; so the idea of momentum is used to boost the </text>
<text top="308" left="477" width="371" height="14" font="1">performance.  On  each  pass  through  the  layers,  the </text>
<text top="331" left="477" width="371" height="14" font="1">weight change of a matrix of synapses is influenced by the </text>
<text top="354" left="477" width="371" height="14" font="1">previous pass's weight change. The degree to which it is </text>
<text top="377" left="477" width="371" height="14" font="1">influenced  is  determined  by  the  momentum  term </text>
<text top="400" left="477" width="371" height="14" font="1">(generally  varies  between  0  and  1).  The  weight </text>
<text top="423" left="477" width="371" height="14" font="1">adjustments are made as epoch based training where at </text>
<text top="446" left="477" width="371" height="14" font="1">the  end  of  each  epoch  the  cumulative  error  is  also </text>
<text top="469" left="477" width="371" height="14" font="1">tracked.  All  the  factors  such  as  size  of  the  three  level </text>
<text top="491" left="477" width="371" height="14" font="1">vectors <i>L1, L2, L3,</i> each BP input, hidden and output layers, </text>
<text top="514" left="477" width="371" height="14" font="1">learning rate parameters are user defined and are being </text>
<text top="537" left="477" width="371" height="14" font="1">set  in  a  configuration  file.  Figure  2  shows  the  block </text>
<text top="560" left="477" width="371" height="14" font="1">diagram of the HDL- CNN model. In order to generate the </text>
<text top="144" left="76" width="371" height="14" font="1">memory consumption involved in computing the feature </text>
<text top="167" left="76" width="371" height="14" font="1">vector depends on the length of it. Thus instead of deriving </text>
<text top="189" left="76" width="371" height="14" font="1">all  the  values  of  the  vector  at  a  time,  three  levels  are </text>
<text top="212" left="76" width="371" height="14" font="1">involved in order to improve the speed and performance </text>
<text top="235" left="76" width="146" height="14" font="1">of the HDL-CNN model.</text>
<text top="262" left="76" width="371" height="14" font="1">The activation function used in the modeling of the CNN </text>
<text top="285" left="76" width="371" height="14" font="1">architecture is the sigmoid function.   Each node in the </text>
<text top="308" left="76" width="371" height="14" font="1">network receives several input values and combines them </text>
<text top="331" left="76" width="371" height="14" font="1">to  produce  an  output  value.  The  node's  activation </text>
<text top="354" left="76" width="371" height="14" font="1">function determines the manner in which these values are </text>
<text top="377" left="76" width="371" height="14" font="1">combined.  It  is  necessary  for  the  activation  function  to </text>
<text top="400" left="76" width="371" height="14" font="1">combine the input values in a non-linear manner so as to </text>
<text top="423" left="76" width="371" height="14" font="1">fit for wider range of task applications. Since each stage of </text>
<text top="446" left="76" width="371" height="14" font="1">CNN  is  constructed  using  back-propagation  network </text>
<text top="468" left="76" width="371" height="14" font="1">(three  layers-  input,  hidden  and  output  layers)  it  is </text>
<text top="491" left="76" width="371" height="14" font="1">important that the activation function used needs to be </text>
<text top="514" left="76" width="371" height="14" font="1">continuously differentiable. There exists several functions </text>
<text top="537" left="76" width="371" height="14" font="1">which  meet  this  criteria  but  the  most  commonly  used </text>
<text top="560" left="76" width="371" height="14" font="1">activation function is the sigmoid function as described </text>
<text top="583" left="76" width="240" height="14" font="1">by the Equation 3 below (Kwan, 1992).</text>
<text top="610" left="400" width="16" height="14" font="1">(3)</text>
<text top="637" left="76" width="371" height="14" font="1">It is not easy to represent sigmoid function in digital design </text>
<text top="659" left="76" width="371" height="14" font="1">since  it  contains  the  exponential  series.  In  the  object </text>
<text top="682" left="76" width="371" height="14" font="1">oriented programming based models it is defined with the </text>
<text top="705" left="76" width="371" height="14" font="1">help  of  a  look  up  table  consuming  more  memory </text>
<text top="728" left="76" width="371" height="14" font="1">resources. In the HDL-CNN design piecewise second order </text>
<text top="751" left="76" width="371" height="14" font="1">approximation of the function using quadratic functions is </text>
<text top="774" left="76" width="371" height="14" font="1">implemented shown in Equation 3 where c , c  and c are </text>
<text top="782" left="350" width="4" height="7" font="6">0</text>
<text top="782" left="370" width="4" height="7" font="6">1</text>
<text top="782" left="415" width="6" height="7" font="6">2 </text>
<text top="797" left="76" width="371" height="14" font="1">coefficients of the quadratic function (Tommiska, 2003). </text>
<text top="820" left="76" width="371" height="14" font="1">This  requires  two  adders  and  three  multiplier  operators </text>
<text top="843" left="76" width="371" height="14" font="1">redesigned as two MAC operations and a shift register for </text>
<text top="866" left="76" width="148" height="14" font="1">calculating the square.</text>
<text top="893" left="76" width="371" height="14" font="1">Each  level  vector  based  back  propagation  module  is </text>
<text top="915" left="76" width="371" height="14" font="1">evaluated as described below. Assuming <i>H</i> is the vector of </text>
<text top="938" left="76" width="371" height="14" font="1">hidden-layer neurons, <i>I </i>is the vector of input-layer neurons </text>
<text top="961" left="76" width="371" height="14" font="1">and <i>W1</i> is the weight matrix between the input and hidden </text>
<text top="984" left="76" width="371" height="14" font="1">layer, W2 is the matrix of synapses connection hidden and </text>
<text top="1007" left="76" width="371" height="14" font="1">output  layers,  <i>th1</i>  and  <i>th2</i>  are  the  effect  biases  on  the </text>
<text top="1030" left="76" width="371" height="14" font="1">computed activations (set to value 1 for this design), <i>T</i> is </text>
<text top="1053" left="76" width="371" height="14" font="1">the  target  activation  of  the  output  layer,  μ  is  the </text>
<text top="1076" left="76" width="371" height="14" font="1">momentum  factor  used  to  allow  the  previous  weight </text>
<text top="702" left="544" width="234" height="12" font="7">Table 2. Equations governing each level of</text>
<text top="716" left="551" width="219" height="12" font="7">CNN (Back propagation neural network)</text>
<text top="599" left="478" width="148" height="11" font="8">Hidden layer neuron activations</text>
<text top="614" left="478" width="147" height="11" font="8">Output-layer neuron activations</text>
<text top="629" left="478" width="81" height="11" font="8">Output-layer error</text>
<text top="644" left="478" width="83" height="11" font="8">Hidden-layer error</text>
<text top="659" left="478" width="162" height="11" font="8">Weights for second layer of synapses</text>
<text top="674" left="478" width="181" height="11" font="8">Weight adjustment of first layer of synapses</text>
<text top="600" left="713" width="57" height="10" font="12"><i>H</i> = j</text>
<text top="600" left="738" width="48" height="10" font="8">(<i>I.W</i>1<i>+th</i>1)</text>
<text top="614" left="712" width="75" height="10" font="12"><i>0</i> =  (<i>H.W</i>2<i>+th</i>2)</text>
<text top="614" left="728" width="38" height="10" font="8">j</text>
<text top="629" left="716" width="67" height="11" font="8">D = 0(1-0)(0-T)</text>
<text top="644" left="713" width="72" height="11" font="8">E = H(1-H)W2.D</text>
<text top="660" left="655" width="14" height="10" font="12"><i>W</i>2</text>
<text top="659" left="669" width="71" height="11" font="8">= <i>W</i>2<i>+</i>D</text>
<text top="660" left="709" width="17" height="10" font="12"><i>W</i>2 </text>
<text top="660" left="757" width="39" height="10" font="8">D</text>
<text top="660" left="763" width="14" height="10" font="12"><i>W</i>2</text>
<text top="659" left="777" width="48" height="11" font="8"> =m</text>
<text top="660" left="795" width="61" height="10" font="12"><i>HD+</i>D</text>
<text top="660" left="823" width="14" height="10" font="12"><i>W</i>2</text>
<text top="665" left="837" width="6" height="5" font="13">t-1</text>
<text top="659" left="725" width="31" height="11" font="8">where </text>
<text top="675" left="668" width="14" height="10" font="12"><i>W</i>1</text>
<text top="674" left="682" width="51" height="11" font="8">= <i>W</i>1<i>+W</i>1  </text>
<text top="675" left="764" width="14" height="10" font="12"><i>W1</i></text>
<text top="674" left="779" width="51" height="11" font="8"> =a</text>
<text top="675" left="798" width="53" height="10" font="12"><i>IE+</i>m</text>
<text top="675" left="821" width="39" height="10" font="8">D</text>
<text top="675" left="827" width="14" height="10" font="12"><i>W</i>1</text>
<text top="681" left="728" width="1" height="5" font="13">t</text>
<text top="681" left="778" width="1" height="5" font="13">t</text>
<text top="681" left="841" width="6" height="5" font="13">t-1</text>
<text top="674" left="732" width="31" height="11" font="8">where </text>
<text top="1096" left="511" width="304" height="12" font="7">Figure 2. HDL- CNN recognition model – Block diagram </text>
<text top="1140" left="826" width="7" height="12" font="4"><i>9</i></text>
<text top="1142" left="381" width="34" height="7" font="6">l</text>
<text top="1140" left="80" width="288" height="12" font="4"><i>i-manager’s Journal o  Electronics Engineering, Vol.</i></text>
<text top="1140" left="388" width="3" height="11" font="4"><i> </i></text>
<text top="1140" left="391" width="33" height="12" font="4"><i>No. 1 </i></text>
<text top="1142" left="423" width="34" height="7" font="6">l</text>
<text top="1142" left="429" width="2" height="8" font="6"> </text>
<text top="1140" left="202" width="7" height="12" font="4"><i>n</i></text>
<text top="1140" left="368" width="14" height="12" font="4"><i> 2 </i></text>
<text top="1140" left="431" width="161" height="12" font="4"><i>September - November 2011</i></text>
</page>
<page number="5" position="absolute" top="0" left="0" height="1209" width="919">
<text top="67" left="79" width="218" height="24" font="0">RESEARCH PAPERS</text>
<text top="370" left="477" width="225" height="14" font="1">3. Sign/Gesture Recognition Model</text>
<text top="397" left="477" width="371" height="14" font="1">A  recognition  model  is  shown  below  in  Figure  3.  Sign </text>
<text top="420" left="477" width="371" height="14" font="1">recognition using neural networks is based on the learning </text>
<text top="443" left="477" width="371" height="14" font="1">of  the  network  using  a  database  set  of  signs/  gestures </text>
<text top="466" left="477" width="371" height="14" font="1">(Vargas, Barba, Torres &amp; Mottos, 2011). The architecture is </text>
<text top="488" left="477" width="372" height="14" font="1">designed  based  on  camera-based  recognition </text>
<text top="511" left="477" width="371" height="14" font="1">methodology.  Once  the  video/  image  are  being </text>
<text top="534" left="477" width="371" height="14" font="1">obtained from the acquisition unit, the image (256x256 </text>
<text top="557" left="477" width="371" height="14" font="1">pixels) is processed in various stages and data is extracted </text>
<text top="580" left="477" width="371" height="14" font="1">to implement the recognition model. The first step of the </text>
<text top="603" left="477" width="371" height="14" font="1">model after the image data acquired is pre-processing. </text>
<text top="626" left="477" width="371" height="14" font="1">In  general  raw  image  data  processing  consumes  high </text>
<text top="649" left="477" width="371" height="14" font="1">memory and other resources due to redundancy in the </text>
<text top="672" left="477" width="371" height="14" font="1">spatial  and  temporal  basis.  Pre-processing  involves </text>
<text top="695" left="477" width="371" height="14" font="1">filtering and background subtraction in order to consider </text>
<text top="718" left="477" width="371" height="14" font="1">various  environment  factors  such  as  illumination, </text>
<text top="741" left="477" width="371" height="14" font="1">unwanted noise and other scene conditions done using </text>
<text top="764" left="477" width="371" height="14" font="1">MATLAB. These pre-processed frames are taken as input </text>
<text top="787" left="477" width="371" height="14" font="1">(bitmapped images) into the FPGA for LoG (Laplacian of </text>
<text top="144" left="76" width="371" height="14" font="1">random  weights  a  linear  shift  register  module  is  used </text>
<text top="167" left="76" width="371" height="14" font="1">(weights  between  -1  and  1  are  generated).  The </text>
<text top="189" left="76" width="371" height="14" font="1">asynchronous  RESET  when  set  to  high,  the  internal  finite </text>
<text top="212" left="76" width="371" height="14" font="1">state machine of CNN is reset to the initial state. During the </text>
<text top="235" left="76" width="371" height="14" font="1">initialization phase, the CNN randomizes all connection </text>
<text top="258" left="76" width="371" height="14" font="1">weights  using  the  shift  register  module  and  when </text>
<text top="281" left="76" width="371" height="14" font="1">completed it enters the idle state. The training and testing </text>
<text top="304" left="76" width="371" height="14" font="1">is  done  in  two  different  modes  called  TRAIN  and  TEST. </text>
<text top="327" left="76" width="371" height="14" font="1">When the mode is set to TRAIN - the CNN enters the training </text>
<text top="350" left="76" width="371" height="14" font="1">state from idle and during TEST- CNN enters the run state </text>
<text top="373" left="76" width="243" height="14" font="1">with the corresponding flags being set.</text>
<text top="400" left="76" width="187" height="14" font="5"><i>2.2.1 Computational Analysis</i></text>
<text top="426" left="76" width="371" height="14" font="1">The  number  of  computations  involved  and  the  gain </text>
<text top="449" left="76" width="371" height="14" font="1">acquired  by  shifting  the  architecture  to  HDL  platform  is </text>
<text top="472" left="76" width="371" height="14" font="1">being  modeled  below.  With  the  feature  vector  <i>V  </i>of </text>
<text top="495" left="76" width="371" height="14" font="1">variable  length  is  a  function  of  number  of  patterns  '<i>p</i>' </text>
<text top="518" left="76" width="371" height="14" font="1">considered  for  recognition  and  the  number  of  features </text>
<text top="541" left="76" width="82" height="14" font="1">extracted '<i>n</i>'.</text>
<text top="568" left="400" width="16" height="14" font="1">(4)</text>
<text top="594" left="76" width="371" height="14" font="1">General consideration of the CNN level is <i>n</i> input neurons, </text>
<text top="617" left="76" width="367" height="14" font="5"><i>h</i> hidden neurons and <i>l</i> output neurons where <i>n &lt; h &lt; 2n-</i></text>
<text top="640" left="76" width="371" height="14" font="5"><i>1</i>; <i>MAC</i> represents multiply and accumulate, <i>A</i> is adder, <i>M</i> </text>
<text top="663" left="76" width="371" height="14" font="1">is  multiplier  and  <i>S</i>  is  a  shifter  operation.  For  a  sigmoid </text>
<text top="686" left="76" width="371" height="14" font="1">operation the software solution using a look up table (<i>LUT</i>) </text>
<text top="709" left="76" width="371" height="14" font="1">where the time taken for performing one <i>LUT</i> depends on </text>
<text top="732" left="76" width="371" height="14" font="1">the speed of the processor. In the HDL-CNN the quadratic </text>
<text top="755" left="76" width="371" height="14" font="1">equation depicted uses one MAC and one Shifter <i>(MAC </i></text>
<text top="778" left="76" width="371" height="14" font="5"><i>+S)  </i>for  the  calculation  of  single  neuron  activation </text>
<text top="801" left="76" width="57" height="14" font="1">function.</text>
<text top="827" left="76" width="371" height="14" font="1">In  the  HDL-CNN  the  matrix  operations  involved  in  the </text>
<text top="850" left="76" width="371" height="14" font="1">weight  layer  updates  and  error  calculations  are </text>
<text top="873" left="76" width="371" height="14" font="1">performed by the dedicated adders, multipliers, shifters </text>
<text top="896" left="76" width="371" height="14" font="1">and MAC units and hence concurrently (in one complete </text>
<text top="919" left="76" width="371" height="14" font="1">clock cycle) done at a time for all neurons rather than the </text>
<text top="942" left="76" width="371" height="14" font="1">for loop control used in software models. Each level of the </text>
<text top="965" left="76" width="371" height="14" font="1">CNN has the different number of computations involved </text>
<text top="988" left="76" width="371" height="14" font="1">listed in Table 3 where the values of <i>n</i>, <i>h</i>, <i>and l</i> (the input, </text>
<text top="1011" left="76" width="371" height="14" font="1">hidden and output layer neurons) vary from level1, level2 </text>
<text top="1034" left="76" width="371" height="14" font="1">and level3. The average gain in training time is plotted </text>
<text top="1057" left="76" width="371" height="14" font="1">and discussed in the results section supports the above </text>
<text top="1080" left="76" width="54" height="14" font="1">analysis.</text>
<text top="330" left="504" width="312" height="12" font="7">Table 3. Comparison of Number of computation involved</text>
<text top="344" left="513" width="295" height="12" font="7">in each level between the MATLAB CNN and HDL-CNN</text>
<text top="149" left="639" width="22" height="11" font="8">CNN</text>
<text top="149" left="764" width="44" height="11" font="8">HDL-CNN</text>
<text top="169" left="482" width="59" height="11" font="8">Hidden layer</text>
<text top="181" left="482" width="86" height="11" font="8">neuron activations</text>
<text top="198" left="482" width="32" height="11" font="8">Output</text>
<text top="210" left="482" width="50" height="11" font="8">activations</text>
<text top="198" left="515" width="62" height="11" font="8">-layer neuron</text>
<text top="227" left="482" width="81" height="11" font="8">Output-layer error</text>
<text top="245" left="482" width="83" height="11" font="8">Hidden-layer error</text>
<text top="263" left="483" width="83" height="11" font="8">Weight adjustment</text>
<text top="276" left="482" width="72" height="11" font="8">of second layer </text>
<text top="293" left="483" width="87" height="11" font="8">Weight adjustment</text>
<text top="305" left="483" width="55" height="11" font="8">of first layer </text>
<text top="174" left="583" width="134" height="10" font="8">{<i>nhM</i> + (1+h(n-1))<i>A</i>} + h(<i>LUT</i>)</text>
<text top="203" left="587" width="127" height="10" font="8">{<i>hlM</i> + (1+<i>l</i>(h-1))<i>A</i>} + <i>l</i>(<i>LUT</i>)</text>
<text top="228" left="628" width="43" height="10" font="8">2<i>lA</i> + 2<i>lM</i></text>
<text top="246" left="603" width="52" height="10" font="8">(<i>h</i>( -1))<i>A</i> + (</text>
<text top="246" left="684" width="12" height="10" font="8">)<i>M</i></text>
<text top="246" left="615" width="3" height="10" font="12"><i>l</i></text>
<text top="246" left="655" width="30" height="10" font="12"><i>lh</i> + 2<i>h</i></text>
<text top="270" left="616" width="41" height="10" font="8">(2<i>h </i>)<i>A</i> + (</text>
<text top="270" left="671" width="12" height="10" font="8">)<i>M</i></text>
<text top="270" left="630" width="3" height="10" font="12"><i>l</i></text>
<text top="270" left="658" width="13" height="10" font="8">2<i>lh</i></text>
<text top="299" left="616" width="39" height="10" font="8">(<i>nh</i>)<i>A</i> + (</text>
<text top="299" left="671" width="12" height="10" font="8">)<i>M</i></text>
<text top="299" left="655" width="16" height="10" font="8">3<i>nh</i></text>
<text top="174" left="736" width="5" height="10" font="8">{</text>
<text top="174" left="764" width="25" height="10" font="8">}  + (</text>
<text top="174" left="827" width="3" height="10" font="8">)</text>
<text top="174" left="741" width="32" height="10" font="12"><i>MAC h</i></text>
<text top="174" left="789" width="47" height="10" font="12"><i>MAC</i> + <i>S h</i></text>
<text top="203" left="738" width="95" height="10" font="8">{<i>MAC</i>}<i>l</i> + (<i>MAC</i> + <i>S</i>)<i>l</i></text>
<text top="227" left="761" width="3" height="10" font="8">(</text>
<text top="227" left="779" width="31" height="10" font="8">1)<i>MAC</i></text>
<text top="227" left="765" width="14" height="10" font="12"><i>l </i>+ </text>
<text top="246" left="759" width="9" height="10" font="12"><i>h</i>(</text>
<text top="246" left="810" width="3" height="10" font="8">)</text>
<text top="246" left="768" width="42" height="10" font="12"><i>MAC</i> + <i>M</i></text>
<text top="271" left="761" width="27" height="10" font="12"><i>hMAC</i></text>
<text top="271" left="800" width="12" height="10" font="8">2<i>A</i></text>
<text top="271" left="789" width="11" height="10" font="8"> + </text>
<text top="299" left="748" width="47" height="10" font="12"><i>nMAC</i> + 2(</text>
<text top="299" left="821" width="3" height="10" font="8">)</text>
<text top="299" left="795" width="26" height="10" font="12"><i>A</i> + <i>M</i></text>
<text top="1095" left="507" width="307" height="12" font="7">Figure 3. System Overview of the sign recognition model</text>
<text top="1140" left="86" width="14" height="12" font="4"><i>10</i></text>
<text top="1140" left="323" width="288" height="12" font="4"><i>i-manager’s Journal o  Electronics Engineering, Vol.</i></text>
<text top="1140" left="445" width="7" height="12" font="4"><i>n</i></text>
<text top="1140" left="611" width="3" height="12" font="4"><i> </i></text>
<text top="1142" left="624" width="34" height="7" font="6">l</text>
<text top="1142" left="666" width="34" height="7" font="6">l</text>
<text top="1142" left="672" width="2" height="8" font="6"> </text>
<text top="1140" left="615" width="221" height="12" font="4"><i>2   No. 1  September - November 2011</i></text>
</page>
<page number="6" position="absolute" top="0" left="0" height="1209" width="919">
<text top="69" left="79" width="218" height="24" font="0">RESEARCH PAPERS</text>
<text top="779" left="477" width="371" height="14" font="1">components in use in this case study realization of CNN </text>
<text top="802" left="477" width="371" height="14" font="1">using HDL for sign language recognition. Figure 5 shows </text>
<text top="825" left="477" width="371" height="14" font="1">the connections between the FPGA board – Xilinx Spartan </text>
<text top="144" left="76" width="371" height="14" font="1">Gaussian) edge detection. The feature extraction block </text>
<text top="167" left="76" width="371" height="14" font="1">and  CNN  block  perform  in  parallel  with  dual  bus </text>
<text top="189" left="76" width="372" height="14" font="1">communication  interface  provided.  The  feature </text>
<text top="212" left="76" width="371" height="14" font="1">extraction  layer  extracts  the  necessary  features  of  size, </text>
<text top="235" left="76" width="371" height="14" font="1">shape and state attributes of the hand (described in detail </text>
<text top="258" left="76" width="371" height="14" font="1">(Mekala,  Gao,  Fan  &amp;  Davari,  2011)).  Since  it  is  time </text>
<text top="281" left="76" width="371" height="14" font="1">consuming for the processor to wait until all the 55 features </text>
<text top="304" left="76" width="371" height="14" font="1">have  been  extracted,  the  CNN  layer  is  initiated  at  the </text>
<text top="327" left="76" width="371" height="14" font="1">arrival of first 15 feature elements to Level1 and then the </text>
<text top="350" left="76" width="371" height="14" font="1">40 for the next level 2 or level 3 adopted based on the </text>
<text top="373" left="76" width="371" height="14" font="1">decision of level 1 network (Mekala, Gao, Fan &amp; Davari, </text>
<text top="396" left="76" width="371" height="14" font="1">2011). The training of CNN is done using the sign language </text>
<text top="419" left="76" width="371" height="14" font="1">patterns from A to Z (without J and Z characters involving </text>
<text top="442" left="76" width="371" height="14" font="1">the motion). In order to test the ability and performance of </text>
<text top="465" left="76" width="371" height="14" font="1">the network, usually a test set of independent examples is </text>
<text top="488" left="76" width="371" height="14" font="1">used  in  order  to  generalize  the  network  with  regards  to </text>
<text top="511" left="76" width="334" height="14" font="1">example sets which are not present in the training set.</text>
<text top="537" left="76" width="371" height="14" font="1">The  case  study  of  American  Sign  Language  (ASL) </text>
<text top="560" left="76" width="344" height="14" font="1">recognition is being interpreted step by step as follows:</text>
<text top="588" left="76" width="39" height="13" font="1">·</text>
<text top="587" left="102" width="345" height="14" font="1">Image  acquisition  via  camera  and  generating  still </text>
<text top="610" left="102" width="345" height="14" font="1">image frame data (Video to Frames conversion with </text>
<text top="633" left="102" width="345" height="14" font="1">background  subtraction)  –  done  using  MATLAB  and </text>
<text top="656" left="102" width="131" height="14" font="1">stored as “.coe” files.</text>
<text top="683" left="76" width="39" height="13" font="1">·</text>
<text top="682" left="102" width="345" height="14" font="1">Transfer of the image data to a Xilinx Spartan 3E FPGA </text>
<text top="705" left="102" width="345" height="14" font="1">(Field Programmable Gate Array) board via USB 2.0 </text>
<text top="728" left="102" width="70" height="14" font="1">using a PC.</text>
<text top="756" left="76" width="39" height="13" font="1">·</text>
<text top="755" left="102" width="345" height="14" font="1">Saving the data to the onboard SRAM (Static Random </text>
<text top="778" left="102" width="345" height="14" font="1">Access Memory) to allow image processing functions </text>
<text top="801" left="102" width="197" height="14" font="1">to be performed on the image.</text>
<text top="828" left="76" width="39" height="13" font="1">·</text>
<text top="827" left="102" width="345" height="14" font="1">Implementing  the  edge  detection  and  feature </text>
<text top="850" left="102" width="345" height="14" font="1">extraction  algorithms  on  the  image  and  storing  the </text>
<text top="873" left="102" width="204" height="14" font="1">feature vector back in the SRAM.</text>
<text top="901" left="76" width="39" height="13" font="1">·</text>
<text top="900" left="102" width="345" height="14" font="1">Recognition via CNN model with parallel interaction </text>
<text top="923" left="102" width="181" height="14" font="1">to the feature extraction unit.</text>
<text top="951" left="76" width="39" height="13" font="1">·</text>
<text top="950" left="102" width="345" height="14" font="1">Display of the input frame and processed frame on to </text>
<text top="973" left="102" width="345" height="14" font="1">the PC to be viewed by the user via the VGA controller, </text>
<text top="996" left="102" width="303" height="14" font="1">recognized sign displayed on the LCD segment.</text>
<text top="1022" left="76" width="371" height="14" font="1">The model schematic of the sign recognition is shown in </text>
<text top="1045" left="76" width="371" height="14" font="1">Figure  4. It  contains  the  SRAM  module,  preprocessor </text>
<text top="1068" left="76" width="371" height="14" font="1">module,  feature  extraction  module  and  the  CNN </text>
<text top="1091" left="76" width="371" height="14" font="1">recognition  module.  There  are  three  main  hardware </text>
<text top="1045" left="135" width="4" height="14" font="11"> </text>
<text top="739" left="504" width="313" height="12" font="7">Figure 4. System Overview of the sign recognition model–</text>
<text top="753" left="521" width="280" height="12" font="7">Preprocessing, Feature Extraction and CNN Engine.</text>
<text top="1094" left="519" width="283" height="12" font="7">Figure 5. Xilinx Spartan 3E kit Hardware connections</text>
<text top="1140" left="826" width="14" height="12" font="4"><i>11</i></text>
<text top="1142" left="381" width="34" height="7" font="6">l</text>
<text top="1140" left="80" width="288" height="12" font="4"><i>i-manager’s Journal o  Electronics Engineering, Vol.</i></text>
<text top="1140" left="388" width="3" height="11" font="4"><i> </i></text>
<text top="1140" left="391" width="33" height="12" font="4"><i>No. 1 </i></text>
<text top="1142" left="423" width="34" height="7" font="6">l</text>
<text top="1142" left="429" width="2" height="8" font="6"> </text>
<text top="1140" left="202" width="7" height="12" font="4"><i>n</i></text>
<text top="1140" left="368" width="14" height="12" font="4"><i> 2 </i></text>
<text top="1140" left="431" width="161" height="12" font="4"><i>September - November 2011</i></text>
</page>
<page number="7" position="absolute" top="0" left="0" height="1209" width="919">
	<fontspec id="17" size="6" family="Times" color="#231f20"/>
	<fontspec id="18" size="6" family="Times" color="#231f20"/>
	<fontspec id="19" size="4" family="Times" color="#231f20"/>
<text top="69" left="79" width="218" height="24" font="0">RESEARCH PAPERS</text>
<text top="144" left="477" width="371" height="14" font="1">parallel  procedures  hence  the  gain  in  speed.  On  an </text>
<text top="167" left="477" width="371" height="14" font="1">average  (mean  over  different  number  of  patterns)  the </text>
<text top="190" left="477" width="371" height="14" font="1">time  consumed  for  the  same  design  on  MATLAB  was </text>
<text top="212" left="477" width="371" height="14" font="1">around  381.36  seconds  while  the  time  taken  for  the </text>
<text top="235" left="477" width="371" height="14" font="1">design on VHDL is CPU time 29.34 seconds and real time </text>
<text top="258" left="477" width="371" height="14" font="1">30 seconds as listed in Table 4. The HDL solution proposed </text>
<text top="281" left="477" width="371" height="14" font="1">proved to be 13x times better in consideration with time </text>
<text top="304" left="477" width="230" height="14" font="1">and speed of the training of network.</text>
<text top="331" left="477" width="371" height="14" font="1">The images after background subtraction are converted </text>
<text top="354" left="477" width="371" height="14" font="1">from grayscale to bitmap of depth 4 (sent as input in the </text>
<text top="377" left="477" width="371" height="14" font="1">form of '.coe' files to the ISE). Grayscale involves 8 bits to </text>
<text top="400" left="477" width="371" height="14" font="1">represent each pixel where as the bitmap image of depth </text>
<text top="423" left="477" width="371" height="14" font="1">4.  Thus  processing  of  256x256  image  saves  262144 </text>
<text top="446" left="477" width="371" height="14" font="1">(256x256x4)  bits  in  representation  that  is  256Kb  in </text>
<text top="469" left="477" width="170" height="14" font="1">bandwidth listed in Table 4.</text>
<text top="495" left="477" width="371" height="14" font="1">To  validate  the  performance  of  the  HDL-CNN,  they </text>
<text top="518" left="477" width="371" height="14" font="1">generate the mean square error test-bench considering </text>
<text top="541" left="477" width="371" height="14" font="1">the actual situation of the neural network operation. The </text>
<text top="564" left="477" width="371" height="14" font="1">test-bench  adopting  the  three-level  feature  vector  as </text>
<text top="587" left="477" width="371" height="14" font="1">input signal vectors, and the weight coefficient of hidden </text>
<text top="610" left="477" width="371" height="14" font="1">and output layers are stored in RAMs.  Both mean square </text>
<text top="633" left="477" width="371" height="14" font="1">error and evolution of weights are transferred to text files </text>
<text top="656" left="477" width="371" height="14" font="1">and plotted using MATLAB. Epoch based updating of the </text>
<text top="679" left="477" width="371" height="14" font="1">weights is performed and Mean Square Error is decreasing </text>
<text top="702" left="477" width="371" height="14" font="1">at  an  exponential  rate  and  settling  down  to  an  almost </text>
<text top="725" left="477" width="208" height="14" font="1">constant value shown in Figure 7.</text>
<text top="751" left="477" width="371" height="14" font="1">An epoch is the presentation of the entire training set to </text>
<text top="774" left="477" width="371" height="14" font="1">the neural network once and for the network to reach the </text>
<text top="797" left="477" width="371" height="14" font="1">minimum  threshold  error  the  training  is  done  multiple </text>
<text top="820" left="477" width="371" height="14" font="1">times  counted  as  number  of  epochs.  Maximal  weight </text>
<text top="843" left="477" width="371" height="14" font="1">change in each epoch is decreasing and finally reaching </text>
<text top="866" left="477" width="371" height="14" font="1">to a least value possible. The best, intermediate and worst </text>
<text top="889" left="477" width="371" height="14" font="1">case scenarios are shown in Figure 7 where the evolution </text>
<text top="912" left="477" width="371" height="14" font="1">of weights is settling down to a constant value at the end of </text>
<text top="935" left="477" width="371" height="14" font="1">1815  epochs  for  the  best  case  obtained  by  the  global </text>
<text top="144" left="76" width="371" height="14" font="1">3E, the USB to Peripheral communications module and a </text>
<text top="167" left="76" width="371" height="14" font="1">monitor  with  VGA  connection  in  order  to  display  the </text>
<text top="189" left="76" width="371" height="14" font="1">recognized  output  sign.  The  authors  adopt  Xilinx </text>
<text top="212" left="76" width="371" height="14" font="1">Integrated  Software  Environment  (ISE  10.1)  which  is  a </text>
<text top="235" left="76" width="371" height="14" font="1">powerful,  flexible  integrated  design  environment  that </text>
<text top="258" left="76" width="371" height="14" font="1">allows designing Xilinx FPGA devices from basic modules </text>
<text top="281" left="76" width="371" height="14" font="1">to  complete  microprocessor  architectures.  Project </text>
<text top="304" left="76" width="371" height="14" font="1">Navigator is the user interface that manages the entire </text>
<text top="327" left="76" width="371" height="14" font="1">design  process  including  design  entry,  simulation, </text>
<text top="350" left="76" width="371" height="14" font="1">synthesis,  implementation,  and  finally  download  the </text>
<text top="373" left="76" width="371" height="14" font="1">configuration of the FPGA device. PACE is responsible for </text>
<text top="396" left="76" width="371" height="14" font="1">placing  and  routing  the  code  for  optimization.  IMPACT </text>
<text top="419" left="76" width="371" height="14" font="1">then generates the programming files and downloads the </text>
<text top="442" left="76" width="199" height="14" font="1">code to hardware (Xilinx, 2009).</text>
<text top="468" left="76" width="59" height="14" font="1">4. Results</text>
<text top="495" left="76" width="371" height="14" font="1">Most components of the architecture perform in parallel </text>
<text top="518" left="76" width="371" height="14" font="1">and  hence  the  potentially  infinite  training  times  are </text>
<text top="541" left="76" width="371" height="14" font="1">reduced reasonably. The training time is the time taken to </text>
<text top="564" left="76" width="371" height="14" font="1">train the network for a given number of patterns '<i>p</i>' without </text>
<text top="587" left="76" width="371" height="14" font="1">duplicates input frames. The training time is being plotted </text>
<text top="610" left="76" width="371" height="14" font="1">as varied with respect to number of patterns being trained </text>
<text top="633" left="76" width="371" height="14" font="1">in Figure 6. It clearly shows that the HDL-CNN model saves </text>
<text top="656" left="76" width="371" height="14" font="1">at an average 13x times the time involved in training when </text>
<text top="679" left="76" width="371" height="14" font="1">compared to the software based CNN model. Also the </text>
<text top="702" left="76" width="371" height="14" font="1">curve states that the time saved increases exponentially </text>
<text top="725" left="76" width="371" height="14" font="1">as the number of patterns increases i.e. as complexity of </text>
<text top="747" left="76" width="371" height="14" font="1">recognition  is  more  non-linear  and  thus  an  average  is </text>
<text top="770" left="76" width="371" height="14" font="1">considered  for  comparison.  The  adjustments  of  the </text>
<text top="793" left="76" width="371" height="14" font="1">weight  matrices  and  neuron  activation  vector  are  all </text>
<text top="1082" left="82" width="357" height="12" font="7">Figure 6. HDL-CNN vs. CNN architecture Training time (in seconds)</text>
<text top="1097" left="117" width="286" height="12" font="7">variation based on number of patterns to be trained</text>
<text top="1047" left="107" width="5" height="8" font="17">3</text>
<text top="1047" left="147" width="5" height="8" font="17">4</text>
<text top="1047" left="187" width="5" height="8" font="17">5</text>
<text top="1047" left="228" width="5" height="8" font="17">6</text>
<text top="1047" left="269" width="5" height="8" font="17">7</text>
<text top="1047" left="309" width="5" height="8" font="17">8</text>
<text top="1047" left="350" width="5" height="8" font="17">9</text>
<text top="1047" left="389" width="9" height="8" font="17">10</text>
<text top="1047" left="430" width="9" height="8" font="17">11</text>
<text top="1040" left="102" width="5" height="8" font="17">0</text>
<text top="1004" left="94" width="14" height="8" font="17">100</text>
<text top="969" left="94" width="14" height="8" font="17">200</text>
<text top="933" left="94" width="14" height="8" font="17">300</text>
<text top="897" left="94" width="14" height="8" font="17">400</text>
<text top="861" left="94" width="14" height="8" font="17">500</text>
<text top="826" left="94" width="14" height="8" font="17">600</text>
<text top="817" left="214" width="119" height="8" font="17">Training time vs. No. of Patterns</text>
<text top="954" left="87" width="0" height="8" font="18">T</text>
<text top="949" left="87" width="0" height="8" font="18">ra</text>
<text top="942" left="87" width="0" height="8" font="18">in</text>
<text top="936" left="87" width="0" height="8" font="18">in</text>
<text top="930" left="87" width="0" height="8" font="18">g</text>
<text top="923" left="87" width="0" height="8" font="18">T</text>
<text top="919" left="87" width="0" height="8" font="18">im</text>
<text top="911" left="87" width="0" height="8" font="18">e</text>
<text top="1117" left="-7" width="4" height="14" font="1"> </text>
<text top="739" left="565" width="4" height="14" font="1"> </text>
<text top="1058" left="212" width="121" height="8" font="17">Number of Patterns to be trained</text>
<text top="842" left="344" width="39" height="8" font="17">CNN Train</text>
<text top="847" left="379" width="2" height="6" font="19">t</text>
<text top="842" left="381" width="13" height="8" font="17">ime</text>
<text top="858" left="344" width="76" height="8" font="17">HDL CNN Train time</text>
<text top="1014" left="191" width="31" height="8" font="17">9x times</text>
<text top="964" left="353" width="36" height="8" font="17">13x times</text>
<text top="894" left="405" width="13" height="8" font="17">15x</text>
<text top="904" left="405" width="20" height="8" font="17">times</text>
<text top="1080" left="505" width="313" height="12" font="7">Table 4. HDL-CNN recognition model vs. CNN recognition</text>
<text top="1094" left="543" width="240" height="12" font="7">model (Software vs. Hardware architecture) </text>
<text top="958" left="605" width="236" height="11" font="8">HDL-CNN (Hardware solution) CNN (MATLAB solution)</text>
<text top="977" left="480" width="102" height="11" font="8">Average Training Time</text>
<text top="978" left="646" width="48" height="11" font="8">29.34secs</text>
<text top="978" left="764" width="54" height="11" font="8">381.36secs</text>
<text top="990" left="480" width="137" height="11" font="8">Single Pattern Recognition Time</text>
<text top="991" left="650" width="41" height="11" font="8">43.45ms</text>
<text top="991" left="770" width="42" height="11" font="8">0.52secs</text>
<text top="1003" left="480" width="103" height="11" font="8">Average Performance</text>
<text top="1004" left="660" width="21" height="11" font="8">95%</text>
<text top="1004" left="776" width="30" height="11" font="8">92.8%</text>
<text top="1016" left="480" width="114" height="11" font="8">Average Noise Immunity</text>
<text top="1017" left="660" width="21" height="11" font="8">51%</text>
<text top="1017" left="781" width="21" height="11" font="8">48%</text>
<text top="1029" left="480" width="131" height="11" font="8">Epochs (Best case scenario)</text>
<text top="1030" left="658" width="24" height="11" font="8">1815</text>
<text top="1030" left="779" width="24" height="11" font="8">1832</text>
<text top="1042" left="480" width="48" height="11" font="8">Limitations</text>
<text top="1041" left="620" width="100" height="11" font="8">J,Z (signs with motion)</text>
<text top="1040" left="789" width="3" height="11" font="8">-</text>
<text top="1055" left="480" width="87" height="11" font="8">Gain in bandwidth</text>
<text top="1054" left="630" width="80" height="11" font="8">256Kb per frame</text>
<text top="1053" left="789" width="3" height="11" font="8">-</text>
<text top="1140" left="86" width="14" height="12" font="4"><i>12</i></text>
<text top="1140" left="323" width="288" height="12" font="4"><i>i-manager’s Journal o  Electronics Engineering, Vol.</i></text>
<text top="1140" left="445" width="7" height="12" font="4"><i>n</i></text>
<text top="1140" left="611" width="3" height="12" font="4"><i> </i></text>
<text top="1142" left="624" width="34" height="7" font="6">l</text>
<text top="1142" left="666" width="34" height="7" font="6">l</text>
<text top="1142" left="672" width="2" height="8" font="6"> </text>
<text top="1140" left="615" width="221" height="12" font="4"><i>2   No. 1  September - November 2011</i></text>
</page>
<page number="8" position="absolute" top="0" left="0" height="1209" width="919">
<text top="186" left="163" width="105" height="11" font="8">Best case scenario–11</text>
<text top="202" left="163" width="105" height="11" font="8">Hidden layer neurons, </text>
<text top="217" left="163" width="83" height="11" font="8">learning rate 0.1, </text>
<text top="232" left="163" width="99" height="11" font="8">momentum 0.4 and </text>
<text top="247" left="163" width="81" height="10" font="8">threshold 0.0001.</text>
<text top="350" left="163" width="90" height="11" font="8">Intermediate case </text>
<text top="368" left="163" width="103" height="11" font="8">scenario-Hidden layer</text>
<text top="387" left="163" width="88" height="11" font="8">units=11; learning </text>
<text top="405" left="163" width="134" height="11" font="8">rate=0.1, momentum=0.7; </text>
<text top="422" left="163" width="84" height="11" font="8">threshold=0.0001</text>
<text top="528" left="163" width="97" height="11" font="8">Worst case scenario-</text>
<text top="546" left="163" width="109" height="11" font="8">Hidden layer units=11; </text>
<text top="565" left="163" width="108" height="11" font="8">learning rate=0.01,No </text>
<text top="583" left="163" width="63" height="11" font="8">momentum; </text>
<text top="600" left="163" width="78" height="11" font="8">threshold=0.001</text>
<text top="69" left="79" width="218" height="24" font="0">RESEARCH PAPERS</text>
<text top="717" left="477" width="108" height="14" font="1">strange patterns.</text>
<text top="743" left="477" width="371" height="14" font="1">Figure 8 generalizes the results of few test patterns with the </text>
<text top="766" left="477" width="371" height="14" font="1">LoG edge operator and the sign recognized. Few noisy </text>
<text top="789" left="477" width="371" height="14" font="1">patterns are also tested in order to evaluate the accuracy </text>
<text top="812" left="477" width="371" height="14" font="1">of  the  architecture.  Though  the  network  is  trained  using </text>
<text top="835" left="477" width="371" height="14" font="1">different test patterns, it appears that the noise immunity </text>
<text top="858" left="477" width="371" height="14" font="1">levels are varying for each sign involved. Noise immunity is </text>
<text top="881" left="477" width="371" height="14" font="1">the  level  of  noise  under  which  the  pattern  can  still  be </text>
<text top="904" left="477" width="371" height="14" font="1">recognized accurately. The correlation between the signs </text>
<text top="927" left="477" width="371" height="14" font="1">plays  a  role  factor  for  the  inconsistency  of  the  noise </text>
<text top="950" left="477" width="371" height="14" font="1">immunity  seen.  The  performance  is  calculated  as  the </text>
<text top="973" left="477" width="371" height="14" font="1">ratio of correct patterns recognized to the total number of </text>
<text top="996" left="477" width="371" height="14" font="1">test  patterns.  On  an  average  performance  of  95%  is </text>
<text top="1018" left="477" width="371" height="14" font="1">achieved with the pattern identification and takes around </text>
<text top="1041" left="477" width="371" height="14" font="1">43.45ms to retrieve one image pattern. Given an input </text>
<text top="1064" left="477" width="371" height="14" font="1">frame  for  testing  the  time  taken  by  the  network </text>
<text top="1087" left="477" width="371" height="14" font="1">architecture  to  process  and  recognize  the  sign  is  the </text>
<text top="717" left="76" width="371" height="14" font="1">parameters  (learning  rate,  momentum  and  error </text>
<text top="740" left="76" width="371" height="14" font="1">threshold)  optimization.  Inclusion  of  the  momentum  is </text>
<text top="763" left="76" width="371" height="14" font="1">proved to be useful with the training sets that include a few </text>
<text top="786" left="76" width="371" height="14" font="1">patterns that are very different than the rest (as in patterns </text>
<text top="809" left="76" width="371" height="14" font="1">B, W, Y are completely different from the patterns A, C, O </text>
<text top="832" left="76" width="371" height="14" font="1">where the finger tips are not present) demonstrated in the </text>
<text top="855" left="76" width="371" height="14" font="1">worst case where there is no momentum compared to </text>
<text top="878" left="76" width="371" height="14" font="1">the best and intermediate cases. Normally, these patterns </text>
<text top="901" left="76" width="371" height="14" font="1">will upset the convergence towards the minimum defined </text>
<text top="924" left="76" width="371" height="14" font="1">by the majority of the patterns. To improve that, one could </text>
<text top="947" left="76" width="371" height="14" font="1">use  a  very  small  learning  rate  (&lt;0.1),  but  then  the </text>
<text top="970" left="76" width="371" height="14" font="1">convergence would be very slow. Instead the study keep </text>
<text top="993" left="76" width="371" height="14" font="1">a moderate learning rate (0.1) but the authors will involve </text>
<text top="1016" left="76" width="371" height="14" font="1">the  previous  weight  change,  in  addition  to  the  current </text>
<text top="1039" left="76" width="371" height="14" font="1">data (weight change), for defining the weight upgrade. </text>
<text top="1061" left="76" width="371" height="14" font="1">This  will  provide  certain  inertia  to  the  training,  which  will </text>
<text top="1084" left="76" width="371" height="14" font="1">minimize the disruption of the convergence caused by </text>
<text top="674" left="235" width="451" height="12" font="7">Figure 7. Various simulations for Best, Intermediate and Worst case considerations</text>
<text top="688" left="283" width="355" height="12" font="7">for the each level of HDL-CNN training acquired (15 input neurons)</text>
<text top="1140" left="826" width="14" height="12" font="4"><i>13</i></text>
<text top="1142" left="381" width="34" height="7" font="6">l</text>
<text top="1140" left="80" width="288" height="12" font="4"><i>i-manager’s Journal o  Electronics Engineering, Vol.</i></text>
<text top="1140" left="388" width="3" height="11" font="4"><i> </i></text>
<text top="1140" left="391" width="33" height="12" font="4"><i>No. 1 </i></text>
<text top="1142" left="423" width="34" height="7" font="6">l</text>
<text top="1142" left="429" width="2" height="8" font="6"> </text>
<text top="1140" left="202" width="7" height="12" font="4"><i>n</i></text>
<text top="1140" left="368" width="14" height="12" font="4"><i> 2 </i></text>
<text top="1140" left="431" width="161" height="12" font="4"><i>September - November 2011</i></text>
</page>
<page number="9" position="absolute" top="0" left="0" height="1209" width="919">
	<fontspec id="20" size="39" family="Times" color="#231f20"/>
<text top="67" left="79" width="218" height="24" font="0">RESEARCH PAPERS</text>
<text top="632" left="477" width="371" height="14" font="1">updates and hence concurrently done at a time for all </text>
<text top="654" left="477" width="371" height="14" font="1">neurons  rather  than  the  for  loop  control  used  in  object </text>
<text top="677" left="477" width="371" height="14" font="1">oriented languages. Arithmetic precision is also achieved </text>
<text top="700" left="477" width="371" height="14" font="1">due  to  the  use  of  floating  point  libraries.  Moving  to  this </text>
<text top="723" left="477" width="371" height="14" font="1">parallel  hardware  provided  the  speedups  in  orders  of </text>
<text top="746" left="477" width="371" height="14" font="1">magnitude  (13x  times  in  this  case).    Many  advanced </text>
<text top="769" left="477" width="371" height="14" font="1">families  of  FPGAs  have  been  manufactured  (Vargas, </text>
<text top="792" left="477" width="371" height="14" font="1">Barba,  Torres  &amp;  Mattos,  2011)  that  contain  more  logic </text>
<text top="815" left="477" width="371" height="14" font="1">blocks  and  also  video  input  controllers   which  clearly </text>
<text top="838" left="477" width="371" height="14" font="1">implies the design to be optimized on different goals of </text>
<text top="861" left="477" width="371" height="14" font="1">area,  power  and  speed.  The  use  of  VHDL  for  the </text>
<text top="884" left="477" width="371" height="14" font="1">architectural  design  represents  a  very  practical  option </text>
<text top="907" left="477" width="371" height="14" font="1">when  dealing  with  complex  systems.  Thus  the  FPGAs </text>
<text top="930" left="477" width="371" height="14" font="1">constitute a very powerful option for implementing CNNs </text>
<text top="953" left="477" width="371" height="14" font="1">since  we  can  really  exploit  their  parallel  processing </text>
<text top="976" left="477" width="371" height="14" font="1">capabilities to improve the performance. To progress the </text>
<text top="999" left="477" width="371" height="14" font="1">research  the  algorithm  needs  to  be  extended  to </text>
<text top="1021" left="477" width="371" height="14" font="1">recognize  words  or  sentences  which  involve  a  set  of </text>
<text top="1044" left="477" width="371" height="14" font="1">images (i.e. video frames) to be processed at a time with </text>
<text top="1067" left="477" width="371" height="14" font="1">the help of a vector bank. Also the HDL-CNN architecture is </text>
<text top="1090" left="477" width="371" height="14" font="1">generic as it could be used for other pattern recognition </text>
<text top="815" left="744" width="4" height="14" font="11">,</text>
<text top="632" left="76" width="371" height="14" font="1">single pattern recognition time. The signs involving motion </text>
<text top="654" left="76" width="371" height="14" font="1">(J, Z) are the limitations of the architecture as compared to </text>
<text top="677" left="76" width="371" height="14" font="1">the software solution. The epochs involved to reach the </text>
<text top="700" left="76" width="371" height="14" font="1">steady  state  and  the  noise  immunity  achieved  are </text>
<text top="723" left="76" width="371" height="14" font="1">approximately equal in both cases. An inclusion of a SRAM </text>
<text top="746" left="76" width="371" height="14" font="1">vector  back  to  store  the  motion  vectors  of  adjacent </text>
<text top="769" left="76" width="371" height="14" font="1">frames could be considered for future research in order to </text>
<text top="792" left="76" width="200" height="14" font="1">eliminate the above limitations.</text>
<text top="819" left="76" width="74" height="14" font="1">Conclusion</text>
<text top="845" left="76" width="371" height="14" font="1">The combinational neural networks are one of the most </text>
<text top="868" left="76" width="371" height="14" font="1">powerful  tools  in  the  recognition/  identification  process </text>
<text top="891" left="76" width="371" height="14" font="1">applications. The VHDL based model design of the sign </text>
<text top="914" left="76" width="371" height="14" font="1">recognition model presents a performance pretty good </text>
<text top="937" left="76" width="371" height="14" font="1">to  identify  the  static  images  of  the  American  Sign </text>
<text top="960" left="76" width="371" height="14" font="1">Language  alphabets  with  implementation  on  the  Xilinx </text>
<text top="983" left="76" width="371" height="14" font="1">Spartan  3E  FPGA.    Performance  is  achieved  as  the </text>
<text top="1006" left="76" width="371" height="14" font="1">expensive operations are optimized in VHDL by the use of </text>
<text top="1029" left="76" width="371" height="14" font="1">a  matrix-vector  multiplication  performed  during  each </text>
<text top="1052" left="76" width="371" height="14" font="1">layer  and  level  data  flow.  Dedicated  adder  and </text>
<text top="1075" left="76" width="371" height="14" font="1">multipliers  are  used  for  performing  the  weight  layer </text>
<text top="152" left="122" width="83" height="11" font="8">Image Processed</text>
<text top="164" left="134" width="146" height="11" font="8">of Gaussian) Edge detection   </text>
<text top="152" left="205" width="76" height="11" font="8">-LoG (Laplacian</text>
<text top="152" left="348" width="23" height="11" font="8">Sign </text>
<text top="164" left="353" width="60" height="11" font="8">Recognized </text>
<text top="152" left="371" width="44" height="11" font="8">Alphabet</text>
<text top="151" left="548" width="115" height="11" font="8">Noisy Image Processed-</text>
<text top="164" left="557" width="99" height="11" font="8">LoG Edge detection </text>
<text top="152" left="729" width="67" height="11" font="8">Sign Alphabet</text>
<text top="164" left="733" width="57" height="11" font="8">Recognized</text>
<text top="229" left="370" width="33" height="42" font="20"><b>B </b></text>
<text top="229" left="751" width="31" height="42" font="20"><b>Y </b></text>
<text top="365" left="377" width="21" height="42" font="20"><b>I </b></text>
<text top="365" left="750" width="25" height="42" font="20"><b>V</b></text>
<text top="499" left="371" width="32" height="42" font="20"><b>C </b></text>
<text top="499" left="753" width="27" height="42" font="20"><b>L </b></text>
<text top="604" left="234" width="452" height="12" font="7">Figure 8. Sign language alphabets recognized by the HDL-CNN recognition model</text>
<text top="1140" left="86" width="14" height="12" font="4"><i>14</i></text>
<text top="1140" left="323" width="288" height="12" font="4"><i>i-manager’s Journal o  Electronics Engineering, Vol.</i></text>
<text top="1140" left="445" width="7" height="12" font="4"><i>n</i></text>
<text top="1140" left="611" width="3" height="12" font="4"><i> </i></text>
<text top="1142" left="624" width="34" height="7" font="6">l</text>
<text top="1142" left="666" width="34" height="7" font="6">l</text>
<text top="1142" left="672" width="2" height="8" font="6"> </text>
<text top="1140" left="615" width="221" height="12" font="4"><i>2   No. 1  September - November 2011</i></text>
</page>
<page number="10" position="absolute" top="0" left="0" height="1209" width="919">
<text top="67" left="79" width="218" height="24" font="0">RESEARCH PAPERS</text>
<text top="144" left="477" width="371" height="14" font="1">October).    A  VLSI  Implementation  of  an  Analog  Neural </text>
<text top="167" left="477" width="371" height="14" font="1">Network  suitable  for  Genetic  Algorithms.  <i>ICES  '01 </i></text>
<text top="190" left="477" width="371" height="14" font="5"><i>Proceedings  of  the  4th  International  Conference  on </i></text>
<text top="212" left="477" width="368" height="14" font="5"><i>Evolvable  Systems:  From  Biology  to  Hardware</i>.  Springer-</text>
<text top="235" left="477" width="140" height="14" font="1">Verlag London, 50-61.</text>
<text top="262" left="477" width="371" height="14" font="1">[11].  Short,  Kenneth  L.,  (2009).  <i>VHDL  for  Engineer</i>.  NJ: </text>
<text top="285" left="477" width="136" height="14" font="1">Pearson Prentice Hall.</text>
<text top="312" left="477" width="371" height="14" font="1">[12]. Ashenden, Peter J., (1995). The designer's guide to </text>
<text top="335" left="477" width="325" height="14" font="1">VHDL. San Francisco: Morgan Kaufmann publishers.</text>
<text top="361" left="477" width="371" height="14" font="1">[13].  Mekala,  P.,  Erdogan,  S.  and  Fan,  J.,  (2010, </text>
<text top="384" left="477" width="372" height="14" font="1">November).  Automatic  object  recognition  using </text>
<text top="407" left="477" width="371" height="14" font="1">combinational  neural  networks  in  surveillance  networks. </text>
<text top="430" left="477" width="371" height="14" font="5"><i>IEEE  3rd  International  Conference  on  Computer  and </i></text>
<text top="453" left="477" width="371" height="14" font="5"><i>Electrical Engineering (ICCEE'10)</i>, Chengdu, China, Vol. </text>
<text top="476" left="477" width="97" height="14" font="1">8, pp. 387-391.</text>
<text top="503" left="477" width="371" height="14" font="1">[14]. Mehrotra, K., Chilukuri, K.M., and Ranka, S., (1997). </text>
<text top="526" left="477" width="367" height="14" font="5"><i>Elements of Artificial Neural Networks</i>, The MIT Press, pp1-</text>
<text top="549" left="477" width="12" height="14" font="1">2.</text>
<text top="575" left="477" width="371" height="14" font="1">[15]. Caudill, M., Butler, C., (1992). <i>Understanding neural </i></text>
<text top="598" left="477" width="281" height="14" font="5"><i>networks: Computer explorations, MIT press.</i></text>
<text top="625" left="477" width="371" height="14" font="5"><i>[16]. Stergiou, C., and Siganos, D., (1996).</i> <i>Report: Neural </i></text>
<text top="648" left="477" width="376" height="14" font="5"><i>N e t w o r k s </i>.   V o l   1 4 .   R e t r i e v e d   f r o m  </text>
<text top="671" left="477" width="367" height="14" font="1">http://www.doc.ic.ac.uk/~nd/surprise_96/journal/vol4/cs</text>
<text top="694" left="477" width="95" height="14" font="1">11/report.html.</text>
<text top="720" left="477" width="371" height="14" font="1">[17]. Dreyfus, G., (2005). <i>Neural networks: methodology </i></text>
<text top="743" left="477" width="274" height="14" font="5"><i>and applications</i>. Berlin, New York: Springer.</text>
<text top="770" left="477" width="371" height="14" font="1">[18].  Kwan,  H.K.,  (1992,  July).  <i>Simple  sigmoid  like </i></text>
<text top="793" left="477" width="372" height="14" font="5"><i>activation  function  suitable  for  digital  hardware </i></text>
<text top="816" left="477" width="371" height="14" font="5"><i>implementation</i>.  Electronic  Letters,  28(15),  1379-1380. </text>
<text top="839" left="477" width="175" height="14" font="1">doi: 10.1049/EL: 19920877.</text>
<text top="865" left="477" width="371" height="14" font="1">[19]. Fausett, L., (1994). <i>Fundamentals of Neural Networks </i></text>
<text top="888" left="477" width="367" height="14" font="5"><i>– architecture, algorithms and applications</i>. Prentice Hall.</text>
<text top="915" left="477" width="371" height="14" font="1">[20]. Mekala, P., Gao, Y., Fan, J., and Davari, A., (2011, </text>
<text top="938" left="477" width="371" height="14" font="1">March). Real-time sign language recognition based on </text>
<text top="961" left="477" width="371" height="14" font="1">neural  network  architecture.  <i>Joint  IEEE  International </i></text>
<text top="984" left="477" width="372" height="14" font="5"><i>Conference  on  Industrial  Technology  &amp;  43rd </i></text>
<text top="1007" left="477" width="371" height="14" font="5"><i>Southeastern  Symposium  on  System  Theory  (SSST'11</i>), </text>
<text top="1030" left="477" width="156" height="14" font="1">Auburn, AL, pp. 197-201.</text>
<text top="1056" left="477" width="371" height="14" font="1">[21]. Xilinx (2009). <i>XST User Guide, Xilinx Inc</i>. Retrieved from </text>
<text top="1079" left="477" width="367" height="14" font="1">http://www.xilinx.com/support/documentation/sw_manu</text>
<text top="144" left="76" width="371" height="14" font="1">(like objects, face) provided the training sequences have </text>
<text top="167" left="76" width="80" height="14" font="1">to be varied.</text>
<text top="193" left="76" width="75" height="14" font="1">References</text>
<text top="220" left="76" width="371" height="14" font="1">[1]. Torres-Huitzil, C., Girau, B., and Gauffriau, A., (2007). </text>
<text top="243" left="76" width="372" height="14" font="1">H a r d w a r e / S o f t w a r e  C o - d e s i g n  f o r  E m b e d d e d </text>
<text top="266" left="76" width="371" height="14" font="1">Implementation  of  Neural  Networks.  <i>Reconfigurable </i></text>
<text top="289" left="76" width="371" height="14" font="5"><i>computing:  architectures,  tools  and  applications- </i></text>
<text top="312" left="76" width="327" height="14" font="5"><i>Lecture notes in computer science</i>, 4419, 167-178.</text>
<text top="338" left="76" width="371" height="14" font="1">[2].  Cantrell,  C.,  and  Wurtz,  L.,  (1993).  A  Parallel  Bus </text>
<text top="361" left="76" width="371" height="14" font="1">Architecture for artificial neural networks.<i>Southeastcon'93 </i></text>
<text top="384" left="76" width="377" height="14" font="5"><i>P r o c e e d i n g s ,   I E E E </i>  ( p p . 5 ) .   d o i :  </text>
<text top="407" left="76" width="199" height="14" font="1">10.1109/SECON.1993.465674.</text>
<text top="434" left="76" width="372" height="14" font="1">[3].  Baker,  T.,  and  Hammerstrom,  D.,  (1989). </text>
<text top="457" left="76" width="371" height="14" font="1">Characterization  of  Artificial  Neural  Network  Algorithms. </text>
<text top="480" left="76" width="371" height="14" font="5"><i>Circuits and Systems- IEEE International Symposium</i>, vol.1, </text>
<text top="503" left="76" width="261" height="14" font="1">78-81. doi: 10.1109/ISCAS.1989.100296.</text>
<text top="529" left="76" width="371" height="14" font="1">[4]. Blais, A., and Mertz, D., (2001, July). <i>An Introduction to </i></text>
<text top="552" left="76" width="372" height="14" font="5"><i>Neural  Networks  –  Pattern  Learning  with  Back </i></text>
<text top="575" left="76" width="374" height="14" font="5"><i>P r o p a g a t i o n   A l g o r i t h m </i>.   R e t r i e v e d   f r o m  </text>
<text top="598" left="76" width="342" height="14" font="1">http://www.ibm.com/developerworks/library/l-neural/.</text>
<text top="625" left="76" width="371" height="14" font="1">[5]. Vargas, P. Lorena, Barba, L., Torres, C. O., and Mattos, </text>
<text top="648" left="76" width="371" height="14" font="1">L.,  (2011).  Sign  Language  Recognition  System  using </text>
<text top="671" left="76" width="371" height="14" font="1">Neural  Network  for  Digital  Hardware  Implementation. </text>
<text top="694" left="76" width="371" height="14" font="5"><i>Journal  of  Physics:  Conference  Series</i>,  274(1).  doi: </text>
<text top="717" left="76" width="207" height="14" font="1">1088/1742-6596/374/1/012051.</text>
<text top="743" left="76" width="371" height="14" font="1">[6].  Ali,  H.  K.,  and  Mohammed,  E.  Z.,  (2010,  August). </text>
<text top="766" left="76" width="371" height="14" font="1">Design Artificial Neural Network using FPGA. <i>International </i></text>
<text top="789" left="76" width="371" height="14" font="5"><i>journal of computer science and network security</i>, 10(8), </text>
<text top="812" left="76" width="42" height="14" font="1">88-92.</text>
<text top="839" left="76" width="371" height="14" font="1">[7]. Omondi, R. Amos, and Rajapakse, Jagath C., (2006, </text>
<text top="862" left="76" width="371" height="14" font="1">July).  <i>FPGA  Implementations  of  Neural  Networks</i>. </text>
<text top="885" left="76" width="55" height="14" font="1">Springer.</text>
<text top="911" left="76" width="371" height="14" font="1">[8]. Izeboudjen, N., Farah, A., Bessalah, H., Bouridane, A., </text>
<text top="934" left="76" width="371" height="14" font="1">and Chikhi, N., (2008, July). High Level Design Approach </text>
<text top="957" left="76" width="371" height="14" font="1">for  FPGA  Implementation  of  ANNs.    <i>Encyclopedia  of </i></text>
<text top="980" left="76" width="372" height="14" font="5"><i>Artificial  Intelligence,  IGI-Global  Publishers</i>.  doi: </text>
<text top="1003" left="76" width="185" height="14" font="1">10.4018/978-1-599-4-849-9.</text>
<text top="1030" left="76" width="371" height="14" font="1">[9]. Berry, D. L., (2002). <i>VHDL programming by examples</i>. </text>
<text top="1053" left="76" width="171" height="14" font="1">McGraw-Hill, fourth edition.</text>
<text top="1079" left="76" width="371" height="14" font="1">[10]. Schemmel, J., Meier, K. and Schurmann, F., (2001, </text>
<text top="1140" left="826" width="14" height="12" font="4"><i>15</i></text>
<text top="1142" left="381" width="34" height="7" font="6">l</text>
<text top="1140" left="80" width="288" height="12" font="4"><i>i-manager’s Journal o  Electronics Engineering, Vol.</i></text>
<text top="1140" left="388" width="3" height="11" font="4"><i> </i></text>
<text top="1140" left="391" width="33" height="12" font="4"><i>No. 1 </i></text>
<text top="1142" left="423" width="34" height="7" font="6">l</text>
<text top="1142" left="429" width="2" height="8" font="6"> </text>
<text top="1140" left="202" width="7" height="12" font="4"><i>n</i></text>
<text top="1140" left="368" width="14" height="12" font="4"><i> 2 </i></text>
<text top="1140" left="431" width="161" height="12" font="4"><i>September - November 2011</i></text>
</page>
<page number="11" position="absolute" top="0" left="0" height="1209" width="919">
	<fontspec id="21" size="13" family="Times" color="#231f20"/>
<text top="67" left="79" width="218" height="24" font="0">RESEARCH PAPERS</text>
<text top="144" left="477" width="371" height="14" font="1">reprogrammable  logic.  <i>IEEE  proceedings,  Computer </i></text>
<text top="167" left="477" width="372" height="14" font="5"><i>Digital  Techniques</i>,  150(6).  doi:  10.1049/ip-cdt: </text>
<text top="190" left="477" width="70" height="14" font="1">20030965.</text>
<text top="144" left="76" width="132" height="14" font="1">als/xilinx12_2/xst.pdf.</text>
<text top="170" left="76" width="371" height="14" font="1">[22]. Tommiska, M.T., (2003, November). Efficient digital </text>
<text top="193" left="76" width="372" height="14" font="1">i m p l e m e n t a t i o n  o f  t h e  s i g m o i d  f u n c t i o n  f o r </text>
<text top="265" left="76" width="674" height="12" font="4"><i>Priyanka Mekala received her M.S. degree in Electrical Engineering from Arizona State University and B.E. degree in Electronics </i></text>
<text top="280" left="76" width="674" height="12" font="4"><i>and Communications from Osmania University, India in May 2009 and June 2007, respectively. She started to work on her Ph.D. </i></text>
<text top="294" left="76" width="674" height="12" font="4"><i>degree in Electrical Engineering at FIU in fall 2009. She is currently a Ph.D. candidate. Her research interests include Signal </i></text>
<text top="308" left="76" width="570" height="12" font="4"><i>Processing, Real-time Image/ Video processing and VLSI design/ testing. She is also a student member of IEEE.</i></text>
<text top="366" left="76" width="674" height="12" font="4"><i>Dr. Fan is currently working as an Assistant Professor in Electrical and Computer Engineering at Florida International University. His </i></text>
<text top="380" left="76" width="674" height="12" font="4"><i>research  interests  include  very-large-scaled-integrated  (VLSI)  circuit  simulation,  modeling,  optimization,  bio-electronics, </i></text>
<text top="395" left="76" width="674" height="12" font="4"><i>embedded real-time operating systems in application to robotic control, and wireless communications in sensor networks. Prior </i></text>
<text top="409" left="76" width="674" height="12" font="4"><i>to his academic career, He served as Vice President of Vivavr Technology, Inc., and General Manager/co-founder of Musica </i></text>
<text top="424" left="76" width="674" height="12" font="4"><i>Technologies,  Inc.  From  1988  to  2002,  he  held  various  senior  technical  positions  in  California  at  Western  Digital,  Emulex </i></text>
<text top="438" left="76" width="674" height="12" font="4"><i>Corporation, Adaptec Inc., and Toshiba America. His product line of research and development includes Virtual Reality (VR) 3-D </i></text>
<text top="452" left="76" width="674" height="12" font="4"><i>animation, MP3 players, hard drives, fiber channel adapters, SCSI/ATAPI adapters, RAID disk array, PCMCIA cards and laser </i></text>
<text top="467" left="76" width="674" height="12" font="4"><i>printer controllers. He received his Ph.D. degree in Electrical Engineering at University of California, Riverside in 2007, and the </i></text>
<text top="481" left="76" width="674" height="12" font="4"><i>Master of Science degree in Electrical Engineering from State University of New York at Buffalo in 1987. He also holds Bachelor of </i></text>
<text top="496" left="76" width="674" height="12" font="4"><i>Science degree in Electronics Engineering from National Chiao Tung University in Taiwan, R.O.C. He has served as a steering </i></text>
<text top="510" left="76" width="674" height="12" font="4"><i>committee member of SSST, a technical program committee member for ICESS, CAMAD, ISQED, ISCAS, and an invited tutorial </i></text>
<text top="524" left="76" width="281" height="12" font="4"><i>speaker for ASICON'07. He is a Senior Member of IEEE.</i></text>
<text top="243" left="393" width="134" height="15" font="21">ABOUT THE AUTHORS</text>
<text top="1140" left="86" width="14" height="12" font="4"><i>16</i></text>
<text top="1140" left="323" width="288" height="12" font="4"><i>i-manager’s Journal o  Electronics Engineering, Vol.</i></text>
<text top="1140" left="445" width="7" height="12" font="4"><i>n</i></text>
<text top="1140" left="611" width="3" height="12" font="4"><i> </i></text>
<text top="1142" left="624" width="34" height="7" font="6">l</text>
<text top="1142" left="666" width="34" height="7" font="6">l</text>
<text top="1142" left="672" width="2" height="8" font="6"> </text>
<text top="1140" left="615" width="221" height="12" font="4"><i>2   No. 1  September - November 2011</i></text>
</page>
</pdf2xml>
